<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">















  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Basic Embedding Model1. NNLM(Neural Network  Language Model) 不同与 $n-gram$ 统计语言模型的一种神经网络语言模型，并且可以在计算出句子联合概率的同时，求出词向量，但是速度较慢。   模型可以分成两部分理解:   首先是一个线性的embedding 层。它将输入的 n−1 个one−hot词向量，通过一个共享的$|V| \time">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Basic Embedding Model">
<meta property="og:url" content="https://gongjintao.com/post/603c955b.html">
<meta property="og:site_name" content="GongJintao&#39; Blog">
<meta property="og:description" content="Basic Embedding Model1. NNLM(Neural Network  Language Model) 不同与 $n-gram$ 统计语言模型的一种神经网络语言模型，并且可以在计算出句子联合概率的同时，求出词向量，但是速度较慢。   模型可以分成两部分理解:   首先是一个线性的embedding 层。它将输入的 n−1 个one−hot词向量，通过一个共享的$|V| \time">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://i.loli.net/2020/03/01/coV9SEJCf2v5xsr.png">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005XIOOugy1gcqwxeaugoj30an0d376c.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005XIOOugy1gcqz161w3xj30ca0e8q5a.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005XIOOugy1gcrfg3xw4tj30nv0dcmyg.jpg">
<meta property="og:image" content="http://ww1.sinaimg.cn/large/005XIOOugy1gcvv4e966bj30ha0bv3z3.jpg">
<meta property="og:updated_time" content="2020-03-29T14:28:17.382Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Basic Embedding Model">
<meta name="twitter:description" content="Basic Embedding Model1. NNLM(Neural Network  Language Model) 不同与 $n-gram$ 统计语言模型的一种神经网络语言模型，并且可以在计算出句子联合概率的同时，求出词向量，但是速度较慢。   模型可以分成两部分理解:   首先是一个线性的embedding 层。它将输入的 n−1 个one−hot词向量，通过一个共享的$|V| \time">
<meta name="twitter:image" content="https://i.loli.net/2020/03/01/coV9SEJCf2v5xsr.png">



  <link rel="alternate" href="/atom.xml" title="GongJintao' Blog" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://gongjintao.com/post/603c955b">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Basic Embedding Model | GongJintao' Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">GongJintao' Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://gongjintao.com/post/603c955b.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Gong Jintao">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="GongJintao' Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Basic Embedding Model

              
            
          </h2>
        

        <div class="post-meta">

          
          
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2020-03-17 19:27:46" itemprop="dateCreated datePublished" datetime="2020-03-17T19:27:46+08:00">2020-03-17</time>
            </span>
          

          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-03-29 22:28:17" itemprop="dateModified" datetime="2020-03-29T22:28:17+08:00">2020-03-29</time>
              </span>
            
          

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">评论数：</span>
                <a href="/post/603c955b.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/post/603c955b.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          <br>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Basic-Embedding-Model"><a href="#Basic-Embedding-Model" class="headerlink" title="Basic Embedding Model"></a>Basic Embedding Model</h1><h2 id="1-NNLM-Neural-Network-Language-Model"><a href="#1-NNLM-Neural-Network-Language-Model" class="headerlink" title="1. NNLM(Neural Network  Language Model)"></a>1. NNLM(Neural Network  Language Model)</h2><blockquote>
<p>不同与 $n-gram$ 统计语言模型的一种神经网络语言模型，并且可以在计算出句子联合概率的同时，求出词向量，但是速度较慢。</p>
</blockquote>
<p><img src="https://i.loli.net/2020/03/01/coV9SEJCf2v5xsr.png" alt="NNLM"></p>
<p>模型可以分成两部分理解:</p>
<blockquote>
<ol>
<li><p>首先是一个线性的embedding 层。它将输入的 n−1 个one−hot词向量，通过一个共享的$|V| \times $的矩阵$C$，映射为 n−1 个分布式的词向量。其中，$|V|$是词典的大小，$m$ 是embedding向量的维度（一个先验参数）。$C $矩阵里存储了要学习的词向量。</p>
</li>
<li><p>其次是一个简单的前向反馈神经网络 g 。它由一个tanh 隐层和一个softmax 输出层组成。通过将embedding 层输出的N−1 个词向量映射为一个长度为 $|V| $ 的概率分布向量，从而对词典中的word 在输入 context 下的条件概率做出预估：</p>
<script type="math/tex; mode=display">
\hat{P}\left(w_{t} | w_{1}^{t-1}\right) \approx f\left(i, w_{t-1}, \cdots, w_{t-n+1}\right)=g\left(i, C\left(w_{t-1}\right), \cdots, C\left(w_{t-n+1}\right)\right)</script></li>
</ol>
</blockquote>
<p>然后可以通过最小化一个cross−entropy 的正则化损失函数来调整模型的参数$\theta =(C,w)$：</p>
<script type="math/tex; mode=display">
L = \frac{1}{T}\sum_t \log f(w_t,w_{t-1},\cdots,w_{t-n+1},\theta) + R(\theta)</script><p>其中 $R(\theta)$ 为正则项。</p>
<p>上述模型中，自由参数的个数仅仅和$|V|$ 线性相关。然后神经网络模型通过计算如下式子，即<code>softmax</code> 层，来得到输出概率：</p>
<script type="math/tex; mode=display">
\hat{P}(w_t|w_{t-1},\cdots,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}</script><p>其中$y_i$ 是每个单词$i$ 的未规范化对数概率，由下式计算：</p>
<script type="math/tex; mode=display">
y = b + Wx + U\tanh(d+Hx)</script><p>$\tanh$逐元素计算的。其中 $W$ 是可选为 0 的，$x$ 则是输入的单词向量连接而成的：</p>
<script type="math/tex; mode=display">
x=\left(C\left(w_{t-1}\right), C\left(w_{t-2}\right), \cdots, C\left(w_{t-n+1}\right)\right)</script><p>令 $h$ 为隐藏单元的数量，$m$ 是每个词的词向量维度。</p>
<p>参数$\theta = (b,d,W,U,H,C)$说明：</p>
<blockquote>
<ol>
<li>$W$ 为词特征层到输出层权的值矩阵，大小为 $|V| \times (n-1)m$，当没有直接连接词特征与输出层时，设置为 0</li>
<li>$b$ 为输出偏置矩阵，大小为 $|V| $</li>
<li>$d$ 为隐藏层偏置矩阵，大小为 $h$</li>
<li>$U$ 为隐藏层到输出层的权值矩阵，大小为 $|V| \times h$</li>
<li>$H$ 为隐藏层的权值矩阵，大小为 $h \times (n-1)m$</li>
<li>$C$ 即嵌入矩阵(词向量)，大小为 $|V| \times m$</li>
</ol>
</blockquote>
<p>通过随机梯度下降法的每一步迭代为：</p>
<script type="math/tex; mode=display">
\theta \leftarrow \theta+\varepsilon \frac{\partial \log \hat{P}\left(w_{t} | w_{t-1}, \cdots w_{t-n+1}\right)}{\partial \theta}</script><h2 id="2-Word2vec"><a href="#2-Word2vec" class="headerlink" title="2.Word2vec"></a>2.Word2vec</h2><p>word2vec有两种常用算法：</p>
<blockquote>
<ol>
<li>continuous bag-of-words (CBOW) :根据词向量从周围语境中预测中心词</li>
<li>skip-gram: 从中心词预测上下文词的分布（概率)</li>
</ol>
</blockquote>
<p>两种训练方法：</p>
<blockquote>
<ol>
<li>negative sampling(负采样)：采样负样例</li>
<li>hierarchical softmax(分层softmax)：使用一个有效的树结构来计算所有词的概率</li>
</ol>
</blockquote>
<h3 id="2-1-CBOW"><a href="#2-1-CBOW" class="headerlink" title="2.1 CBOW"></a>2.1 CBOW</h3><blockquote>
<ol>
<li><p>首先用 <code>one-hot</code> 编码来表示上下文单词</p>
</li>
<li><p>定义两个矩阵：$ \mathcal{V} \in \mathbb{R}^{n \times|V|} 和 \mathcal{U} \in \mathbb{R}^{|V|\times n}$</p>
<p>$\mathcal{V}$ 是输入矩阵，其第 $i$ 列 $v_i$ 是词 $w_i$ 的 $n$ 维嵌入向量，用来表示上下文；</p>
<p>$\mathcal{U}$ 是输出矩阵，其第 $j$ 行 $u_i$ 是词 $w_j$ 的 $n$ 维嵌入向量，用来表示中心词；</p>
<p>所以对于每个词都学习了两个向量。</p>
</li>
</ol>
</blockquote>
<p><strong>具体步骤</strong>：</p>
<ol>
<li><p>对<code>one-hot</code>编码来表示输入上下文:</p>
<script type="math/tex; mode=display">
m: (x^{(c-m)},\dots,x^{(c-1)},x^{(c+1)},\dots,x^{(c+m)} \in \mathbb{R}^{|V|})</script></li>
<li><p>求得上下文的嵌入向量:</p>
<script type="math/tex; mode=display">
(v_{c-m}=\mathcal{V}x^{(c-m)},v_{c-m+1}=\mathcal{V}x^{(c-m+1)},\dots,v_{c+m}=\mathcal{V}x^{(c+m)} \in \mathbb{R}^n)</script></li>
<li><p>对这些向量求平均值: </p>
<script type="math/tex; mode=display">
\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^{n}</script></li>
<li><p>生成记分向量$z$，因为点积越高，相似度越高，为了达到较高的分数，$z$ 会把相似的词推的更近：</p>
<script type="math/tex; mode=display">
z = \mathcal{U}\hat{v} \in \mathbb{R}^{|V|}</script></li>
<li><p>将分数转化成概率</p>
<script type="math/tex; mode=display">
\hat{y} = \text{softmax}(z) \in \mathbb{R}^{|V|}</script></li>
<li><p>生成的概率$\hat{y} \in \mathbb{R}^{|V|}$ 与真实概率$y \in \mathbb{R}^{|V|}$相对比</p>
</li>
</ol>
<p>求解两个矩阵 $\mathcal{U} 和 \mathcal{V}$ 的训练过程：</p>
<ol>
<li>首先定义损失函数为交叉熵函数：<script type="math/tex; mode=display">
H(\hat{y},y) = -\sum_{j=1}^{|V|}y_i \log(\hat{y_j})</script>因为 $y$  是<code>one-hot</code> 向量，因此损失函数可以简化为：<script type="math/tex; mode=display">
H(\hat{y},y) = -y_i \log(\hat{y_i})</script>所以最终的模型即：<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } J &=-\log P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\
&=-\log P\left(u_{c} | \hat{v}\right) \\
&=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \\
&=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)
\end{aligned}</script></li>
<li><p>然后用随机梯度下降法来更新所有的参数 $u_i,v_i$。</p>
<p>基本模型图如下所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcqwxeaugoj30an0d376c.jpg" alt="CBOW"></p>
</li>
</ol>
<h3 id="2-2-skip-gram"><a href="#2-2-skip-gram" class="headerlink" title="2.2 skip-gram"></a>2.2 skip-gram</h3><p>过程与 <strong>CBOW</strong> 相反，矩阵$ \mathcal{V} 和 \mathcal{U}$ 定义和上面相同</p>
<p><strong>具体步骤</strong>：</p>
<ol>
<li><p>用<code>one-hot</code> 编码来表示中心词 $x \in \mathbb{R}^{|V|}$</p>
</li>
<li><p>得到中心词的嵌入单词向量 $v_c = \mathcal{V}x \in \mathbb{R}^n$</p>
</li>
<li><p>计算得分向量 $z = \mathcal{U}v_c $</p>
</li>
<li><p>将得分向量用 <strong>softmax</strong>  转换成概率</p>
<script type="math/tex; mode=display">
\hat{y} = \text{softmax}(z)</script><p>$\hat{y}_{c-m}, \ldots, \hat{y}_{c-1}, \hat{y}_{c+1}, \ldots, \hat{y}_{c+m}$ 即对应上下文每个单词的概率</p>
</li>
<li><p>与真实单词$y^{(c-m)}, \ldots, y^{(c-1)}, y^{(c+1)}, \ldots, y^{(c+m)}$ 做对比</p>
</li>
</ol>
<p><strong>定义损失函数：</strong></p>
<p>与 <strong>CBOW</strong> 不同之处在于，此处用到了朴素贝叶斯假设，即在给定中心词的条件下，输出词之间是相互独立的，所以有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text{minimize} J &=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \\

&=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^{T} v_{c}\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)} \\
&=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)
\end{aligned}</script><p>最后用随机梯度下降求解。</p>
<p>基本模型如下图所示：</p>
<p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcqz161w3xj30ca0e8q5a.jpg" alt="skip-gram"></p>
<h3 id="2-3-Negative-Sampling-负采样"><a href="#2-3-Negative-Sampling-负采样" class="headerlink" title="2.3 Negative Sampling(负采样)"></a>2.3 Negative Sampling(负采样)</h3><p>因为在计算目标函数的时候，$|V|$上的求和计算量很大。因此可以近似计算它。对于每一个训练步骤，我们不必在整个词汇表上循环，只需举几个反面的例子，我们从噪声分布(Pn(w))中采样，其概率与词汇的频率顺序相匹配。</p>
<ol>
<li><p>用 $(w,c)$ 来表示单词和上下文</p>
</li>
<li><p>$P(D=1 |w,c)$ 表示$(w,c)$来自与语料库中的概率，$P(D=0 | w,c)$表示不是来自与语料库的概率。然后用 sigmoid 函数对 $P(D = 1 | w,c)$ 建模：</p>
<script type="math/tex; mode=display">
P(D=1 | w, c, \theta)=\sigma\left(v_{c}^{T} v_{w}\right)=\frac{1}{1+e^{\left(-v_{c}^{T} v_{w}\right)}}</script></li>
<li><p>建立新的目标函数，使得 $P(D=1|w,c,\theta) 和 P(D=0|w,c,\theta)$ 最大化，$\theta$ 代表模型的参数，在 <strong>word2vec</strong> 中即 $\mathcal{V} 和 \mathcal{U}$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 | w, c, \theta) \\
&=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 | w, c, \theta)) \\
&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 | w, c, \theta)) \\
&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\
&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \\
&=\underset{\theta}{\operatorname{argmin}}- \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)
\end{aligned}</script><p>$\tilde{D}$ 是“负”语料库，即里面是一些出现概率较低的语句。</p>
<p>对于 <strong>skip-gram</strong> ，在中心词是 $c$ 上下文是 $c-m+j$ 时，目标函数是：</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)</script><p>对于 <strong>CBOW</strong> ，当中心词是 $u_c$ ，上下文是 $\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m}$时，目标函数是：</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c}^{T} \cdot \hat{v}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot \hat{v}\right)</script><p>上式中的$\{\tilde{u}_k |k=1,\dots,K \}$即从 $P_n(w)$ 中采样的负样本。负采样率建议是二元语法模型下词频的 3/4 次方。</p>
</li>
</ol>
<h3 id="2-4-Hierarchical-softmax-分层softmax"><a href="#2-4-Hierarchical-softmax-分层softmax" class="headerlink" title="2.4 Hierarchical softmax(分层softmax)"></a>2.4 Hierarchical softmax(分层softmax)</h3><p>分层 <strong>softmax</strong> 用一个二叉树来表示词典中的所有单词。每个叶子节点即一个单词，顶点到叶子节点都只有唯一的一条路径。该模型中没有表示单词的输出，而每个中间节点与将模型将要学习的向量相关联。</p>
<p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcrfg3xw4tj30nv0dcmyg.jpg" alt="Hierarchical softmax"></p>
<p>该模型中，$P(w|w_i)$ 等价于从根节点到与代表单词$w$ 的叶结点的随机游走概率，其最大的优点即时间复杂度只有$O(\log(|V|))$。</p>
<p>用$L(w)$ 表示从根节点到叶结点 $w$ 的路径长度，$n(w,i)$ 表示该路径中第 $i$ 个中间节点，则 $n(w,L(w))$ 即 $w$ 的父节点。现对每一个中间节点 $n$ ，我们任意选择其子节点中的一个，记为 $ch(n)$。则概率可由下式算出：</p>
<script type="math/tex; mode=display">
P\left(w | w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^{T} v_{w_{i}}\right)</script><p>where</p>
<script type="math/tex; mode=display">
[x]=\left\{\begin{array}{l}
1 \text { if } x \text { is true } \\
-1 \text { otherwise }
\end{array}\right.</script><p>$\sigma(\cdot)$即 <strong>sigmoid</strong> 函数</p>
<h2 id="3-GloVe-Global-Vector"><a href="#3-GloVe-Global-Vector" class="headerlink" title="3. GloVe(Global Vector)"></a>3. GloVe(Global Vector)</h2><blockquote>
<p>LSA和word2vec作为两大类方法的代表，一个是利用了全局特征的矩阵分解方法，一个是利用局部上下文的方法。GloVe模型就是将这两中特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。</p>
</blockquote>
<ol>
<li><p>首先引入共现矩阵(Co-occurrence Matrix)</p>
<p>用 $X$  来表示词与词之间的共现矩阵，每个元素 $X_{ij}$ 表示词 $j$ 出现在词 $i$ 上下文的次数。$X_i = \sum_kX{ik}$ 表示出现在词 $i$ 上下文所有单词的次数。然后用 $P_{ij} = P(w_j|w_i) =\frac{X_{ij}}{X_i}$ 表示词 $j$ 出现在词上下文的概率。</p>
</li>
<li><p>最小二乘法</p>
<p>回顾 <strong>skip-gram</strong> 模型，我们用 <strong>softmax</strong> 来计算词 $j$ 出现在词 $i$ 上下文的概率：</p>
<script type="math/tex; mode=display">
Q_{i j}=\frac{\exp \left(\vec{u}_{j}^{T} \vec{v}_{i}\right)}{\sum_{w=1}^{W} \exp \left(\vec{u}_{w}^{T} \vec{v}_{i}\right)}</script><p>其损失函数定义如下：</p>
<script type="math/tex; mode=display">
J=-\sum_{i \in \text {corpus}} \sum_{j \in \text {context}(i)} \log Q_{i j}</script><p>由于相同的单词i和j可以在语料库中多次出现，因此首先将i和j的相同值组合在一起更有效:</p>
<script type="math/tex; mode=display">
J=-\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i j} \log Q_{i j}</script><p>其中 $X_{ij}$ 即共现矩阵。交叉熵损失的一个显著缺点是它要求分布 $Q$ 被适当地规范化，这涉及到整个词汇表上昂贵的求和。相反，我们使用一个最小二乘目标，其中 $P$ 和 $Q$ 中的标准化因子被丢弃：</p>
<script type="math/tex; mode=display">
\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\hat{P}_{i j}-\hat{Q}_{i j}\right)^{2}</script><p>其中 $\hat{P}_{ij} = X_{ij}, \hat{Q}_{ij} = \exp(\vec{u}_j^T\vec{v}_i)$ 。但这也导致了新的问题：$X_{ij}$ 具有非常大的值，使优化变得困难。一个有效的办法就是给 $\hat{P}$ 和 $\hat{Q}$ 加一个 <strong>log</strong>：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{J} &=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\log (\hat{P})_{i j}-\log \left(\hat{Q}_{i j}\right)\right)^{2} \\
&=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\vec{u}_{j}^{T} \vec{v}_{i}-\log X_{i j}\right)^{2}
\end{aligned}</script><p>另一个观察是，权重因子 $X_i$ 不能保证是最优的。相反，我们引入了一个更一般的加权函数，我们也可以根据上下文单词自由选择:</p>
<script type="math/tex; mode=display">
\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} f\left(X_{i j}\right)\left(\vec{u}_{j}^{T} \vec{v}_{i}-\log X_{i j}\right)^{2}</script></li>
<li><p>结论</p>
<p>综上所述，GloVe 模型通过只训练单词共现矩阵中的非零元素，有效地利用了全局统计信息，生成了一个具有有意义子结构的向量空间。在相同的语料库、词汇、窗口大小和训练时间下，它在词汇类比任务上始终优于word2vec。它可以更快地获得更好的结果，而且无论速度如何，也可以获得最佳的结果。</p>
</li>
</ol>
<h2 id="4-FastText"><a href="#4-FastText" class="headerlink" title="4. FastText"></a>4. FastText</h2><p>模型结构：</p>
<p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcvv4e966bj30ha0bv3z3.jpg" alt="FastText"></p>
<ol>
<li><p>首先是权值矩阵 $A$ ，将单词 $x_1,x_2,\dots,x_{N-1},x_{N}$ 映射成词向量，然后再对这些词向量取平均，得到隐藏层的文本表示，作为线性分类器的输入。</p>
</li>
<li><p>通过线性分类器后的输出，再使用 <strong>softmax</strong> 函数 $f$ 得到预测类别的概率分布。</p>
</li>
<li><p>损失函数可表达为:</p>
<script type="math/tex; mode=display">
-\frac{1}{N} \sum_{n=1}^{N} y_{n} \log \left(f\left(B A x_{n}\right)\right)</script><p>该模型使用随机梯度下降法和线性衰减学习率来进行训练。</p>
</li>
</ol>
<p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/603c955b.html">https://gongjintao.com/post/603c955b.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/post/f5fe0882.html" rel="next" title="《统计学习方法》——决策树">
                <i class="fa fa-chevron-left"></i> 《统计学习方法》——决策树
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/post/51f80686.html" rel="prev" title="CNN IN NLP">
                CNN IN NLP <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Gong Jintao">
            
              <p class="site-author-name" itemprop="name">Gong Jintao</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">52</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-Embedding-Model"><span class="nav-text">Basic Embedding Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-NNLM-Neural-Network-Language-Model"><span class="nav-text">1. NNLM(Neural Network  Language Model)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Word2vec"><span class="nav-text">2.Word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-CBOW"><span class="nav-text">2.1 CBOW</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-skip-gram"><span class="nav-text">2.2 skip-gram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Negative-Sampling-负采样"><span class="nav-text">2.3 Negative Sampling(负采样)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Hierarchical-softmax-分层softmax"><span class="nav-text">2.4 Hierarchical softmax(分层softmax)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-GloVe-Global-Vector"><span class="nav-text">3. GloVe(Global Vector)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-FastText"><span class="nav-text">4. FastText</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gong Jintao</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  



  
    
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script>













  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>



  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  
  

<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 's5fvqKKJksk0PBu2LvJSV34O-gzGzoHsz',
    appKey: 'Kwu7wGQh8a4tGmuMJkdG8vqo',
    placeholder: 'Just go go',
    avatar: 'retro',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: 'zh-cn' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>
  <script>
    
    
      flOptions = {};
      
        flOptions.iconStyle = "box";
      
        flOptions.boxForm = "horizontal";
      
        flOptions.position = "middleRight";
      
        flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>


  

  

  

  
<script>
  $('.highlight').not('.gist .highlight').each(function(i, e) {
    var $wrap = $('<div>').addClass('highlight-wrap');
    $(e).after($wrap);
    $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function(e) {
      var code = $(this).parent().find('.code').find('.line').map(function(i, e) {
        return $(e).text();
      }).toArray().join('\n');
      var ta = document.createElement('textarea');
      var yPosition = window.pageYOffset || document.documentElement.scrollTop;
      ta.style.top = yPosition + 'px'; // Prevent page scroll
      ta.style.position = 'absolute';
      ta.style.opacity = '0';
      ta.readOnly = true;
      ta.value = code;
      document.body.appendChild(ta);
      const selection = document.getSelection();
      const selected = selection.rangeCount > 0 ? selection.getRangeAt(0) : false;
      ta.select();
      ta.setSelectionRange(0, code.length);
      ta.readOnly = false;
      var result = document.execCommand('copy');
      
        if (result) $(this).text('复制成功');
        else $(this).text('复制失败');
      
      ta.blur(); // For iOS
      $(this).blur();
      if (selected) {
        selection.removeAllRanges();
        selection.addRange(selected);
      }
    })).on('mouseleave', function(e) {
      var $b = $(this).find('.copy-btn');
      setTimeout(function() {
        $b.text('复制');
      }, 300);
    }).append(e);
  })
</script>


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300,"hOffset":80,"vOffset":-50},"mobile":{"show":false},"log":false,"tagMode":false});</script></body>
</html>
