<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>GongJintao&#39; Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://gongjintao.com/"/>
  <updated>2020-03-29T14:07:30.728Z</updated>
  <id>https://gongjintao.com/</id>
  
  <author>
    <name>Gong Jintao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Attention Based Model</title>
    <link href="https://gongjintao.com/post/4b37d30a.html"/>
    <id>https://gongjintao.com/post/4b37d30a.html</id>
    <published>2020-03-28T11:27:46.000Z</published>
    <updated>2020-03-29T14:07:30.728Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention-Based-Model"><a href="#Attention-Based-Model" class="headerlink" title="Attention Based Model"></a>Attention Based Model</h1><h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2><h3 id="早期的机器翻译"><a href="#早期的机器翻译" class="headerlink" title="早期的机器翻译"></a>早期的机器翻译</h3><ol><li><p>早期机器翻译是基于规则的，通过使用一个双语词典，将源语言的单词映射到目标语言对应的单词。</p></li><li><p>统计机器翻译：主要思想是从数据中学习一个概率模型。</p><p>假如要将法语翻译成英语。则需要找到一个在给定法语序列 $x$ 的条件下最合适的英文序列 $y$，即：</p><script type="math/tex; mode=display">\text{argmax}_y P(y|x)</script><p>通过贝叶斯定理，可以将上式分为两个部分，分别进行学习：</p><script type="math/tex; mode=display">= \text{argmax}_y P(x|y) P(y)</script><ol><li>$P(x|y)$ 即翻译模型，如何合理的翻译出单词和短语，需要从平行语料中学习。</li><li>$P(y)$ 即语言模型，如何写好流利的英语。从单语数据中学习。</li></ol></li></ol><p><strong>如何从平行语料库中学习翻译模型 $ P(x|y)$</strong> ：</p><p>进一步分解：</p><script type="math/tex; mode=display">P(x,a|y)</script><p>其中a是对齐方式，即源语言句子 $x$ 和目标语言句子 $y$之 间的字级的对应。</p><p>对齐方式有多种情况：</p><ol><li>可能有些单词在另一种语言中没有对齐单词</li><li>多对一</li><li>一对多</li><li>多对多</li></ol><p><strong>那么如何来计算最优值</strong>：</p><p>如果列举每一种可能的 $y$ ，然后计算概率，则计算量太大。我们可以用一种启发式的搜索算法来寻找最佳翻译，摒弃概率太低的假设。这个过程称为解码(decoding)</p><h2 id="神经机器翻译"><a href="#神经机器翻译" class="headerlink" title="神经机器翻译"></a>神经机器翻译</h2><ol><li>神经机器翻译是用单个神经网络来进行机器翻译</li><li>其神经网络结构被称为 <strong>seq2seq</strong> ，它包括两个 RNNs</li></ol><h3 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h3><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd694bx0vsj30us0kgq79.jpg" alt="Snipaste_2020-03-25_16-25-45.png"></p><p style="color:red">seq2seq 除了用于机器翻译，也可以用于其他方面，如文本摘要，对话系统，语法分析，代码生成</p><h3 id="训练-NMT"><a href="#训练-NMT" class="headerlink" title="训练 NMT"></a>训练 NMT</h3><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd9x1pbkq6j30uw0m4n0q.jpg" alt="image-20200328202816387.png"></p><h4 id="解码方法"><a href="#解码方法" class="headerlink" title="解码方法"></a>解码方法</h4><p><strong>贪婪解码(Greedy Decoding)</strong>：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gdakh1tk7cj30ep0ad75q.jpg" alt="image-20200328210623603.png"></p><p>即在每一步中取最可能的单词。存在的问题是可能当前的最大概率的单词对于翻译整个句子来讲不一定是最优的选择，但由于每次我们都做greedy的选择我们没机会选择另一条整体最优的路径。</p><p>为解决这一问题，一个常用的方法是<strong>beam search decoding</strong>。其<strong>主要思想</strong>是，其基本思想是在decoder的每一步，我们不仅仅是取概率最大的单词，而是保存k个当前最有可能的翻译假设，其中k称作beam size，通常在5到10之间。</p><p>假设 $y_1,\dots,y_t$ 的分数是其概率的对数来表示，即</p><script type="math/tex; mode=display">\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\log P_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} | x\right)=\sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><p>分数越高越好。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gdamgc7v9qj30ut0lxju9.jpg" alt="Snipaste_2020-03-29_11-09-00.png"></p><p>从上图可知当 Beam size = k = 2 时，每一步都会找得分最高的 2 个，直到最后，再进行回溯。</p><p>编码结束，一般是产生了<end>标识符，但每个假设产生<end>标识符的时间不同，所以一般整个结束是在到了时刻 T(T是一个预定义的截止值)或者产生了至少 n 个完整假设。(n 是一个预定义的截止值)。</end></end></p><p><strong>当我们有一组完全假设，如何选择分数最高的那一个？</strong></p><p>虽然分数越高越好，但求和中的每一项都是负数，这会导致长的翻译分数更低，所以最后在选取整句概率最大的翻译时，要对分数做归一化。</p><script type="math/tex; mode=display">\frac{1}{t} \sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} | y_{1}, \ldots, y_{i-1}, x\right)</script><h3 id="NML的优缺点"><a href="#NML的优缺点" class="headerlink" title="NML的优缺点"></a>NML的优缺点</h3><p><strong>优点：</strong></p><p>相比于 SMT ，NMT 有以下优点：</p><ol><li>更好的效果<ul><li>更加流畅</li><li>对上下文有更好的利用</li><li>对短语相似度有更好的利用</li></ul></li><li>端到端优化单个神经网络：<ul><li>不用单独优化某个子组件</li></ul></li><li>所需的人力更少<ul><li>不需要特征工程</li><li>对于所有的语言对可以用同样的方法</li></ul></li></ol><p><strong>缺点：</strong></p><p>相比于 SMT，NMT 有以下缺点：</p><ol><li>NMT 解释性较差：<ul><li>难以 debug</li></ul></li><li>NMT较难控制：<ul><li>无法轻松指定翻译规则或指导原则</li><li>存在安全问题</li></ul></li></ol><h2 id="评估机器翻译"><a href="#评估机器翻译" class="headerlink" title="评估机器翻译"></a>评估机器翻译</h2><p><strong>BLEU(Bilingual Evaluation Understudy)</strong></p><p>BLEU 将机器翻译的结果和人工翻译的结果进行比较，然后基于以下条件计算得分：</p><ol><li>n-gram 精度（通常是1，2，3 和 4-grams）</li><li>对太短的系统翻译进行惩罚</li></ol><p>BLEU 有用但是不够完美：</p><ol><li>翻译一个句子有很多有效的方法</li><li>所以一个好的翻译可以得到一个很差的BLEU分数，因为它与人类翻译有很低的n-gram重叠</li></ol><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>seq2seq 结构的瓶颈问题：在编码阶段，需要对所输入的所有信息都encode到encoder的最后一个hidden state上，这通常是不现实的，因此引入Attention来解决这个瓶颈。</p><p>主要思想：在解码器的每个步骤中，使用与编码器的直接连接来聚焦源序列的特定部分。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gdavu3d7xij31da0j9q7y.jpg" alt="Snipaste_2020-03-29_16-32-27.png"></p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gdavyq44o6j312o0q2td7.jpg" alt="Snipaste_2020-03-29_16-38-16.png"></p><ol><li><ul><li><p>先用解码器的每个状态于编码器的各状态做点积得到 Attention scores</p></li><li><p>然后在 Attention scores 上取 softmax 得到其概率分布 Attention distribution</p></li><li>用 Attention distribution 求 encoder hidden state 的加权和，即 Attention output(主要包含来自被高度注意的隐藏状态的信息)</li></ul></li><li><ul><li>将 Attention output 与 decoder hidden state 连接，然后用其计算得到 $\hat{y}$(有时我们以从上一步中提取 Attention output，并将其输入解码器)</li></ul></li></ol><p>公式说明：</p><ol><li><p>encoder 隐变量状态 $h_1,\dots,h_N \in \mathbb{R}^h$</p></li><li><p>第 $t$ 个时间步的decoder 隐变量状态 $s_t \in \mathbb{R}^h$</p></li><li><p>得到第 $t$ 时间步的 attention scores：</p><script type="math/tex; mode=display">e^{t}=\left[s_{t}^{T} h_{1}, \ldots, s_{t}^{T} h_{N}\right] \in \mathbb{R}^{N}</script></li><li><p>用 softmax 求 attention distribution：</p><script type="math/tex; mode=display">\alpha^{t}=\operatorname{softmax}\left(e^{t}\right) \in \mathbb{R}^{N}</script></li><li><p>用 $\alpha^t$ 作为权重，来计算 encoder hidden state 的加权和，得到 attention output：</p><script type="math/tex; mode=display">\boldsymbol{a}_{t}=\sum_{i=1}^{N} \alpha_{i}^{t} \boldsymbol{h}_{i} \in \mathbb{R}^{h}</script></li><li><p>最后，将 attention output 与 decoder hidden state 连接起来:</p><script type="math/tex; mode=display">\left[\boldsymbol{a}_{t} ; \boldsymbol{s}_{t}\right] \in \mathbb{R}^{2 h}</script></li></ol><p><strong>Attention 的好处</strong>：</p><ol><li>Attention 提升了 NMT 性能<ul><li>让 decoder 关注源序列确定部分是非常有用的</li></ul></li><li>Attention 解决了瓶颈问题<ul><li>Attention 允许 decoderr 直接关注源序列，可以绕过瓶颈</li></ul></li><li>Attention 有助于解决梯度消失问题<ul><li>提供了直接连接远处状态的捷径</li></ul></li><li>Attention 提供了一些可解释性<ul><li>通过 attention distribute，可以知道 decoder 关注的重点</li><li>可以轻易得到(软)对齐，因为无需特意训练一个对齐系统</li><li>网络自动学习对齐</li></ul></li></ol><p>Attention 机制不仅可以用于seq2seq，也可以用于其它结构和任务。一个 Attention 更一般的定义如下：</p><blockquote><p>给定一组向量值(value)和一个向量查询(query)，注意力机制是一种根据查询计算值的加权和的技术。</p></blockquote><p><strong>Attention 的一般步骤</strong>：</p><ol><li><p>有向量值(values) $h_1,\dots,h_N \in \mathbb{R}^{d_1}$ 和 向量查询(query) $s \in \mathbb{R}^{d_2}$ 、</p></li><li><p>计算 attention scores $e \in \mathbb{R}^N$</p></li><li><p>求 softmax 得到 attention distribution $\alpha$:</p><script type="math/tex; mode=display">\alpha=\operatorname{softmax}(e) \in \mathbb{R}^{N}</script></li><li><p>用 attention distribution 来计算 values 的加权和：</p><script type="math/tex; mode=display">\boldsymbol{a}=\sum_{i=1}^{N} \alpha_{i} \boldsymbol{h}_{i} \in \mathbb{R}^{d_{1}}</script><p>从而获得了 attention output $\boldsymbol{a}$，有时候也称为上下文向量(context vector)</p><p>在计算第二步的时候，有多种方法因此有多种 attention 变体：</p></li><li><p>Basic dot-product attention：$e_i = s^T h_i \in \mathbb{R}$，<span style="color:red">注：此处假设 $d_1 = d_2$ </span></p></li><li>Multiplicative attention: $e_i = s^TWh_i \in \mathbb{R}$，其中 $W \in \mathbb{R}^{d_2 \times d_1}$ 是权值矩阵</li><li>Additive attention：$e_i = v^T \tanh(W_1h_i +W_2s) \in \mathbb{R}$<ul><li>其中 $W_1 \in \mathbb{R}^{d_3 \times d_1} ， W_2 \in \mathbb{R}^{d_3 \times d_2}$ 是权值矩阵，$v \in \mathbb{R}^{d_3}$ 是权值向量</li><li>$d_3$ 是一个超参数</li></ul></li></ol><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/undefined.html">https://gongjintao.com/post/undefined.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Attention-Based-Model&quot;&gt;&lt;a href=&quot;#Attention-Based-Model&quot; class=&quot;headerlink&quot; title=&quot;Attention Based Model&quot;&gt;&lt;/a&gt;Attention Based Model&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="https://gongjintao.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://gongjintao.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RNN IN NLP</title>
    <link href="https://gongjintao.com/post/9994ec50.html"/>
    <id>https://gongjintao.com/post/9994ec50.html</id>
    <published>2020-03-24T11:27:46.000Z</published>
    <updated>2020-03-29T14:08:50.691Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RNN-IN-NLP"><a href="#RNN-IN-NLP" class="headerlink" title="RNN IN NLP"></a>RNN IN NLP</h1><h2 id="RNN-Language-Model"><a href="#RNN-Language-Model" class="headerlink" title="RNN Language Model"></a>RNN Language Model</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd3xdmx4jtj30v30nadjx.jpg" alt="Snipaste_2020-03-23_16-08-01.png"></p><ol><li><p>首先是词嵌入层。</p><script type="math/tex; mode=display">e^{(t)} = Ex^{(t)}</script><p>其中$x^{(t)} \in \mathbb{R}^{|V|}$代表单词或单词的one-hot编码。</p></li><li><p>然后是隐藏层单元，每个隐藏层单元的状态都等于对应的词向量的加权再加上其前一个隐藏层状态的加权之和。再通过激活函数。即：</p><script type="math/tex; mode=display">\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{e} \boldsymbol{e}^{(t)}+\boldsymbol{b}_{1}\right)</script><p>其中 $h^{(0)}$ 是初始的隐藏层状态。</p><font color="red">注：其中 $W_h$ 和 $W_e$ 在每一时间步都是相同的。</font></li><li><p>最后的输出概率，如下：</p><script type="math/tex; mode=display">\hat{\boldsymbol{y}}^{(t)}=\operatorname{softmax}\left(\boldsymbol{U} \boldsymbol{h}^{(t)}+\boldsymbol{b}_{2}\right) \in \mathbb{R}^{|V|}</script></li></ol><h3 id="模型优缺点"><a href="#模型优缺点" class="headerlink" title="模型优缺点"></a>模型优缺点</h3><ol><li>优点：<ul><li>可以处理任意长度的输入</li><li>理论上计算第 $t$ 步的值可以使用前面许多步的信息</li><li>输入规模的增加不会增加模型规模</li><li>对于每一个时间步所使用的权重参数是一样的，保证了对称性。</li></ul></li><li>缺点：<ul><li>递归计算速度较慢</li><li>在实践中，很难从前面的许多步骤获取信息</li></ul></li></ol><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><ol><li><p>输入：一个大规模文本语料库，$x^{(1)},\dots,x^{(T)}$</p></li><li><p>输出：每个单词的概率，$\hat{y}^{(t)}$</p></li><li><p>对于每个时间步的损失函数可定义为交叉熵损失函数：</p><script type="math/tex; mode=display">J^{(t)}(\theta)=C E\left(\boldsymbol{y}^{(t)}, \hat{\boldsymbol{y}}^{(t)}\right)=-\sum_{w \in V} \boldsymbol{y}_{w}^{(t)} \log \hat{\boldsymbol{y}}_{w}^{(t)}=-\log \hat{\boldsymbol{y}}_{x_{t+1}}^{(t)}</script><p>其中 $y^{(t)}$ 即单词 $x^{(t+1)}$ 的one-hot编码。</p></li><li><p>对整个训练集的所有损失函数取平均得到整体损失：</p><script type="math/tex; mode=display">J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{x_{t+1}}^{(t)}</script><p>训练过程如图所示:</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd3yx2fqfij30ui0mitbs.jpg" alt></p></li></ol><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd3yx2hzqbj30u40me77e.jpg" alt></p><ol><li><p>模型的反向传播：</p><p>每一个时间步的损失函数对 $W_h$ 求导结果为（这是多变量求导的链式法则)：</p><script type="math/tex; mode=display">\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}=\left.\sum_{i=1}^{t} \frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}\right|_{(i)}</script><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd3zb7tr6ej30tk0mdacq.jpg" alt="Snipaste_2020-03-23_17-15-11.png"></p></li></ol><h3 id="模型的评估"><a href="#模型的评估" class="headerlink" title="模型的评估"></a>模型的评估</h3><p>通常使用<strong>困惑度</strong>来衡量语言模型的好坏，主要思想是，句子概率越大，困惑度越小，模型越好：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd3zhoyi9hj30m506xq3x.jpg" alt="Snipaste_2020-03-23_17-21-18.png"></p><p>RNN模型中困惑度也等于交叉熵损失函数的指数:</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd3zjqqvfbj30iq03e3ys.jpg" alt="Snipaste_2020-03-23_17-23-26.png"></p><p>而RNN模型对比 n-gram 模型有更小的困惑度。</p><h2 id="梯度消失和梯度爆炸问题"><a href="#梯度消失和梯度爆炸问题" class="headerlink" title="梯度消失和梯度爆炸问题"></a>梯度消失和梯度爆炸问题</h2><p><strong>直觉</strong>：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd47d8mo64j30ud0m9wha.jpg" alt="Snipaste_2020-03-23_21-53-52.png"></p><p><strong>推导</strong>：</p><p>因为：$\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)$ </p><p>因此：$\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}}=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right) \boldsymbol{W}_{h}$</p><p>则第 $i$ 步的损失对前第 $j$ 步隐藏状态 $h^{(j)}$ 求导为：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}} &=\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} \prod_{j<t \leq i} \frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}} \\&=\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} \boldsymbol{W}_{h}^{(i-j)} \prod_{j<t \leq i} \operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right)\end{aligned}</script><p>考虑上式的 L2 范数：</p><script type="math/tex; mode=display">\left\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}}\right\| \leq\left\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}}\right\|\left\|\boldsymbol{W}_{h}\right\|^{(i-j)} \prod_{j<t \leq i}\left\|\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right)\right\|</script><p>可证明 $W_h$ 的最大特征值小于1 ，则 $\left|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}}\right|$ 将呈指数级收缩，即梯度消失问题；同样的当 $W_h$ 的最大特征值大于1，则呈指数级增加，即梯度爆炸问题。</p><p><strong>梯度消失带来的问题</strong>：</p><p>它导致我们无法分辨 $t$ 时刻与 $k$ 时刻究竟是数据本身毫无关联还是由于梯度消失而导致我们无法捕捉到这一关联。这就导致了我们只能学习到近程的关系而不能学习到远程的关系，会影响很多语言处理问题的准确度。</p><p><strong>梯度爆炸带来得问题：</strong></p><p>如果梯度过大，则在梯度下降得更新步也变得很大。</p><p>这会导致错误得更新步骤，使得参数配置错误，即损失函数很大。甚至导致网络中出现 <code>INF</code> 或者 <code>NaN</code> ，以至于要在更早的检查点重新训练。</p><p><strong>梯度爆炸的解决办法：</strong></p><p>梯度剪枝(Gradient clipping)，伪代码如下:</p><script type="math/tex; mode=display">\begin{array}{l}\text { Algorithm 1 Pseudo-code for norm clipping } \\\hline \hat{\mathbf{g}} \leftarrow \frac{\partial \mathcal{E}}{\partial \theta} \\\text { if }\|\hat{\mathbf{g}}\| \geq \text { threshold then } \\\hspace{1em}\hat{\mathbf{g}} \leftarrow \frac{\text { threshold }}{\|\mathbf{g}\|} \hat{\mathbf{g}} \\\text { end if }\end{array}</script><p>如图所示:</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5batccbxj30iu09wq5s.jpg" alt="Snipaste_2020-03-24_20-55-10.png"></p><p><strong>梯度消失问题的解决办法：</strong></p><p>主要问题是 RNN 很难学会多个时间段内保存的信息。在一个普通的 RNN 网络中，隐藏层状态一直被下式重写：</p><script type="math/tex; mode=display">\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}\right)</script><p>所以其基本思路是设置一些存储单元来更有效的进行长程信息的存储。</p><h2 id="LSTM-Long-Short-Term-Memory"><a href="#LSTM-Long-Short-Term-Memory" class="headerlink" title="LSTM(Long Short-Term Memory)"></a>LSTM(Long Short-Term Memory)</h2><p>主要特点是：</p><ol><li>在每个时间步 $t$ ,有隐藏层状态 $h^{(t)}$ 和单元状态 $c^{(t)}$</li><li>$h^{(t)}$ 和 $c^{(t)}$ 都是长为 $n$ 的向量</li><li>$c^{(t)}$ 用来存储长期信息</li><li>LSTM可以擦除，写入和读取单元中的信息<br>信息的擦除/写入/读取的选择由三个相应的<strong>门</strong>来控制</li><li>门也是长为 $n$ 的向量。</li><li>在每一个时间步，门的每个元素可以是打开(1)、关闭(0)或者介于两者之间的某个位置。</li><li><p>门是动态的：其值是根据上下文计算的。</p><p>假设输入序列是 $x^{(t)}$ ，则LSTM的公式如下：<br><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5dn7j9dkj30tq0i7jw4.jpg" alt="Snipaste_2020-03-24_22-16-34.png"><br><strong>LSTM结构图：</strong></p></li></ol><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5du57mbjj30t00qn7a0.jpg" alt="Snipaste_2020-03-24_22-23-14.png"></p><ol><li>新的记忆单元：$\tilde{c}_{t}$ 通过将新的输入 $x_{t}$ 与代表之前的 context 的隐藏层单元 $h_{（t-1）}$ 结合成新的记忆</li><li>输入门：$i_t$ 决定新的信息是否由保留的价值</li><li>遗忘门：$f_t$ 决定之前的记忆单元(cell state)有多少保留的价值</li><li>最终的记忆单元：$c_t$ 通过遗忘门 $f_t$ 和前一个记忆单元 $c_{(t-1)}$ 作元素积得到 $c_{(t-1)}$ 需要传递下去的信息，并与输入门 $i_t$ 和新的记忆单元$c_t$的元素积求和。极端情况即遗忘门的值为 1，则之前的记忆单元的全部信息都会传递下去，<strong>使得梯度消失问题不复存在</strong></li><li>输出门：$o_t$ 决定了记忆单元有多少信息需要保存在隐藏层状态中</li></ol><p>LSTM不能保证不存在消失/爆炸梯度，但它确实为模型提供了一种更容易学习远程依赖关系的方法。</p><h2 id="GRU-Gated-Recurrent-Units"><a href="#GRU-Gated-Recurrent-Units" class="headerlink" title="GRU(Gated Recurrent Units)"></a>GRU(Gated Recurrent Units)</h2><p>对 LSTM 做了简单的改变。每个时间步 $t$ 只有输入 $x^{(t)}$ 和 $h^{(t)}$ 。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5f06z6kkj30rg0drdjc.jpg" alt="Snipaste_2020-03-24_23-03-27.png"></p><ol><li>新的记忆生成：$\tilde{h}_t$ 是由一个新的输入 $x_t$ 和上一个隐藏状态 $h_{t-1}$ 组合而成</li><li>复位门：复位信号 $r_t$ 负责确定 $h_{t-1}$ 对 $ \tilde{h}_t $ 的重要程度。如果复位信号发现 $h_{t-1}$ 对于新的记忆 $ \tilde{h}_t $ 是无关的，则它能够完全减小过去的隐藏状态</li><li>更新门：更新信号 $u_t$ 负责将多少  $h_{t-1}$  转结到下一个状态。例如，如果 $u_{t} \approx 1$ ，则 $h_{t-1}$ 几乎都被复制到 $h_t$，相反，则新的记忆 $\tilde{h}_{t}$ 则被传递给下一个隐藏状态 $ h_t $ </li><li><p>隐藏状态： $h_t$ 即在更新门的取舍下，由上一个隐藏状态 $h_{t-1} $和新的记忆生成 $\tilde{h}_t$ 组合而成</p><p>GRU的结构图如下：</p></li></ol><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5y6x24l0j30sc0e2acz.jpg" alt="Snipaste_2020-03-25_10-04-07.png"></p><p style="color:red">梯度消失仅仅局限于RNN结构吗？实际上对于深度神经网络，尤其层数较多时，由于链式法则以及非线性函数的选择，梯度消失问题也会出现，导致底层的layer学习速度很慢，只不过由于RNN中由于不断的乘以相同的权重矩阵导致其梯度消失问题更显著。</p><p style="color:red">对于非RNN的深度神经网络如前馈神经网络与卷积神经网络，为了解决梯度消失问题，通常可以引入更直接的联结使得梯度更容易传递。</p><p>例如在ResNet中，通过引入与前层输入一样的联结使得信息更有效的流动，这使得我们可以能够训练更深层的模型。这一联结被称作Residual Connection或Skip Connection</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5z70qf1mj30e9087751.jpg" alt></p><p>在DenseNet中，我们可以将几层之间全部建立直接联结，形成dense connection。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd5z70t78fj30bk09f40m.jpg" alt></p><h2 id="双向-RNN-Bidirectional-RNNs"><a href="#双向-RNN-Bidirectional-RNNs" class="headerlink" title="双向 RNN(Bidirectional RNNs)"></a>双向 RNN(Bidirectional RNNs)</h2><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd6367ftj1j30ul0l0n1f.jpg" alt="Snipaste_2020-03-25_12-58-00.png"></p><p>如上图所示，如果我们只看到terribly的上文，我们会认为这是一个负面的词，但如果结合下文exciting，就会发现terribly是形容exciting的激烈程度，在这里是一个很正面的词，所以我们既要结合上文又要结合下文来处理这一问题。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd63ivtp2uj30op0iitbh.jpg" alt="Snipaste_2020-03-25_13-12-03.png"></p><p>可知在双向 RNN 中的每个时间步 t 有:</p><p> <span style="color:red">Forward RNN：</span> $\overrightarrow{h}^{(t)} = \text{RNN}_{\text{FW}} (\overrightarrow{h}^{(t-1)},x^{(t)})$</p><p><span style="color:green">Backward RNN：</span> $\overleftarrow{h}^{(t)} = \text{RNN}_{\text{BW}}(\overleftarrow{h}^{(t+1)},x^{(t)})$</p><p>上式中的 RNN ，可以是普通RNN，LSTM或GRU，而且通常正向和反向的RNN拥有不同的权重参数。</p><p>然后将隐藏状态连接：$h^{(t)} = [\overrightarrow{h}^{(t)},\overleftarrow{h}^{(t)}]$ 将其传递给网络的下一部分。</p><p> 注：</p><ol><li>双向 RNN 仅在你可以访问整个输入序列时才适用。并不适用于语言模型，因为语言模型只需知道左边的序列即可。</li><li>如果可以访问整个序列，双向 RNN 可以作为默认使用。</li></ol><h2 id="多层-RNN"><a href="#多层-RNN" class="headerlink" title="多层 RNN"></a>多层 RNN</h2><p>RNN 仅在一个维度上延深，即时间维度上的展开。我们可以在另一个维度上延深，即通过应用多层 RNN。</p><p>这允许网络学习更加复杂的表示，低层 RNN用以学习底层表示，高层的RNN 可以用来学习高层表示。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd65a4cc16j30os0i0mz5.jpg" alt="Snipaste_2020-03-25_14-12-54.png"></p><p>有较好表现的 RNN 模型通常是多层的。例如，对于机器翻译，2到4层最适合编码器RNN，4层最适合解码器RNN。但在训练较深层的 RNN 时，需要 skip-connections/dense-connections。</p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/undefined.html">https://gongjintao.com/post/undefined.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RNN-IN-NLP&quot;&gt;&lt;a href=&quot;#RNN-IN-NLP&quot; class=&quot;headerlink&quot; title=&quot;RNN IN NLP&quot;&gt;&lt;/a&gt;RNN IN NLP&lt;/h1&gt;&lt;h2 id=&quot;RNN-Language-Model&quot;&gt;&lt;a href=&quot;#RNN
      
    
    </summary>
    
      <category term="NLP" scheme="https://gongjintao.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://gongjintao.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>CNN IN NLP</title>
    <link href="https://gongjintao.com/post/51f80686.html"/>
    <id>https://gongjintao.com/post/51f80686.html</id>
    <published>2020-03-22T11:27:46.000Z</published>
    <updated>2020-03-29T14:08:30.240Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP中的CNN"><a href="#NLP中的CNN" class="headerlink" title="NLP中的CNN"></a>NLP中的CNN</h1><p>（Convolutional Neural Networks for Sentence Classification)</p><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>假设 $x_i \in \mathbb{R}^k$ 是一个 $k$ 维的词向量，对应句子中的第 $i$ 个单词。则一个长为 $n$ 的句子可以表示为:</p><script type="math/tex; mode=display">\mathbf{x}_{1: n}=\mathbf{x}_{1} \oplus \mathbf{x}_{2} \oplus \ldots \oplus \mathbf{x}_{n}</script><p>其中 $\oplus$ 是连接符。通常，用 $\mathbf{X}_{i:i+j}$ 来表示单词 $\mathbf{x_i,x_{i+1},\cdots,x_{i+j}}$ 的连接。卷积操作中的卷积核 $ \mathbf{w} \in \mathbb{R}^{hk}$，应用于一个窗口大小为 $h$ 个词上以产生一个特征。即：</p><script type="math/tex; mode=display">c_i = f(\mathbf{w} \cdot \mathbf{x}_{i:i+h-1} + b)</script><p>其中 $b \in  \mathbb{R}$ 是偏置项。则在整个句子的每个 $h$ 大小的窗口上$\{\mathbf{x}_{1：h},\mathbf{x}_{2:h+1},\cdots, \mathbf{x}_{n-h+1:n}\}$可以产生一张特征图如下：</p><script type="math/tex; mode=display">\mathbf{c} = [c_1,c_2,\cdots,c_{n-h+1}]</script><p>其中 $\mathbf{c} \in \mathbb{R}^{n-h+1}$。 </p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd2ha9kg2ij31b90f3my5.jpg" alt="convolution.png"></p><p>上图即一个句子在一个 $h$ 为 3 的卷积核下的卷积操作得到的特征图。也可以在卷积操作中填充 padding。</p><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>最大池化即选出特征图 $\mathbf{c} 中最大的值$。</p><p>均值池化即取特征图的均值</p><h2 id="多通道和多卷积核"><a href="#多通道和多卷积核" class="headerlink" title="多通道和多卷积核"></a>多通道和多卷积核</h2><p>即可以使用多个卷积核（有不同的窗口$h$）来代表 一元语法，二元语法，三元语法等。也可以对于同样的 $h$ 有多个卷积核</p><p>下图即不同的 CNN 模型：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd2ifky05oj311k0hzgoh.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gd2iftugq8j31d91gcwo1.jpg" alt></p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>Dropout: 建立一个伯努利随机变量的 masking 向量 $r$ ，每个变量以 $p$ 的概率为1。则在训练过程中的输出为：</p><script type="math/tex; mode=display">y = \text{softmax} (W^{(S)}(r \circ z)+b)</script><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/undefined.html">https://gongjintao.com/post/undefined.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;NLP中的CNN&quot;&gt;&lt;a href=&quot;#NLP中的CNN&quot; class=&quot;headerlink&quot; title=&quot;NLP中的CNN&quot;&gt;&lt;/a&gt;NLP中的CNN&lt;/h1&gt;&lt;p&gt;（Convolutional Neural Networks for Sentence Cl
      
    
    </summary>
    
      <category term="NLP" scheme="https://gongjintao.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://gongjintao.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Basic Embedding Model</title>
    <link href="https://gongjintao.com/post/603c955b.html"/>
    <id>https://gongjintao.com/post/603c955b.html</id>
    <published>2020-03-17T11:27:46.000Z</published>
    <updated>2020-03-29T14:08:21.186Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Basic-Embedding-Model"><a href="#Basic-Embedding-Model" class="headerlink" title="Basic Embedding Model"></a>Basic Embedding Model</h1><h2 id="1-NNLM-Neural-Network-Language-Model"><a href="#1-NNLM-Neural-Network-Language-Model" class="headerlink" title="1. NNLM(Neural Network  Language Model)"></a>1. NNLM(Neural Network  Language Model)</h2><blockquote><p>不同与 $n-gram$ 统计语言模型的一种神经网络语言模型，并且可以在计算出句子联合概率的同时，求出词向量，但是速度较慢。</p></blockquote><p><img src="https://i.loli.net/2020/03/01/coV9SEJCf2v5xsr.png" alt></p><p>模型可以分成两部分理解:</p><blockquote><ol><li><p>首先是一个线性的embedding 层。它将输入的 n−1 个one−hot词向量，通过一个共享的$|V| \times $的矩阵$C$，映射为 n−1 个分布式的词向量。其中，$|V|$是词典的大小，$m$ 是embedding向量的维度（一个先验参数）。$C $矩阵里存储了要学习的词向量。</p></li><li><p>其次是一个简单的前向反馈神经网络 g 。它由一个tanh 隐层和一个softmax 输出层组成。通过将embedding 层输出的N−1 个词向量映射为一个长度为 $|V| $ 的概率分布向量，从而对词典中的word 在输入 context 下的条件概率做出预估：</p><script type="math/tex; mode=display">\hat{P}\left(w_{t} | w_{1}^{t-1}\right) \approx f\left(i, w_{t-1}, \cdots, w_{t-n+1}\right)=g\left(i, C\left(w_{t-1}\right), \cdots, C\left(w_{t-n+1}\right)\right)</script></li></ol></blockquote><p>然后可以通过最小化一个cross−entropy 的正则化损失函数来调整模型的参数$\theta =(C,w)$：</p><script type="math/tex; mode=display">L = \frac{1}{T}\sum_t \log f(w_t,w_{t-1},\cdots,w_{t-n+1},\theta) + R(\theta)</script><p>其中 $R(\theta)$ 为正则项。</p><p>上述模型中，自由参数的个数仅仅和$|V|$ 线性相关。然后神经网络模型通过计算如下式子，即<code>softmax</code> 层，来得到输出概率：</p><script type="math/tex; mode=display">\hat{P}(w_t|w_{t-1},\cdots,w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}</script><p>其中$y_i$ 是每个单词$i$ 的未规范化对数概率，由下式计算：</p><script type="math/tex; mode=display">y = b + Wx + U\tanh(d+Hx)</script><p>$\tanh$逐元素计算的。其中 $W$ 是可选为 0 的，$x$ 则是输入的单词向量连接而成的：</p><script type="math/tex; mode=display">x=\left(C\left(w_{t-1}\right), C\left(w_{t-2}\right), \cdots, C\left(w_{t-n+1}\right)\right)</script><p>令 $h$ 为隐藏单元的数量，$m$ 是每个词的词向量维度。</p><p>参数$\theta = (b,d,W,U,H,C)$说明：</p><blockquote><ol><li>$W$ 为词特征层到输出层权的值矩阵，大小为 $|V| \times (n-1)m$，当没有直接连接词特征与输出层时，设置为 0</li><li>$b$ 为输出偏置矩阵，大小为 $|V| $</li><li>$d$ 为隐藏层偏置矩阵，大小为 $h$</li><li>$U$ 为隐藏层到输出层的权值矩阵，大小为 $|V| \times h$</li><li>$H$ 为隐藏层的权值矩阵，大小为 $h \times (n-1)m$</li><li>$C$ 即嵌入矩阵(词向量)，大小为 $|V| \times m$</li></ol></blockquote><p>通过随机梯度下降法的每一步迭代为：</p><script type="math/tex; mode=display">\theta \leftarrow \theta+\varepsilon \frac{\partial \log \hat{P}\left(w_{t} | w_{t-1}, \cdots w_{t-n+1}\right)}{\partial \theta}</script><h2 id="2-Word2vec"><a href="#2-Word2vec" class="headerlink" title="2.Word2vec"></a>2.Word2vec</h2><p>word2vec有两种常用算法：</p><blockquote><ol><li>continuous bag-of-words (CBOW) :根据词向量从周围语境中预测中心词</li><li>skip-gram: 从中心词预测上下文词的分布（概率)</li></ol></blockquote><p>两种训练方法：</p><blockquote><ol><li>negative sampling(负采样)：采样负样例</li><li>hierarchical softmax(分层softmax)：使用一个有效的树结构来计算所有词的概率</li></ol></blockquote><h3 id="2-1-CBOW"><a href="#2-1-CBOW" class="headerlink" title="2.1 CBOW"></a>2.1 CBOW</h3><blockquote><ol><li><p>首先用 <code>one-hot</code> 编码来表示上下文单词</p></li><li><p>定义两个矩阵：$ \mathcal{V} \in \mathbb{R}^{n \times|V|} 和 \mathcal{U} \in \mathbb{R}^{|V|\times n}$</p><p>$\mathcal{V}$ 是输入矩阵，其第 $i$ 列 $v_i$ 是词 $w_i$ 的 $n$ 维嵌入向量，用来表示上下文；</p><p>$\mathcal{U}$ 是输出矩阵，其第 $j$ 行 $u_i$ 是词 $w_j$ 的 $n$ 维嵌入向量，用来表示中心词；</p><p>所以对于每个词都学习了两个向量。</p></li></ol></blockquote><p><strong>具体步骤</strong>：</p><ol><li><p>对<code>one-hot</code>编码来表示输入上下文:</p><script type="math/tex; mode=display">m: (x^{(c-m)},\dots,x^{(c-1)},x^{(c+1)},\dots,x^{(c+m)} \in \mathbb{R}^{|V|})</script></li><li><p>求得上下文的嵌入向量:</p><script type="math/tex; mode=display">(v_{c-m}=\mathcal{V}x^{(c-m)},v_{c-m+1}=\mathcal{V}x^{(c-m+1)},\dots,v_{c+m}=\mathcal{V}x^{(c+m)} \in \mathbb{R}^n)</script></li><li><p>对这些向量求平均值: </p><script type="math/tex; mode=display">\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^{n}</script></li><li><p>生成记分向量$z$，因为点积越高，相似度越高，为了达到较高的分数，$z$ 会把相似的词推的更近：</p><script type="math/tex; mode=display">z = \mathcal{U}\hat{v} \in \mathbb{R}^{|V|}</script></li><li><p>将分数转化成概率</p><script type="math/tex; mode=display">\hat{y} = \text{softmax}(z) \in \mathbb{R}^{|V|}</script></li><li><p>生成的概率$\hat{y} \in \mathbb{R}^{|V|}$ 与真实概率$y \in \mathbb{R}^{|V|}$相对比</p></li></ol><p>求解两个矩阵 $\mathcal{U} 和 \mathcal{V}$ 的训练过程：</p><ol><li>首先定义损失函数为交叉熵函数：<script type="math/tex; mode=display">H(\hat{y},y) = -\sum_{j=1}^{|V|}y_i \log(\hat{y_j})</script>因为 $y$  是<code>one-hot</code> 向量，因此损失函数可以简化为：<script type="math/tex; mode=display">H(\hat{y},y) = -y_i \log(\hat{y_i})</script>所以最终的模型即：<script type="math/tex; mode=display">\begin{aligned}\text { minimize } J &=-\log P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\&=-\log P\left(u_{c} | \hat{v}\right) \\&=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \\&=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)\end{aligned}</script></li><li><p>然后用随机梯度下降法来更新所有的参数 $u_i,v_i$。</p><p>基本模型图如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcqwxeaugoj30an0d376c.jpg" alt="image-20200312095845036.png"></p></li></ol><h3 id="2-2-skip-gram"><a href="#2-2-skip-gram" class="headerlink" title="2.2 skip-gram"></a>2.2 skip-gram</h3><p>过程与 <strong>CBOW</strong> 相反，矩阵$ \mathcal{V} 和 \mathcal{U}$ 定义和上面相同</p><p><strong>具体步骤</strong>：</p><ol><li><p>用<code>one-hot</code> 编码来表示中心词 $x \in \mathbb{R}^{|V|}$</p></li><li><p>得到中心词的嵌入单词向量 $v_c = \mathcal{V}x \in \mathbb{R}^n$</p></li><li><p>计算得分向量 $z = \mathcal{U}v_c $</p></li><li><p>将得分向量用 <strong>softmax</strong>  转换成概率</p><script type="math/tex; mode=display">\hat{y} = \text{softmax}(z)</script><p>$\hat{y}_{c-m}, \ldots, \hat{y}_{c-1}, \hat{y}_{c+1}, \ldots, \hat{y}_{c+m}$ 即对应上下文每个单词的概率</p></li><li><p>与真实单词$y^{(c-m)}, \ldots, y^{(c-1)}, y^{(c+1)}, \ldots, y^{(c+m)}$ 做对比</p></li></ol><p><strong>定义损失函数：</strong></p><p>与 <strong>CBOW</strong> 不同之处在于，此处用到了朴素贝叶斯假设，即在给定中心词的条件下，输出词之间是相互独立的，所以有：</p><script type="math/tex; mode=display">\begin{aligned}\text{minimize} J &=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \\&=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \\& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \\& =-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^{T} v_{c}\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)} \\&=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)\end{aligned}</script><p>最后用随机梯度下降求解。</p><p>基本模型如下图所示：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcqz161w3xj30ca0e8q5a.jpg" alt="image-20200312111300751.png"></p><h3 id="2-3-Negative-Sampling-负采样"><a href="#2-3-Negative-Sampling-负采样" class="headerlink" title="2.3 Negative Sampling(负采样)"></a>2.3 Negative Sampling(负采样)</h3><p>因为在计算目标函数的时候，$|V|$上的求和计算量很大。因此可以近似计算它。对于每一个训练步骤，我们不必在整个词汇表上循环，只需举几个反面的例子，我们从噪声分布(Pn(w))中采样，其概率与词汇的频率顺序相匹配。</p><ol><li><p>用 $(w,c)$ 来表示单词和上下文</p></li><li><p>$P(D=1 |w,c)$ 表示$(w,c)$来自与语料库中的概率，$P(D=0 | w,c)$表示不是来自与语料库的概率。然后用 sigmoid 函数对 $P(D = 1 | w,c)$ 建模：</p><script type="math/tex; mode=display">P(D=1 | w, c, \theta)=\sigma\left(v_{c}^{T} v_{w}\right)=\frac{1}{1+e^{\left(-v_{c}^{T} v_{w}\right)}}</script></li><li><p>建立新的目标函数，使得 $P(D=1|w,c,\theta) 和 P(D=0|w,c,\theta)$ 最大化，$\theta$ 代表模型的参数，在 <strong>word2vec</strong> 中即 $\mathcal{V} 和 \mathcal{U}$。</p><script type="math/tex; mode=display">\begin{aligned}\theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 | w, c, \theta) \\&=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 | w, c, \theta)) \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 | w, c, \theta)) \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \\&=\underset{\theta}{\operatorname{argmin}}- \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)\end{aligned}</script><p>$\tilde{D}$ 是“负”语料库，即里面是一些出现概率较低的语句。</p><p>对于 <strong>skip-gram</strong> ，在中心词是 $c$ 上下文是 $c-m+j$ 时，目标函数是：</p><script type="math/tex; mode=display">-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)</script><p>对于 <strong>CBOW</strong> ，当中心词是 $u_c$ ，上下文是 $\hat{v}=\frac{v_{c-m}+v_{c-m+1}+\ldots+v_{c+m}}{2 m}$时，目标函数是：</p><script type="math/tex; mode=display">-\log \sigma\left(u_{c}^{T} \cdot \hat{v}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot \hat{v}\right)</script><p>上式中的$\{\tilde{u}_k |k=1,\dots,K \}$即从 $P_n(w)$ 中采样的负样本。负采样率建议是二元语法模型下词频的 3/4 次方。</p></li></ol><h3 id="2-4-Hierarchical-softmax-分层softmax"><a href="#2-4-Hierarchical-softmax-分层softmax" class="headerlink" title="2.4 Hierarchical softmax(分层softmax)"></a>2.4 Hierarchical softmax(分层softmax)</h3><p>分层 <strong>softmax</strong> 用一个二叉树来表示词典中的所有单词。每个叶子节点即一个单词，顶点到叶子节点都只有唯一的一条路径。该模型中没有表示单词的输出，而每个中间节点与将模型将要学习的向量相关联。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcrfg3xw4tj30nv0dcmyg.jpg" alt="Snipaste_2020-03-12_20-41-01.png"></p><p>该模型中，$P(w|w_i)$ 等价于从根节点到与代表单词$w$ 的叶结点的随机游走概率，其最大的优点即时间复杂度只有$O(\log(|V|))$。</p><p>用$L(w)$ 表示从根节点到叶结点 $w$ 的路径长度，$n(w,i)$ 表示该路径中第 $i$ 个中间节点，则 $n(w,L(w))$ 即 $w$ 的父节点。现对每一个中间节点 $n$ ，我们任意选择其子节点中的一个，记为 $ch(n)$。则概率可由下式算出：</p><script type="math/tex; mode=display">P\left(w | w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^{T} v_{w_{i}}\right)</script><p>where</p><script type="math/tex; mode=display">[x]=\left\{\begin{array}{l}1 \text { if } x \text { is true } \\-1 \text { otherwise }\end{array}\right.</script><p>$\sigma(\cdot)$即 <strong>sigmoid</strong> 函数</p><h2 id="3-GloVe-Global-Vector"><a href="#3-GloVe-Global-Vector" class="headerlink" title="3. GloVe(Global Vector)"></a>3. GloVe(Global Vector)</h2><blockquote><p>LSA和word2vec作为两大类方法的代表，一个是利用了全局特征的矩阵分解方法，一个是利用局部上下文的方法。GloVe模型就是将这两中特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。</p></blockquote><ol><li><p>首先引入共现矩阵(Co-occurrence Matrix)</p><p>用 $X$  来表示词与词之间的共现矩阵，每个元素 $X_{ij}$ 表示词 $j$ 出现在词 $i$ 上下文的次数。$X_i = \sum_kX{ik}$ 表示出现在词 $i$ 上下文所有单词的次数。然后用 $P_{ij} = P(w_j|w_i) =\frac{X_{ij}}{X_i}$ 表示词 $j$ 出现在词上下文的概率。</p></li><li><p>最小二乘法</p><p>回顾 <strong>skip-gram</strong> 模型，我们用 <strong>softmax</strong> 来计算词 $j$ 出现在词 $i$ 上下文的概率：</p><script type="math/tex; mode=display">Q_{i j}=\frac{\exp \left(\vec{u}_{j}^{T} \vec{v}_{i}\right)}{\sum_{w=1}^{W} \exp \left(\vec{u}_{w}^{T} \vec{v}_{i}\right)}</script><p>其损失函数定义如下：</p><script type="math/tex; mode=display">J=-\sum_{i \in \text {corpus}} \sum_{j \in \text {context}(i)} \log Q_{i j}</script><p>由于相同的单词i和j可以在语料库中多次出现，因此首先将i和j的相同值组合在一起更有效:</p><script type="math/tex; mode=display">J=-\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i j} \log Q_{i j}</script><p>其中 $X_{ij}$ 即共现矩阵。交叉熵损失的一个显著缺点是它要求分布 $Q$ 被适当地规范化，这涉及到整个词汇表上昂贵的求和。相反，我们使用一个最小二乘目标，其中 $P$ 和 $Q$ 中的标准化因子被丢弃：</p><script type="math/tex; mode=display">\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\hat{P}_{i j}-\hat{Q}_{i j}\right)^{2}</script><p>其中 $\hat{P}_{ij} = X_{ij}, \hat{Q}_{ij} = \exp(\vec{u}_j^T\vec{v}_i)$ 。但这也导致了新的问题：$X_{ij}$ 具有非常大的值，使优化变得困难。一个有效的办法就是给 $\hat{P}$ 和 $\hat{Q}$ 加一个 <strong>log</strong>：</p><script type="math/tex; mode=display">\begin{aligned}\hat{J} &=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\log (\hat{P})_{i j}-\log \left(\hat{Q}_{i j}\right)\right)^{2} \\&=\sum_{i=1}^{W} \sum_{j=1}^{W} X_{i}\left(\vec{u}_{j}^{T} \vec{v}_{i}-\log X_{i j}\right)^{2}\end{aligned}</script><p>另一个观察是，权重因子 $X_i$ 不能保证是最优的。相反，我们引入了一个更一般的加权函数，我们也可以根据上下文单词自由选择:</p><script type="math/tex; mode=display">\hat{J}=\sum_{i=1}^{W} \sum_{j=1}^{W} f\left(X_{i j}\right)\left(\vec{u}_{j}^{T} \vec{v}_{i}-\log X_{i j}\right)^{2}</script></li><li><p>结论</p><p>综上所述，GloVe 模型通过只训练单词共现矩阵中的非零元素，有效地利用了全局统计信息，生成了一个具有有意义子结构的向量空间。在相同的语料库、词汇、窗口大小和训练时间下，它在词汇类比任务上始终优于word2vec。它可以更快地获得更好的结果，而且无论速度如何，也可以获得最佳的结果。</p></li></ol><h2 id="4-FastText"><a href="#4-FastText" class="headerlink" title="4. FastText"></a>4. FastText</h2><p>模型结构：</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOugy1gcvv4e966bj30ha0bv3z3.jpg" alt="FastText.png"></p><ol><li><p>首先是权值矩阵 $A$ ，将单词 $x_1,x_2,\dots,x_{N-1},x_{N}$ 映射成词向量，然后再对这些词向量取平均，得到隐藏层的文本表示，作为线性分类器的输入。</p></li><li><p>通过线性分类器后的输出，再使用 <strong>softmax</strong> 函数 $f$ 得到预测类别的概率分布。</p></li><li><p>损失函数可表达为:</p><script type="math/tex; mode=display">-\frac{1}{N} \sum_{n=1}^{N} y_{n} \log \left(f\left(B A x_{n}\right)\right)</script><p>该模型使用随机梯度下降法和线性衰减学习率来进行训练。</p></li></ol><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/undefined.html">https://gongjintao.com/post/undefined.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Basic-Embedding-Model&quot;&gt;&lt;a href=&quot;#Basic-Embedding-Model&quot; class=&quot;headerlink&quot; title=&quot;Basic Embedding Model&quot;&gt;&lt;/a&gt;Basic Embedding Model&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="https://gongjintao.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://gongjintao.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》——决策树</title>
    <link href="https://gongjintao.com/post/f5fe0882.html"/>
    <id>https://gongjintao.com/post/f5fe0882.html</id>
    <published>2019-07-23T00:30:00.000Z</published>
    <updated>2020-02-22T08:54:27.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="5-1-决策树模型与学习"><a href="#5-1-决策树模型与学习" class="headerlink" title="5.1 决策树模型与学习"></a>5.1 决策树模型与学习</h1><h2 id="5-1-1-决策树模型"><a href="#5-1-1-决策树模型" class="headerlink" title="5.1.1　决策树模型"></a>5.1.1　决策树模型</h2><p>​        <strong>定义</strong>：分类决策树模型是一种描述对实例进行分类的树形结构。决策树由<strong>结点（node）</strong>和<strong>有向边（directed edge）</strong>组成。结点有两种类型：<strong>内部结点（internal node）</strong>和<strong>叶结点（leaf node）</strong>。内部结点表示一个特征或属性，叶结点表示一个类。</p><h2 id="5-1-2-决策树与if-then规则"><a href="#5-1-2-决策树与if-then规则" class="headerlink" title="5.1.2　决策树与if-then规则"></a>5.1.2　决策树与if-then规则</h2><p>​        由决策树的<strong>根结点到叶结点的每一条路径</strong>构建一条规则；路径上内部结点的的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：<strong>互斥并且完备</strong>。即，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。</p><h3 id="5-1-3-决策树与条件概率分布"><a href="#5-1-3-决策树与条件概率分布" class="headerlink" title="5.1.3 决策树与条件概率分布"></a>5.1.3 决策树与条件概率分布</h3><h3 id="5-1-4-决策树学习"><a href="#5-1-4-决策树学习" class="headerlink" title="5.1.4 决策树学习"></a>5.1.4 决策树学习</h3><p>​        学习的<strong>目标</strong>是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。决策树学习<strong>本质</strong>上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。</p><p>​        决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用<strong>启发式方法</strong>，近似求解这一最优化问题。这样得到的决策树是<strong>次最优</strong>（sub-optimal）的。</p><p>​        决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。</p><p>​        为了防止发生过拟合现象，我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p><h2 id="5-2-特征选择"><a href="#5-2-特征选择" class="headerlink" title="5.2 特征选择"></a>5.2 特征选择</h2><h3 id="5-2-1-特征选择问题"><a href="#5-2-1-特征选择问题" class="headerlink" title="5.2.1 特征选择问题"></a>5.2.1 特征选择问题</h3><p>​        特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是<strong>信息增益</strong>或<strong>信息增益比</strong>。</p><h3 id="5-2-2-信息增益"><a href="#5-2-2-信息增益" class="headerlink" title="5.2.2 信息增益"></a>5.2.2 信息增益</h3><p>​        首先给出<strong>熵</strong> 和 <strong>条件熵</strong>的定义</p><blockquote><p><strong>熵（entropy）</strong>是表示随机变量<strong>不确定性</strong>的度量。设X是一个取有限个值的离散随机变量，其概率分布为</p><script type="math/tex; mode=display">P\left(X=x_{i}\right)=p_{i}, \quad i=1,2, \cdots, n</script><p>则随机变量X的熵定义为</p><script type="math/tex; mode=display">H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}</script></blockquote><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/f5fe0882.html">https://gongjintao.com/post/f5fe0882.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;5-1-决策树模型与学习&quot;&gt;&lt;a href=&quot;#5-1-决策树模型与学习&quot; class=&quot;headerlink&quot; title=&quot;5.1 决策树模型与学习&quot;&gt;&lt;/a&gt;5.1 决策树模型与学习&lt;/h1&gt;&lt;h2 id=&quot;5-1-1-决策树模型&quot;&gt;&lt;a href=&quot;#5-
      
    
    </summary>
    
      <category term="读书笔记" scheme="https://gongjintao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="《统计学习方法》" scheme="https://gongjintao.com/tags/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》——朴素贝叶斯法</title>
    <link href="https://gongjintao.com/post/6ca7c45e.html"/>
    <id>https://gongjintao.com/post/6ca7c45e.html</id>
    <published>2019-07-05T01:34:08.000Z</published>
    <updated>2020-02-22T08:54:27.484Z</updated>
    
    <content type="html"><![CDATA[<h1 id="4-1-朴素贝叶斯法的学习与分类"><a href="#4-1-朴素贝叶斯法的学习与分类" class="headerlink" title="4.1 朴素贝叶斯法的学习与分类"></a>4.1 朴素贝叶斯法的学习与分类</h1><h2 id="4-1-1-基本方法"><a href="#4-1-1-基本方法" class="headerlink" title="4.1.1 基本方法"></a>4.1.1 基本方法</h2><p>​        设输入空间$x⊆\mathrm{R}_n$为n维向量的集合，输出空间为类标记集合$\mathcal{Y}＝\{c_1,c_2,\ldots,c_K\}$。输入为特征向量$x∊\mathcal{X}$，输出为<strong>类标记（class label)</strong>$y\in \mathcal{Y}$。$X$是定义在输入空间$\mathcal{X}$上的随机向量，$Y$是定义在输出空间$\mathcal{Y}$上的随机变量。$P(X,Y)$是$X$和$Y$的联合概率分布。训练数据集</p><script type="math/tex; mode=display">T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}</script><p>由$P(X,Y)$独立同分布产生。</p><p>​        朴素贝叶斯法通过训练数据集学习联合概率分布$P(X,Y)$。具体地，学习以下先验概率分布及条件概率分布。先验概率分布</p><script type="math/tex; mode=display">P\left(Y=c_{k}\right), \quad k=1,2, \cdots, K</script><p>条件概率分布</p><script type="math/tex; mode=display">P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots,\left.X^{(n)}\right|^{(n)} | Y=c_{k}\right), \quad k=1,2, \cdots, K</script><p>于是学习到联合概率分布$P(X,Y)$。</p><p>​        朴素贝叶斯法对条件概率分布作了条件独立性的假设，具体如下：</p><script type="math/tex; mode=display">\begin{aligned} P\left(X=x | Y=c_{k}\right) &=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) \end{aligned}</script><p>​        朴素贝叶斯法分类时，对给定的输入$x$，通过学习到的模型计算后验概率分布$ P(Y＝c_k|X＝x)$，将<strong>后验概率最大的类</strong>作为$x$的类输出。后验概率计算根据贝叶斯定理进行：</p><script type="math/tex; mode=display">P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}</script><p>将条件独立性假设代入上式，则朴素贝叶斯分类器可表示为</p><script type="math/tex; mode=display">y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}</script><p>因为对于所有的$c_k$上式的分母都是一样的，所以</p><script type="math/tex; mode=display">y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)</script><h2 id="4-1-2-后验概率最大化的含义"><a href="#4-1-2-后验概率最大化的含义" class="headerlink" title="4.1.2 后验概率最大化的含义"></a>4.1.2 后验概率最大化的含义</h2><p>​        朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。</p><p><strong>证明：</strong> 假设选择<strong>0-1损失函数</strong>，即：</p><script type="math/tex; mode=display">L(Y, f(X))=\left\{\begin{array}{ll}{1,} & {Y \neq f(X)} \\ {0,} & {Y=f(X)}\end{array}\right.</script><p>这时期望风险函数为：</p><script type="math/tex; mode=display">R_{\mathrm{exp}}(f)=E[L(Y, f(X))]</script><p>期望是对联合分布$P(X,Y)$取的。由此取条件期望</p><script type="math/tex; mode=display">R_{\mathrm{exp}}(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} | X\right)</script><p>为了使期望风险最小化，只需对$X=x$逐个极小化，由此得到：</p><script type="math/tex; mode=display">\begin{aligned} f(x) &=\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right) \\ &=\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} | X=x\right) \\ &=\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} | X=x\right)\right) \\ &=\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} | X=x\right) \end{aligned}</script><p>即根据期望风险最小化准则就得到了后验概率最大化准则：</p><script type="math/tex; mode=display">f(x)=\arg \max _{c_{k}} P\left(c_{k} | X=x\right)</script><h1 id="4-2-朴素贝叶斯法的参数估计"><a href="#4-2-朴素贝叶斯法的参数估计" class="headerlink" title="4.2 朴素贝叶斯法的参数估计"></a>4.2 朴素贝叶斯法的参数估计</h1><h2 id="4-2-1-极大似然估计"><a href="#4-2-1-极大似然估计" class="headerlink" title="4.2.1 极大似然估计"></a>4.2.1 极大似然估计</h2><p>​        朴素贝叶斯法中，学习意味着估计先验概率$P(Y＝c_k)$和条件概率$P(X^{(j)}＝x^{(j)}|Y＝c_k)$</p><p><strong>先验概率</strong>的极大似然估计是</p><script type="math/tex; mode=display">P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K</script><p>设第$j$个特征$x^{(j)}$可能取值的集合为$\{a_j1,a_j2,\ldots,a_{jSj} \}$，<strong>条件概率</strong>$P(x^{(j)}＝a_{jl}|Y＝c_k)$的极大似然估计是:</p><script type="math/tex; mode=display">\begin{array}{l}{\qquad P\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}} \\ {j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{i} ; \quad k=1,2, \cdots, K}\end{array}</script><p>式中，$x_i^{j}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值；$I$为指示函数。</p><h2 id="4-2-2-学习与分类算法"><a href="#4-2-2-学习与分类算法" class="headerlink" title="4.2.2 学习与分类算法"></a>4.2.2 学习与分类算法</h2><p>​        朴素贝叶斯算法：</p><blockquote><p><em>输入：</em> 1. 训练数据$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{N}, y_{N}\right)\right\}$，其中$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}$，具体含义如上述。$y_i \in \{c_1,c_2,\ldots,c_K\}$; </p><p>​             2. 实例$x$</p><p><em>输出：</em> 实例$x$的分类</p><p><em>算法步骤：</em></p><ol><li><p>计算先验概率和条件概率：</p><script type="math/tex; mode=display">P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K \\P\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(i)}=a_{jl}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)} \\j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K</script></li><li><p>对于给定的实例$x=\left(x^{(1)}, x^{(2)}, \ldots, x^{n)}\right)^{\mathrm{T}}$，计算</p><script type="math/tex; mode=display">P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right), \quad k=1,2, \cdots, K</script></li><li><p>确定实例$x$的类</p><script type="math/tex; mode=display">y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)</script></li></ol></blockquote><h2 id="4-2-3-贝叶斯估计"><a href="#4-2-3-贝叶斯估计" class="headerlink" title="4.2.3 贝叶斯估计"></a>4.2.3 贝叶斯估计</h2><p>​        用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。</p><p>​        <strong>条件概率</strong>的贝叶斯估计：</p><script type="math/tex; mode=display">P_{\lambda}\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{jl} ,y_{i}=c_{k}\right)+\lambda}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+S_{j} \lambda}</script><p>式中$\lambda \geqslant 0$。等价于在随机变量各个取值的频数上赋予一个正数$\lambda &gt; 0$.当$\lambda = 0$ 时就是极大似然估计。常取$\lambda = 1$ ，这时称为<strong>拉普拉斯平滑（Laplace smoothing）</strong>。</p><p>​        <strong>先验概率</strong>的贝叶斯估计为 ：</p><script type="math/tex; mode=display">P_{\lambda}\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)+\lambda}{N+K \lambda}</script><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p><strong>4.1</strong>　用极大似然估计法推出朴素贝叶斯法中的概率估计公式（4.8）及公式（4.9）。</p><p><strong>证明：</strong>(4.8式) 设$p = P(Y = c_k)， m = \sum_{i=1}^{N}I(y_i = c_k)$，则似然函数为：</p><script type="math/tex; mode=display">L(p) = \tbinom{N}{m}p^{m}(1-p)^{N-m}</script><p>​            两边取对数可得：</p><script type="math/tex; mode=display">\ln(L(p)) = \ln\tbinom{N}{m} + m\ln p + (N-m)\ln (1-p)</script><p>​            然后对$p$ 求导，并令等式等于 $0$ 可得：</p><script type="math/tex; mode=display">\frac{m}{p} - \frac{N-m}{1-p} = 0 \\</script><p>​            解得$p = \frac{m}{N} $，即</p><script type="math/tex; mode=display">P(Y= c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}</script><p>​            （4.9式）设$p = P(X^{(j)} =a_{jl} | Y = c_k) ，m = \sum_{i=1}^{N}I(x_i^{(j)} = a_{jl},y_i = c_k ) ,   n = \sum_{i=1}^{N}I(y_i = c_k)$</p><p>​                则似然函数为：</p><script type="math/tex; mode=display">L(p) = \tbinom{n}{m}p^{m}(1-p)^{n-m}</script><p>​                两边取对数得：</p><script type="math/tex; mode=display">\ln(L(p)) = \ln \tbinom{n}{m} + m \ln p + (n-m)\ln(1-p)</script><p>​                然后两边对$p$求导，并令等式等于$0$可得</p><script type="math/tex; mode=display">\frac{m}{p} - \frac{n-m}{1-p} = 0</script><p>​                解得$p = \frac{m}{n}$，即</p><script type="math/tex; mode=display">P(X^{(j)} =a_{jl} | Y = c_k)  = \frac{ \sum_{i=1}^{N}I(x_i^{(j)} = a_{jl})}{\sum_{i=1}^{N}I(y_i = c_k)}</script><p><strong>4.2</strong>　用贝叶斯估计法推出朴素贝叶斯法中的概率估计公式（4.10）及公式（4.11）。</p><font color="blue">TODO</font><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/6ca7c45e.html">https://gongjintao.com/post/6ca7c45e.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;4-1-朴素贝叶斯法的学习与分类&quot;&gt;&lt;a href=&quot;#4-1-朴素贝叶斯法的学习与分类&quot; class=&quot;headerlink&quot; title=&quot;4.1 朴素贝叶斯法的学习与分类&quot;&gt;&lt;/a&gt;4.1 朴素贝叶斯法的学习与分类&lt;/h1&gt;&lt;h2 id=&quot;4-1-1-基本方
      
    
    </summary>
    
      <category term="读书笔记" scheme="https://gongjintao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="《统计学习方法》" scheme="https://gongjintao.com/tags/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法源码之k近邻算法</title>
    <link href="https://gongjintao.com/post/5e870969.html"/>
    <id>https://gongjintao.com/post/5e870969.html</id>
    <published>2019-07-03T07:44:29.000Z</published>
    <updated>2020-02-22T08:54:27.494Z</updated>
    
    <content type="html"><![CDATA[<h2 id="KD树实现KNN算法"><a href="#KD树实现KNN算法" class="headerlink" title="KD树实现KNN算法"></a>KD树实现KNN算法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建结点类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.lchild = <span class="literal">None</span></span><br><span class="line">        self.rchild = <span class="literal">None</span></span><br><span class="line">        self.feature = <span class="literal">None</span> <span class="comment"># 结点的特征向量</span></span><br><span class="line">        self.axes = <span class="literal">None</span> <span class="comment"># 结点的划分平面所在的维度</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_str</span><span class="params">(self)</span>:</span> <span class="comment"># 返回结点的信息</span></span><br><span class="line">        <span class="keyword">return</span> [self.lchild,self.rchild,self.feature,self.axes]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KDTree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_x,input_y)</span>:</span></span><br><span class="line">        self._input_x = input_x</span><br><span class="line">        self._input_y = input_y</span><br><span class="line">        self._nearest = [] <span class="comment"># 用于存储离目标点最近的点,包括特征向量，距离，和标签</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建KD树</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_tree</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._build_tree(self._input_x,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_tree</span><span class="params">(self, data, r)</span>:</span></span><br><span class="line">        root = Node()</span><br><span class="line"></span><br><span class="line">        [n, m] = np.shape(data)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对data的第r维坐标进行排序</span></span><br><span class="line">        sorted_data = data[data[:, r].argsort()]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 取中间的元素</span></span><br><span class="line">        mid = n//<span class="number">2</span></span><br><span class="line">        root.feature = sorted_data[mid]</span><br><span class="line">        root.axes = r</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新r</span></span><br><span class="line">        r = (r + <span class="number">1</span>) % m</span><br><span class="line">        root.lchild = self._build_tree(sorted_data[:mid], r)</span><br><span class="line">        root.rchild = self._build_tree(sorted_data[mid + <span class="number">1</span>:], r)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_distance</span><span class="params">(self,a,b)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.sqrt(sum((a-b)**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求离目标点最近的k个点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_nearest</span><span class="params">(self,current,target,k)</span>:</span></span><br><span class="line">        d = self._get_distance(current,target)</span><br><span class="line">        l = len(self._nearest)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> l &lt; k:</span><br><span class="line">            self._nearest.append([current,d])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            farthest_d = self._get_farthest()[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> farthest_d &gt; d:</span><br><span class="line">                <span class="comment"># 将最远点移除</span></span><br><span class="line">                new_nearest = [item <span class="keyword">for</span> item <span class="keyword">in</span> self._nearest <span class="keyword">if</span> item[<span class="number">1</span>] &lt; farthest_d]</span><br><span class="line">                new_nearest.append([current,d])</span><br><span class="line">                self._nearest = new_nearest</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 找出_nearest中距离目标点最远的点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_farthest</span><span class="params">(self)</span>:</span></span><br><span class="line">        farthest = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> self._nearest:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> farthest:</span><br><span class="line">                farthest = item</span><br><span class="line">            <span class="keyword">elif</span> farthest[<span class="number">1</span>] &lt; item[<span class="number">1</span>]:</span><br><span class="line">                farthest = item</span><br><span class="line">        <span class="keyword">return</span> farthest</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 在以tree为根结点的树中寻找叶结点，并将查找路径保存在stack中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_search_leaf</span><span class="params">(self,tree,target,stack)</span>:</span></span><br><span class="line">        travel_tree = tree</span><br><span class="line">        <span class="keyword">while</span> travel_tree:</span><br><span class="line">            [lchild,rchild,feature,axes] = travel_tree.get_str()</span><br><span class="line">            <span class="keyword">if</span> target[axes] &gt;= feature[axes]:</span><br><span class="line">                next_node = rchild</span><br><span class="line">                next_direction = <span class="string">'right'</span> <span class="comment"># 记录哪个方向被访问过了</span></span><br><span class="line">            <span class="keyword">elif</span> target[axes] &lt; feature[axes]:</span><br><span class="line">                next_node = lchild</span><br><span class="line">                next_direction = <span class="string">'left'</span></span><br><span class="line">            stack.append([travel_tree,next_direction])</span><br><span class="line">            travel_tree = next_node   </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 搜索kd树</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search_kd_tree</span><span class="params">(self,tree,target,k=<span class="number">1</span>)</span>:</span></span><br><span class="line">        stack = []</span><br><span class="line">        self._search_leaf(tree,target,stack) <span class="comment"># 一直搜索到叶结点，并将路径入栈</span></span><br><span class="line">        <span class="keyword">while</span> stack:</span><br><span class="line">            [node,next_direction] = stack.pop() <span class="comment">#出栈</span></span><br><span class="line">            <span class="comment"># 当前结点的信息</span></span><br><span class="line">            [lchild,rchild,feature,axes] = node.get_str()</span><br><span class="line">            self._check_nearest(feature,target,k) <span class="comment"># 检查当前结点的距离</span></span><br><span class="line">            <span class="keyword">if</span> node.lchild <span class="keyword">and</span> node.rchild <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#如果当前结点是叶子结点，继续下一个循环</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            [farthest,distance] = self._get_farthest()</span><br><span class="line">            <span class="keyword">if</span> abs(feature[axes] - farthest[axes]) &lt; distance: <span class="comment"># 如果当前结点的轴经过了圆</span></span><br><span class="line">                <span class="keyword">if</span> next_direction == <span class="string">'right'</span>:</span><br><span class="line">                    try_node = lchild</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    try_node = rchild</span><br><span class="line">                self._search_leaf(try_node,target,stack)</span><br><span class="line">        <span class="keyword">return</span> self._nearest</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 分类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self)</span>:</span></span><br><span class="line">        types = dict()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> self._nearest:</span><br><span class="line">            <span class="keyword">if</span> item[<span class="number">-1</span>] <span class="keyword">in</span> types:</span><br><span class="line">                types[item[<span class="number">-1</span>]] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                types[item[<span class="number">-1</span>]] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> max(types,key=types.get)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data = np.array([[<span class="number">6.27</span>, <span class="number">5.50</span>],</span><br><span class="line">                      [<span class="number">1.24</span>, <span class="number">-2.86</span>],</span><br><span class="line">                      [<span class="number">17.05</span>, <span class="number">-12.79</span>],</span><br><span class="line">                      [<span class="number">-6.88</span>, <span class="number">-5.40</span>],</span><br><span class="line">                      [<span class="number">-2.96</span>, <span class="number">-0.50</span>],</span><br><span class="line">                      [<span class="number">7.75</span>, <span class="number">-22.68</span>],</span><br><span class="line">                      [<span class="number">10.80</span>, <span class="number">-5.03</span>],</span><br><span class="line">                      [<span class="number">-4.60</span>, <span class="number">-10.55</span>],</span><br><span class="line">                      [<span class="number">-4.96</span>, <span class="number">12.61</span>],</span><br><span class="line">                      [<span class="number">1.75</span>, <span class="number">12.26</span>],</span><br><span class="line">                      [<span class="number">15.31</span>, <span class="number">-13.16</span>],</span><br><span class="line">                      [<span class="number">7.83</span>, <span class="number">15.70</span>],</span><br><span class="line">                      [<span class="number">14.63</span>, <span class="number">-0.35</span>]])</span><br><span class="line"></span><br><span class="line">    labels = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>])</span><br><span class="line">    kd = KDTree(data,labels)</span><br><span class="line">    tree = kd.build_tree()</span><br><span class="line">    target = [<span class="number">-1</span>,<span class="number">-5</span>]</span><br><span class="line">    <span class="comment"># 求离点(-1,-5)最近的三个点</span></span><br><span class="line">    nearest = kd.search_kd_tree(tree,target,<span class="number">3</span>)</span><br><span class="line">    print(nearest)</span><br><span class="line"></span><br><span class="line">    plt.figure(dpi=<span class="number">100</span>)  <span class="comment"># 指定像素</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labels)):</span><br><span class="line">        <span class="keyword">if</span> labels[i] ==<span class="number">1</span>:</span><br><span class="line">            color = <span class="string">'c'</span></span><br><span class="line">        <span class="keyword">elif</span> labels[i] == <span class="number">2</span>:</span><br><span class="line">            color = <span class="string">'b'</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            color = <span class="string">'g'</span></span><br><span class="line">        plt.scatter(data[i][<span class="number">0</span>],data[i][<span class="number">1</span>],c=color,s=<span class="number">30</span>,alpha=<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line">    plt.scatter(target[<span class="number">0</span>],target[<span class="number">1</span>],c=<span class="string">'r'</span>,s=<span class="number">30</span>,alpha=<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> nearest:</span><br><span class="line">        plt.plot([target[<span class="number">0</span>],item[<span class="number">0</span>][<span class="number">0</span>]],[target[<span class="number">1</span>],item[<span class="number">0</span>][<span class="number">1</span>]],<span class="string">'r--'</span>)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[array([ 1.24, -2.86]), 3.0979347959568164], [array([-6.88, -5.4 ]), 5.89358973801197], [array([-2.96, -0.5 ]), 4.908319468005317]]</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/005XIOOuly1g4o43zf6azj30en09p3yh.jpg" alt></p><p>参考自<a href="https://www.cntofu.com/book/48/kjin-lin-fa/kdshu-python-shi-xian.md" target="_blank" rel="noopener">此处</a></p><p><a href="https://www.joinquant.com/view/community/detail/bb850ee76d1cae16cc587f29c4439ebd" target="_blank" rel="noopener">https://www.joinquant.com/view/community/detail/bb850ee76d1cae16cc587f29c4439ebd</a>)</p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/5e870969.html">https://gongjintao.com/post/5e870969.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;KD树实现KNN算法&quot;&gt;&lt;a href=&quot;#KD树实现KNN算法&quot; class=&quot;headerlink&quot; title=&quot;KD树实现KNN算法&quot;&gt;&lt;/a&gt;KD树实现KNN算法&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://gongjintao.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://gongjintao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》——k近邻算法</title>
    <link href="https://gongjintao.com/post/8f99d48c.html"/>
    <id>https://gongjintao.com/post/8f99d48c.html</id>
    <published>2019-07-02T14:00:19.000Z</published>
    <updated>2020-02-22T08:54:27.626Z</updated>
    
    <content type="html"><![CDATA[<h1 id="3-1-k近邻算法"><a href="#3-1-k近邻算法" class="headerlink" title="3.1 k近邻算法"></a>3.1 k近邻算法</h1><p>​        <strong>k近邻法（k-nearest neighbor，k-NN）</strong>是一种基本分类与回归方法。k近邻法的<strong>输入</strong>为实例的特征向量，对应于特征空间的点；<strong>输出</strong>为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其<strong>k个最近邻的</strong>训练实例的类别，通过<strong>多数表决等方式</strong>进行预测。</p><p>​        算法如下：</p><blockquote><p><em>输入：</em></p><ol><li><p>训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，其中，$x_i \in \mathcal{X} \subseteq \mathrm{R}^{n}$为实例的特证向量，$y_i \in \mathcal{Y} = \{c_1,c_2,\ldots,c_K\}$为实例的类别，$i = 1,2,\ldots,N$; </p></li><li><p>实例特征向量$x$</p></li></ol><p><em>输出：</em> 实例$x$所属的类$y$。</p><p><em>算法步骤：</em></p><ol><li>根据<strong>给定的距离度量</strong>，在训练集 $T$ 中找出与 $x$ 最邻近的 $k$ 个点，涵盖这 $k$ 个点的$x$的邻域记作$N_k(x)$；</li><li>在$N_k(x)$ 中根据分类决策规则 (如多数表决) 决定 $x$ 的类别 $y$：</li></ol><script type="math/tex; mode=display">y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; \quad j=1,2, \cdots, K</script><p>$I$为指示函数，即当 $y_i＝c_j$ 时 $I$ 为1，否则 $I$ 为0。</p></blockquote><h1 id="3-2-k近邻模型"><a href="#3-2-k近邻模型" class="headerlink" title="3.2 k近邻模型"></a>3.2 k近邻模型</h1><p>​        k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——<strong>距离度量</strong>、<strong>k值的选择</strong>和<strong>分类决策规则</strong>决定。</p><h2 id="3-2-1-模型"><a href="#3-2-1-模型" class="headerlink" title="3.2.1 模型"></a>3.2.1 模型</h2><p>​        特征空间中，对每个训练实例点 $x_i$，距离该点比其他点更近的所有点组成一个区域，叫作<strong>单元（cell）</strong>。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个<strong>划分</strong>。最近邻法将实例 $x_i$的类 $y_i$作为其单元中所有点的$类标记（class label）$。这样，每个单元的实例点的类别是确定的。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOuly1g4laqg854vj30bo09uabd.jpg" alt="图3.1"></p><h2 id="3-2-2-距离度量"><a href="#3-2-2-距离度量" class="headerlink" title="3.2.2 距离度量"></a>3.2.2 距离度量</h2><p>​        特征空间中两个实例点的距离是两个实例点<strong>相似程度</strong>的反映。k近邻模型的特征空间一般是$n$ 维实数向量空间$\mathrm{R}^n$。使用的距离是<strong>欧氏距离</strong>，但也可以是其他距离，如更一般的<strong>Lp距离（Lp distance）或Minkowski距离（Minkowski distance）</strong>。</p><blockquote><p>$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}} ,x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}$</p><p>$L_p距离定义为$ ：$L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}$</p><p>当 $p=2$ ,则为欧氏距离； </p><p>当 $p = 1$ ，则为曼哈顿距离</p><p>当 $p = \infty$，则是各个坐标距离的最大值，或称切比雪夫距离。</p></blockquote><h2 id="3-2-3-k值的选择"><a href="#3-2-3-k值的选择" class="headerlink" title="3.2.3 k值的选择"></a>3.2.3 k值的选择</h2><p>​        k值的减小就意味着整体模型变得复杂，容易发生过拟合。k值的增大就意味着整体的模型变得简单。在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p><h2 id="3-2-4-分类决策规则"><a href="#3-2-4-分类决策规则" class="headerlink" title="3.2.4 分类决策规则"></a>3.2.4 分类决策规则</h2><p>​        <strong>多数表决规则（majority voting rule）</strong>有如下解释：如果分类的损失函数为0-1损失函数，分类函数为</p><script type="math/tex; mode=display">f: \mathrm{R}^{n} \rightarrow \{ c_1,c_2,\ldots,c_K\}</script><p>那么误分类的概率是</p><script type="math/tex; mode=display">P(Y \neq f(X)) = 1 - P(Y = f(X))</script><p>对给定的实例$x\in \mathcal{X}$，其最近邻的 k 个训练实例点构成集合$N_k(x)$。如果涵盖$N_k(x)$的区域的类别是$c_j$，那么误分类率是</p><script type="math/tex; mode=display">\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i} \neq c_{j}\right)=1-\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right)</script><p>要使误分类率最小即经验风险最小，就要使$\sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right)$最大。</p><h1 id="3-3-k近邻法的实现：kd树"><a href="#3-3-k近邻法的实现：kd树" class="headerlink" title="3.3 k近邻法的实现：kd树"></a>3.3 k近邻法的实现：kd树</h1><p>​        实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。</p><h2 id="3-3-1-构造kd树"><a href="#3-3-1-构造kd树" class="headerlink" title="3.3.1 构造kd树"></a>3.3.1 构造kd树</h2><p>​        kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是<strong>二叉树</strong>，表示对<strong>k维空间</strong>的一个划分（partition）。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。<strong>构造kd树的算法如下：</strong></p><blockquote><p><em>输入：</em>k维空间数据集$T＝\{x_1，x_2,\ldots,x_N\}$,其中 $x_i =\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(k)}\right)^{\mathrm{T}}, i = 1,2,\ldots,N$;</p><p><em>输出：</em>kd树</p><p><em>算法步骤：</em></p><ol><li><p>选择 $x^{(1)}$ 为坐标轴，以$T$ 中所有实例的 $x^{(1)}$ 坐标的中位数 为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。由根结点生成深度为 1 的左、右子结点：左子结点对应坐标$x^{(1)}$ 小于切分点的子区域，右子结点对应坐标$x^{(1)}$ 大于切分点的子区域。将落在切分超平面上的实例点保存在根结点。</p></li><li><p>重复：对深度为 j 的结点，按 1 的方法进行切分，只不过是将上标 1 换成 $l, l = j(\mathrm{mod} k) + 1$   </p></li><li>直到两个子区域没有实例存在时停止。从而形成kd树的区域划分。</li></ol></blockquote><h2 id="3-3-2-搜索kd树"><a href="#3-3-2-搜索kd树" class="headerlink" title="3.3.2 搜索kd树"></a>3.3.2 搜索kd树</h2><p>​        用kd树的最近邻搜索算法如下：</p><blockquote><p><em>输入：</em> 1. 已构造的 kd 树；2. 目标点 x；</p><p><em>输出：</em> x 的最近邻。</p><p><em>算法步骤：</em></p><ol><li>在 kd 树中找到包含目标点 x 的叶结点：</li></ol><p>从根结点出发，递归地向下访问kd树。若目标点 x <strong>当前维</strong>的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。</p><ol><li><p>以此叶结点为“当前最近点”</p></li><li><p>递归地向上回退，在每个结点进行一下操作：</p><ul><li>如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”</li><li>当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。如果有则移动到该点，如果没有，向上回退。</li></ul></li><li><p>当回退到根结点时，搜索结束。最后的“当前最近点”即为 x 的最近邻点。</p></li></ol></blockquote><p>​        此处的 $KD$树 仅是找出距离目标点的最近的点，对应k近邻算法中的k=1；如果 k ≥ 2，则可以设置一个 k 维数组用于存储距离目标点最近的 k 个实例。然后用下面这一步代替上面算法步骤中 3 的第一小步。</p><p>当递归到$KD$树的某一结点时，</p><ul><li><p>如果数组长度小于 k，则将该结点添加到数组；</p></li><li><p>如果数组长度等于 k，则比较该节点到目标点的距离与数组中的最大距离，若大于数组中的最大距离，则继续下一步，否则用该结点替换数组中最大距离的那个实例</p><p>具体算法步骤可以参考知乎上这篇文章——<a href="https://zhuanlan.zhihu.com/p/23966698" target="_blank" rel="noopener">kd 树算法之详细篇</a>。</p></li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p>​        <strong>3.1</strong> 参照图3.1，在二维空间中给出实例点，画出k为1和2时的k近邻法构成的空间划分，并对其进行比较，体会k值选择与模型复杂度及预测准确率的关系。</p><p>​        <font color="blue">Todo</font></p><p>​        <strong>3.2</strong> 利用例题3.2构造的kd树求点$x＝(3, 4.5)^\mathrm{T}$的最近邻点。</p><p><img src="http://ww1.sinaimg.cn/large/005XIOOuly1g4lrcydze0j30c607ugml.jpg" alt></p><p>​    <strong>解：</strong> </p><ol><li><p>从根结点$(7,2)$出发，向下访问 kd树 ，依次比较$x^{(1)}, x^{(2)},,x^{(1) } $找到包含目标结点$(3,4.5)$ 的叶结点$(4,7)$。以$(4,7)$为“当前最近点”，距离约为 2.69。</p></li><li><p>递归向上回退到$(5,4)$，距$(3,4.5)$约2.06，所以更新“当前最近点”为$(5,4)$</p></li><li><p>以$(3,4.5)$为圆心，到“当前最近点”$(5,4)$距离为半径的圆和以$(5,4)$为父结点的另一个子结点$(2,3)$所在的区域相交，于是移动到$(2,3)$</p></li><li><p>移动到$(2,3)$，发现距离$(3,4.5)$约1.80，于是更新“当前最近点”为(2,3);由于$(2,3)$是叶结点，所以 直接回退</p></li><li><p>回退到根结点$(7,2)$，$(3,4.5)$与根结点距离约为4.72，大于到“当前最近点”的距离；同时以$(3,4.5)$为圆心，以与到“当前最近点”的距离为半径的圆和 根结点的另一个子结点$(9,6)$所在的区域不相交。所以搜索结束，得到最近点$(2,3)$</p></li></ol><p>   <strong>3.3</strong>　参照算法3.3，写出输出为x的k近邻的算法。</p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/8f99d48c.html">https://gongjintao.com/post/8f99d48c.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;3-1-k近邻算法&quot;&gt;&lt;a href=&quot;#3-1-k近邻算法&quot; class=&quot;headerlink&quot; title=&quot;3.1 k近邻算法&quot;&gt;&lt;/a&gt;3.1 k近邻算法&lt;/h1&gt;&lt;p&gt;​        &lt;strong&gt;k近邻法（k-nearest neighbor，k
      
    
    </summary>
    
      <category term="读书笔记" scheme="https://gongjintao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="《统计学习方法》" scheme="https://gongjintao.com/tags/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法源码之感知机</title>
    <link href="https://gongjintao.com/post/86fbaf80.html"/>
    <id>https://gongjintao.com/post/86fbaf80.html</id>
    <published>2019-07-01T15:09:19.000Z</published>
    <updated>2020-02-22T08:54:27.513Z</updated>
    
    <content type="html"><![CDATA[<h2 id="感知机算法的原始形式"><a href="#感知机算法的原始形式" class="headerlink" title="感知机算法的原始形式"></a>感知机算法的原始形式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">data = np.asarray([[<span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">label = np.asarray([<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">w = np.asarray([<span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">learning_rate = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 误分类个数为数据集的长度</span></span><br><span class="line">N = len(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> N &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        <span class="comment"># 选取数据</span></span><br><span class="line">        sample = data[i]</span><br><span class="line">        <span class="comment"># 计算损失函数</span></span><br><span class="line">        loss = label[i] * (w.dot(sample.transpose()) + b)</span><br><span class="line">        <span class="keyword">if</span> loss &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            w = w + learning_rate * label[i] * sample</span><br><span class="line">            b = b + learning_rate * label[i]</span><br><span class="line">            N = N <span class="keyword">if</span> N==len(data) <span class="keyword">else</span> N+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 误分类个数减1</span></span><br><span class="line">            N -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    print(w,b)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.scatter(data[:<span class="number">2</span>, <span class="number">0</span>],data[:<span class="number">2</span>, <span class="number">1</span>], marker=<span class="string">'+'</span>)</span><br><span class="line">plt.scatter(data[<span class="number">-1</span>, <span class="number">0</span>],data[<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">line_x = np.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">0.01</span>)</span><br><span class="line">line_y = -(w[<span class="number">0</span>] * line_x + b) / w[<span class="number">1</span>]</span><br><span class="line">plt.plot(line_x, line_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[2 2] 0</span><br><span class="line">[1 1] -1</span><br><span class="line">[0 0] -2</span><br><span class="line">[2 2] -2</span><br><span class="line">[1 1] -3</span><br><span class="line">[1 1] -3</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/005XIOOuly1g4l9cxzs6vj30ae070dfs.jpg" alt></p><h2 id="感知机算法的对偶形式"><a href="#感知机算法的对偶形式" class="headerlink" title="感知机算法的对偶形式"></a>感知机算法的对偶形式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span>  np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据集</span></span><br><span class="line">data = np.asarray([[<span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">label = np.asarray([<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">alpha = np.zeros(<span class="number">3</span>)</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">learning_rate = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算Gram矩阵</span></span><br><span class="line">G = np.asarray([[x.dot(y) <span class="keyword">for</span> x <span class="keyword">in</span> data  ] <span class="keyword">for</span> y <span class="keyword">in</span> data])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 误分类个数为数据集的长度</span></span><br><span class="line">N = len(data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> N &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">for</span> i  <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        sample = data[i]</span><br><span class="line">        loss = label[i]*(np.dot(alpha*label,G[:,i]) + b)</span><br><span class="line">        <span class="keyword">if</span> loss &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            alpha[i] = alpha[i] + learning_rate</span><br><span class="line">            b = b + learning_rate * label[i]</span><br><span class="line">            N = N <span class="keyword">if</span> N==len(data) <span class="keyword">else</span> N+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 误分类个数减1</span></span><br><span class="line">            N -= <span class="number">1</span></span><br><span class="line">    print(alpha,b)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 求出 w</span></span><br><span class="line">w = np.dot(data.T,alpha*label)</span><br><span class="line">print(w)</span><br><span class="line">    </span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.scatter(data[:<span class="number">2</span>, <span class="number">0</span>],data[:<span class="number">2</span>, <span class="number">1</span>], marker=<span class="string">'+'</span>)</span><br><span class="line">plt.scatter(data[<span class="number">-1</span>, <span class="number">0</span>],data[<span class="number">-1</span>, <span class="number">1</span>])</span><br><span class="line">line_x = np.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">0.01</span>)</span><br><span class="line">line_y = -(w[<span class="number">0</span>] * line_x + b) / w[<span class="number">1</span>]</span><br><span class="line">plt.plot(line_x, line_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[1. 0. 1.] 0</span><br><span class="line">[1. 0. 2.] -1</span><br><span class="line">[1. 0. 3.] -2</span><br><span class="line">[2. 0. 4.] -2</span><br><span class="line">[2. 0. 5.] -3</span><br><span class="line">[2. 0. 5.] -3</span><br><span class="line">[1. 1.]</span><br></pre></td></tr></table></figure><p><img src="http://ww1.sinaimg.cn/large/005XIOOuly1g4l9cy51tjj30ae070dfs.jpg" alt></p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/86fbaf80.html">https://gongjintao.com/post/86fbaf80.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;感知机算法的原始形式&quot;&gt;&lt;a href=&quot;#感知机算法的原始形式&quot; class=&quot;headerlink&quot; title=&quot;感知机算法的原始形式&quot;&gt;&lt;/a&gt;感知机算法的原始形式&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://gongjintao.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://gongjintao.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》——感知机</title>
    <link href="https://gongjintao.com/post/607b5cc4.html"/>
    <id>https://gongjintao.com/post/607b5cc4.html</id>
    <published>2019-07-01T08:59:32.000Z</published>
    <updated>2020-02-22T08:54:27.471Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-感知机模型"><a href="#2-1-感知机模型" class="headerlink" title="2.1 感知机模型"></a>2.1 感知机模型</h1><p><strong>定义</strong>     假设输入空间 （特征空间）是$\mathcal{X}\subseteq R^n$，输出空间是$\mathcal{Y} = \{+1,-1\}$。输入$x \in \mathcal{X}$表示实例的特征向量，对应于输入空间（特征空间）的点；输出$y \in \mathcal{Y}$ 表示实例的类别。由输入空间到输出空间的如下函数</p><script type="math/tex; mode=display">f(x)=\operatorname{sign}(w \cdot x+b)</script><p>称为<strong>感知机</strong>。其中，$w$和$b$为感知机模型参数，$w \in R^n$叫作<strong>权值（weight）或权值向量（weight vector）</strong>，$b\in R$叫作<strong>偏置（bias）</strong>，$w·x$表示$w$和$x$的内积。sign是符号函数。<br>        感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有<strong>线性分类模型（linear classification model）或线性分类器(linear classifier)</strong>，即函数集合$\{f | f(x)=w\cdot x+b\}$。对应于特征空间$R^n$中的一个超平面 $S$，其中$w$是超平面的<strong>法向量</strong>，$b$是超平面的<strong>截距</strong>。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。</p><p><img src="https://i.loli.net/2019/06/30/5d18a8730c6a837897.png" alt="这里写图片描述"></p><h1 id="2-2-感知机学习策略"><a href="#2-2-感知机学习策略" class="headerlink" title="2.2 感知机学习策略"></a>2.2 感知机学习策略</h1><h2 id="2-2-1-数据集的线性可分性"><a href="#2-2-1-数据集的线性可分性" class="headerlink" title="2.2.1 数据集的线性可分性"></a>2.2.1 数据集的线性可分性</h2><p><strong>定义</strong>   给定一个数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，其中,$x_i \in \mathcal{X}=R^n, y_i  \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \ldots, N$，如果存在某个超平面 $S$</p><script type="math/tex; mode=display">w \cdot x + b=0</script><p>能够将数据集的正实例点和负实例点<strong>完全正确</strong>地划分到超平面的两侧，即对所有 $y_i = +1$ 的实例$i$，有$w·xi+b&gt;0$，对所有 $y_i＝-1$ 的实例 $i$，有 $w·xi+b&lt;0$，则称数据集 $T$ 为<strong>线性可分数据集（linearly separable data set）</strong>；否则，称数据集 $T$ 线性不可分。</p><h2 id="2-2-2-感知机学习策略"><a href="#2-2-2-感知机学习策略" class="headerlink" title="2.2.2 感知机学习策略"></a>2.2.2 感知机学习策略</h2><p>​        感知机确定了一个学习策略，即定义（经验）损失函数并将损失函数极小化，该损失函数即<strong>误分类点到超平面 $S$ 的总距离</strong>。给定训练数据集</p><script type="math/tex; mode=display">T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}</script><p>其中，$x_i \in \mathcal{X}=R^n, y_i  \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \ldots, N$。感知机$\operatorname{sign}(w \cdot x+b)$学习的<strong>损失函数</strong>定义为</p><script type="math/tex; mode=display">L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)</script><p>其中$M$为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。感知机学习的策略是在假设空间中选取使损失函数式（4）最小的模型参数$ w,b $，即<strong>感知机模型</strong>。</p><h1 id="2-3-感知机学习算法"><a href="#2-3-感知机学习算法" class="headerlink" title="2.3 感知机学习算法"></a>2.3 感知机学习算法</h1><h2 id="2-3-1-感知机学习算法的原始形式"><a href="#2-3-1-感知机学习算法的原始形式" class="headerlink" title="2.3.1 感知机学习算法的原始形式"></a>2.3.1 感知机学习算法的原始形式</h2><p>感知机学习算法是对以下最优化问题的算法:</p><script type="math/tex; mode=display">\min _{w, b} L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)</script><p>我们采用<strong>随机梯度下降法(stochastic gradient descent)</strong>。算法如下：</p><blockquote><p><em>输入：</em> </p><ol><li>$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，</li></ol><p>其中$x_i \in \mathcal{X}=R^n, y_i  \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \ldots, N$</p><ol><li>学习率 $\eta(0&lt;\eta \leqslant1)$</li></ol><p><em>输出：</em> $ w,b;感知机模型 f(x) = \operatorname{sign}(w \cdot x+b) $ 。</p><p><em>算法步骤：</em></p><ol><li>选取初值$w_0,b_0$</li><li>在训练集中选取数据$(x_i,y_i)$</li><li>如果$y_i(w \cdot x_i + b) \leqslant 0$</li></ol><script type="math/tex; mode=display">w \leftarrow w + \eta y_ix_i\\b \leftarrow b + \eta y_i</script><ol><li>转至 2 ，直至训练集中没有误分类点。</li></ol></blockquote><h2 id="2-3-2-算法的收敛性"><a href="#2-3-2-算法的收敛性" class="headerlink" title="2.3.2 算法的收敛性"></a>2.3.2 算法的收敛性</h2><p>​        对于线性可分数据集感知机学习算法原始形式收敛，即经过<strong>有限次</strong>迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。所以只需证明迭代次数$k$有上界即可。</p><h2 id="2-3-3-感知机学习算法的对偶形式"><a href="#2-3-3-感知机学习算法的对偶形式" class="headerlink" title="2.3.3 感知机学习算法的对偶形式"></a>2.3.3 感知机学习算法的对偶形式</h2><p>​        对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$的线性组合的形式，通过求解其系数而求得$w$和$b$。设初始值$w_0=0,b_0=0$，误分类点$(x_i,y_i)$在更新过程中被使用的<strong>次数为$ n_i $，</strong>则最后学习到的$w和b$可以分别表示为</p><script type="math/tex; mode=display">w = \sum_{i=1}^{N}\alpha_i y_i x_i\\b = \sum_{i=1}^{N} \alpha_i y_i</script><p>其中，$\alpha_i = n_i \eta i= 1,2,\ldots,N$。<strong>实例点更新次数越多，意味着它距离分离超平面越近</strong>，也就越难正确分类。感知机学习算法的对偶形式具体如下：</p><blockquote><p><em>输入：</em> </p><ol><li>$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，</li></ol><p>其中$x_i \in \mathcal{X}=R^n, y_i  \in \mathcal{Y}=\{+1,-1\}, \quad i=1,2, \ldots, N$</p><ol><li>学习率 $\eta(0&lt;\eta \leqslant1)$</li></ol><p><em>输出：</em> $ \alpha ,b; 感知机模型 f(x) =\operatorname{sign}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x+b\right), 其中a = (a_1,a_2,\ldots,a_N)^{\mathrm{T}}$</p><p><em>算法步骤：</em></p><ol><li><p>$a \leftarrow 0, b \leftarrow 0 $</p></li><li><p>在训练集中选取数据$(x_i,y_i)$</p></li><li><p>如果$y_{i}\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j} \cdot x_{i}+b\right) \leqslant 0$</p><script type="math/tex; mode=display">\begin{array}{c}{\alpha_{i} \leftarrow \alpha_{i}+\eta} \\ {b \leftarrow b+\eta y_{i}}\end{array}</script></li><li><p>转至2直到没有误分类数据。</p></li></ol></blockquote><p>​        对偶形式中可知训练实例仅以内积的形式出现。可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的<strong>Gram矩阵（Gram matrix）</strong>。</p><script type="math/tex; mode=display">G=\left[x_{i} \cdot x_{j}\right]_{N \times N}</script><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p>​        <strong>2.1</strong>　Minsky与Papert指出：感知机因为是线性模型，所以不能表示复杂的函数，如异或（XOR）。验证感知机为什么不能表示异或。</p><p>​        <strong>解：</strong>异或函数(XOR)表示如下：            </p><div class="table-container"><table><thead><tr><th style="text-align:center">$x_{1}$</th><th style="text-align:center">$x_{2}$</th><th style="text-align:center">$x_{1} \oplus x_{2}$</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">1</td></tr></tbody></table></div><p>​            在图中展示如下：</p><p>​            <img src="http://ww1.sinaimg.cn/large/005XIOOuly1g4kfyxt43tj30jg0elmx0.jpg" alt="异或图示"></p><p>​            显然异或的数据集是线性不可分的，我们无法用一条直线将两类样本划分，所以感知机不能表示异或。</p><p>​        <strong>2.2</strong>　模仿例题2.1，构建从训练数据集求解感知机模型的例子。</p><p>​        <a href="https://blog.gongjintao.com/2019/07/01/《统计学习方法》——感知机/" target="_blank" rel="noopener">参考此文</a></p><p>​        <strong>2.3</strong>　证明以下定理：样本集线性可分的充分必要条件是正实例点集所构成的凸壳与负实例点集所构成的凸壳互不相交。</p><p>​        <font color="blue">Todo</font></p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/607b5cc4.html">https://gongjintao.com/post/607b5cc4.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;2-1-感知机模型&quot;&gt;&lt;a href=&quot;#2-1-感知机模型&quot; class=&quot;headerlink&quot; title=&quot;2.1 感知机模型&quot;&gt;&lt;/a&gt;2.1 感知机模型&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;     假设输入空间 （特征空间）是$\ma
      
    
    </summary>
    
      <category term="读书笔记" scheme="https://gongjintao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="《统计学习方法》" scheme="https://gongjintao.com/tags/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/"/>
    
  </entry>
  
  <entry>
    <title>《统计学习方法》——概论</title>
    <link href="https://gongjintao.com/post/c179653c.html"/>
    <id>https://gongjintao.com/post/c179653c.html</id>
    <published>2019-06-30T09:23:10.000Z</published>
    <updated>2020-02-22T08:54:27.641Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-1-统计学习"><a href="#1-1-统计学习" class="headerlink" title="1.1 统计学习"></a>1.1 统计学习</h1><blockquote><p><strong>定义</strong>：统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。</p></blockquote><a id="more"></a><h2 id="1-1-1-特点"><a href="#1-1-1-特点" class="headerlink" title="1.1.1 特点"></a>1.1.1 特点</h2><blockquote><ol><li>建立在计算机及网络之上的</li><li>以数据为研究对象</li><li>目的是对数据进行预测与分析</li><li>以方法为中心</li><li>包含了概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域，并且在发展中逐步形成独自的理论体系与方法论。</li></ol></blockquote><h2 id="1-1-2-对象"><a href="#1-1-2-对象" class="headerlink" title="1.1.2  对象"></a>1.1.2  对象</h2><blockquote><p>​        统计学习的对象是数据（data）。它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。</p><p>​        统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。由于它们具有统计规律性，所以可以用概率统计方法来加以处理。比如，可以用随机变量描述数据中的特征，用概率分布描述数据的统计规律。</p><p>​        在统计学习过程中，以变量或变量组表示数据。数据分为由连续变量和离散变量表示的类型。</p></blockquote><h2 id="1-1-3-目的"><a href="#1-1-3-目的" class="headerlink" title="1.1.3 目的"></a>1.1.3 目的</h2><blockquote><p>​        统计学习总的目标就是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提高学习效率。</p></blockquote><h2 id="1-1-4-方法"><a href="#1-1-4-方法" class="headerlink" title="1.1.4 方法"></a>1.1.4 方法</h2><blockquote><p><em>统计学习包括：</em></p><ol><li>监督学习(supervised learning)</li><li>非监督学习(unsupervised learning)</li><li>半监督学习(semi-supervised learning)</li><li>强化学习(reinforcement learning)</li></ol><p>监督学习的方法可以概括成：从给定的、有限的、用于学习的<strong>训练数据(training data)集合</strong>出发，假设数据是<strong>独立同分布</strong>产生的；并且假设要学习的模型属于某个函数的集合，称为<strong>假设空间(hypothesis space)</strong>；应用某个<strong>评价准则(evaluation criterion)</strong>，从假设空间中选取一个<strong>最优的模型</strong>，使它对已知训练数据及<strong>未知测试数据(test data)</strong>在给定的评价准则下有最优的预测；最优模型的选取由<strong>算法</strong>实现。所以统计学习方法的三要素，分别为<strong>模型（model）、策略（strategy）和算法（algorithm）</strong>。</p><p><em>实现统计学习方法的步骤如下：</em></p><ol><li>得到一个有限的<strong>训练数据集合</strong>；</li><li>确定包含所有可能的模型的<strong>假设空间</strong>，即学习模型的集合；</li><li>确定模型选择的准则，即学习的<strong>策略</strong>；</li><li>实现求解最优模型的算法，即学习的<strong>算法</strong>；</li><li>通过学习方法选择最优模型；</li><li>利用学习的最优模型对新数据进行<strong>预测或分析</strong>。</li></ol><p><em>监督学习的主要应用问题：</em></p><ul><li>分类问题</li><li>标注问题</li><li>回归问题</li></ul></blockquote><h2 id="1-1-5-研究"><a href="#1-1-5-研究" class="headerlink" title="1.1.5 研究"></a>1.1.5 研究</h2><blockquote><p>统计学习研究一般包括<strong>统计学习方法（statistical learning method）、统计学习理论（statistical learning theory）及统计学习应用（application of statistical learning）</strong>三个方面。</p><ul><li>统计学习方法：开发新的学习方法</li><li>统计学习理论：探求统计学习方法的有效性与效率，以及统计学习的基本理论问题</li><li>统计学习应用：考虑将统计学习方法应用到实际问题中去，解决实际问题</li></ul></blockquote><h2 id="1-1-6-重要性"><a href="#1-1-6-重要性" class="headerlink" title="1.1.6 重要性"></a>1.1.6 重要性</h2><blockquote><ol><li>统计学习是处理海量数据的有效方法。</li><li>统计学习是计算机智能化的有效手段。</li><li>统计学习是计算机科学发展的一个重要组成部分。</li></ol></blockquote><h1 id="1-2-监督学习"><a href="#1-2-监督学习" class="headerlink" title="1.2 监督学习"></a>1.2 监督学习</h1><h2 id="1-2-1-基本概念"><a href="#1-2-1-基本概念" class="headerlink" title="1.2.1 基本概念"></a>1.2.1 基本概念</h2><blockquote><ol><li><strong>输入空间（input space）</strong>: 输入的所有可能取值的集合。</li><li><strong>输出空间（output space）</strong>: 输出的所有可能 取值的集合。</li><li><strong>特征空间（feature space）</strong>：每个具体的输入是一个<strong>实例（instance）</strong>，通常由<strong>特征向量（feature vector）</strong>表示。这时，所有特征向量存在的空间称为<strong>特征空间（feature space）</strong>。<font color="blue">模型实际上都是定义在特征空间上的</font>。</li></ol><p><em>根据输入、输出变量的不同类型，对预测任务给予不同的名称：</em></p><ul><li>均为连续变量：回归问题</li><li>输出变量为有限个离散变量：分类问题</li><li>均为变量序列：标注问题</li></ul><ol><li><strong>联合概率分布</strong>: 监督学习假设输入与输出的随机变量 X 和 Y 遵循联合概率分布P(X,Y)。P(X,Y)表示分布函数，或分布密度函数。</li><li><strong>假设空间（hypothesis space）</strong>: 监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。</li></ol><p>监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数（decision function）Y=f(X)表示，随具体学习方法而定。对具体的输入进行相应的输出预测时，写作P(y|x)或Y=f(x)。</p></blockquote><h2 id="1-2-2-问题的形式化"><a href="#1-2-2-问题的形式化" class="headerlink" title="1.2.2 问题的形式化"></a>1.2.2 问题的形式化</h2><p><img src="https://i.loli.net/2019/06/28/5d158c3b1400839934.png" alt="监督学习问题"></p><h1 id="1-3-统计学习三要素"><a href="#1-3-统计学习三要素" class="headerlink" title="1.3 统计学习三要素"></a>1.3 统计学习三要素</h1><blockquote><p>方法 = 模型＋策略 + 算法</p></blockquote><h2 id="1-3-1-模型"><a href="#1-3-1-模型" class="headerlink" title="1.3.1 模型"></a>1.3.1 模型</h2><blockquote><p>在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或决策函数。由集合表示如下：</p><script type="math/tex; mode=display">\mathcal{F}=\left\{f | Y=f_{\theta}(X), \theta \in \mathbf{R}^{n}\right\}\\</script><p>或者</p><script type="math/tex; mode=display">\mathcal{F}=\left\{P\left|P_{\theta}(Y | X), \theta \in \mathbf{R}^{n}\right\}\right.</script><p>公式(1)表示定义在决策函数上的集合，公式(2)表示定义在条件概率上的集合。其中$ \theta $ 表示为<strong>参数向量（parameter space）</strong> 。</p></blockquote><h2 id="1-3-2-策略"><a href="#1-3-2-策略" class="headerlink" title="1.3.2 策略"></a>1.3.2 策略</h2><blockquote><p>首先引入损失函数与风险函数的概念。</p><ul><li><p><strong>损失函数(loss function)</strong>度量模型一次预测的好坏，和<strong>代价函数(cost function)</strong>表示的意义相同</p><blockquote><p><strong>常用的损失函数有以下几种：</strong></p><ol><li>0-1损失函数（0-1 loss function）</li></ol><script type="math/tex; mode=display">L(Y, f(X))=\left\{\begin{array}{ll}{1,} & {Y \neq f(X)} \\ {0,} & {Y=f(X)}\end{array}\right.</script><ol><li>平方损失函数（quadratic loss function）</li></ol><script type="math/tex; mode=display">L(Y, f(X))=(Y-f(X))^{2}</script><ol><li>绝对损失函数（absolute loss function）</li></ol><script type="math/tex; mode=display">L(Y, f(X))=|Y-f(X)|</script><ol><li>对数损失函数（logarithmic loss function）或对数似然损失函数</li></ol><script type="math/tex; mode=display">L(Y, P(Y | X))=-\log P(Y | X)</script></blockquote></li><li><p><strong>风险函数(risk function)</strong>度量平均意义下模型预测的好坏，也称为<strong>期望损失（expected loss）</strong>，定义如下：</p><blockquote><script type="math/tex; mode=display">R_{\mathrm{exp}}(f)=E_{P}[L(Y, f(X))]=\int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) P(x, y) \mathrm{d} x \mathrm{d} y</script></blockquote></li></ul><p>​        </p><p>​        学习的目标就是选择期望风险最小的模型。但是由于联合分布$P(X,Y)$是未知的，$R_{exp}(f)$不能直接计算。于是引入<strong>经验风险</strong>。</p><ul><li><p><strong>经验风险（empirical risk）</strong>：模型f(X)关于训练数据集的平均损失，也称<strong>经验损失（empirical loss）</strong>；记作$R_{emp}$:</p><blockquote><script type="math/tex; mode=display">R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)</script></blockquote></li></ul><p>​        </p><p>​        根据大数定律，当样本容量N趋于无穷时，经验风险$R_{emp}f$趋于期望风险$R_{exp}f$。但是，由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的<strong>矫正</strong>。这就关系到监督学习的两个基本策略：<strong>经验风险最小化和结构风险最小化。</strong></p><ul><li><p><strong>经验风险最小化（empirical risk minimization，ERM）</strong>：按照经验风险最小化求最优模型就是求解最优化问题；</p><blockquote><script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)</script><p>其中，$\mathcal{F}$ 是假设空间。</p></blockquote></li><li><p><strong>结构风险最小化（structural risk minimization，SRM）</strong>：是为了防止过拟合而提出来的策略。结构风险最小化等价于<strong>正则化（regularization）</strong>。结构风险在经验风险上加上表示模型复杂度的<strong>正则化项（regularizer）或罚项（penalty term）</strong>。</p><blockquote><script type="math/tex; mode=display">R_{\mathrm{srm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><p>其中$J(f)$为模型的复杂度，是定义在假设空间 $\mathcal{F}$上的泛函。模型 $f$ 越复杂，复杂度$J(f)$就越大；反之，模型 $f$ 越简单，复杂度$J(f)$就越小。$\lambda \ge 0$是系数，用以权衡经验风险和模型复杂度。所以求最优模型，就是求解最优化问题：</p><script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script></blockquote></li></ul><p>这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的<strong>目标函数</strong>。</p></blockquote><h2 id="1-3-3-算法"><a href="#1-3-3-算法" class="headerlink" title="1.3.3 算法"></a>1.3.3 算法</h2><blockquote><p>​        算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。这时，统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。    </p></blockquote><h1 id="1-4-模型评估与模型选择"><a href="#1-4-模型评估与模型选择" class="headerlink" title="1.4 模型评估与模型选择"></a>1.4 模型评估与模型选择</h1><h2 id="1-4-1-训练误差与测试误差"><a href="#1-4-1-训练误差与测试误差" class="headerlink" title="1.4.1 训练误差与测试误差"></a>1.4.1 训练误差与测试误差</h2><blockquote><p>​        当损失函数给定时，基于损失函数的模型的<strong>训练误差（training error）</strong>和模型的<strong>测试误差（test error）</strong>就自然成为学习方法评估的标准。训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要。测试误差反映了学习方法对未知的测试数据集的预测能力，是学习中的重要概念。通常将学习方法对未知数据的预测能力称为<strong>泛化能力（generalization ability）</strong>。</p></blockquote><h2 id="1-4-2-过拟合与模型选择"><a href="#1-4-2-过拟合与模型选择" class="headerlink" title="1.4.2 过拟合与模型选择"></a>1.4.2 过拟合与模型选择</h2><blockquote><p>​        如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高。这种现象称为<strong>过拟合（over-fitting）</strong>。过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。</p></blockquote><h1 id="1-5-正则化与交叉验证"><a href="#1-5-正则化与交叉验证" class="headerlink" title="1.5 正则化与交叉验证"></a>1.5 正则化与交叉验证</h1><h2 id="1-5-1-正则化"><a href="#1-5-1-正则化" class="headerlink" title="1.5.1 正则化"></a>1.5.1 正则化</h2><blockquote><p>​        模型选择的典型方法是<strong>正则化（regularization）</strong>。正则化是结构风险最小化策略的实现，是在经验风险上加一个<strong>正则化项（regularizer）或罚项(penalty term)</strong>。正则化项一般是<strong>模型复杂度的单调递增函数</strong>，模型越复杂，正则化值就越大。比如，正则化项可以是模型参数向量的范数。</p><p>​        正则化一般具有如下形式：</p><script type="math/tex; mode=display">\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)</script><p>其中，第1项是经验风险，第2项是正则化项，$\lambda \ge 0$为调整两者之间关系的系数。</p></blockquote><h2 id="1-5-2-交叉验证"><a href="#1-5-2-交叉验证" class="headerlink" title="1.5.2 交叉验证"></a>1.5.2 交叉验证</h2><blockquote><p>​        如果给定的样本数据<strong>充足</strong>，进行模型选择的一种简单方法是<strong>随机地</strong>将数据集切分成三部分，分别为<strong>训练集（training set）、验证集（validation set）和测试集（test set）</strong>。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。</p><p>​        但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。<strong>交叉验证的基本想法</strong>是重复地使用数据；把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p><p>​        有以下几种常用的交叉验证方法：</p><ol><li><strong>简单交叉验证</strong>：首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集；然后用训练集在各种条件下训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。</li><li><strong>S折交叉验证</strong>：首先随机地将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。</li><li><strong>留一交叉验证</strong>：S折交叉验证的特殊情形是S＝N，称为留一交叉验证（leave-one-out cross validation）。往往在数据缺乏的情况下使用。</li></ol></blockquote><h1 id="1-6-泛化能力"><a href="#1-6-泛化能力" class="headerlink" title="1.6 泛化能力"></a>1.6 泛化能力</h1><h2 id="1-6-1-泛化误差"><a href="#1-6-1-泛化误差" class="headerlink" title="1.6.1 泛化误差"></a>1.6.1 泛化误差</h2><blockquote><p>​        现实中采用最多的办法是通过测试误差来评价学习方法的泛化能力。但这种评价是依赖于测试数据集的。因为测试数据集是有限的，很有可能由此得到的评价结果是不可靠的。统计学习理论试图从理论上对学习方法的泛化能力进行分析。</p><p>​        首先给出泛化误差的定义。如果学到的模型是$\hat{f}$，那么用这个模型对未知数据预测的误差即为<strong>泛化误差（generalization error）</strong></p><script type="math/tex; mode=display">R_{\mathrm{exp}}(\hat{f})=E_{P}[L(Y, \hat{f}(X))]=\int_{\mathcal{X} \times \mathcal{Y}} L(y, \hat{f}(x)) P(x, y) \mathrm{d} x \mathrm{d} y</script><p>事实上，泛化误差就是所学习到的模型的期望风险。</p></blockquote><h2 id="1-6-2-泛化误差上界"><a href="#1-6-2-泛化误差上界" class="headerlink" title="1.6.2 泛化误差上界"></a>1.6.2 泛化误差上界</h2><blockquote><p>​        学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称为<strong>泛化误差上界（generalization error bound）。</strong>泛化误差上界通常具有以下性质：</p><ul><li>它是样本容量的函数，当样本容量增加时，泛化上界趋于0；</li><li>它是假设空间容量（capacity）的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大。</li></ul><p><strong><font color="red">关于对二类分类问题，泛化误差上界不等式的证明，即证明：</font></strong></p><p>​        当假设空间是有限个函数的集合$\mathcal{F}＝{f_1，f_2,…,f_d}$时，对任意一个函数 $f \in \mathcal{F}$，至少以概率$1-\delta$，以下不等式成立：</p><script type="math/tex; mode=display">R(f) \leqslant \hat{R}(f)+\varepsilon(d, N, \delta)</script><p>其中，</p><script type="math/tex; mode=display">\varepsilon(d, N, \delta)=\sqrt{\frac{1}{2 N}\left(\log d+\log \frac{1}{\delta}\right)}</script><p>需要用到Hoeffding不等式：</p><p>​        设是$S_n = \sum_{i-1}^{n}X_i$是独立随机变量$X_1,X_2,…,X_n$之和，$X_i \in [a_i，b_i ]$，则对任意$t &gt; 0$，以下不等式成立：</p><script type="math/tex; mode=display">\begin{array}{l}{P\left(S_{n}-E S_{n} \geqslant t\right) \leqslant \exp \left(\frac{-2 t^{2}}{\sum_{i=1}^{n}\left(b_{i}-a_{i}\right)^{2}}\right)} \\ {P\left(E S_{n}-S_{n} \geqslant t\right) \leqslant \exp \left(\frac{-2 t^{2}}{\sum_{i=1}^{n}\left(b_{i}-a_{i}\right)^{2}}\right)}\end{array}</script><p><strong>证明：</strong></p><p>​        ∵ 对于任意函数$f \in \mathcal{F}$，$\hat{R}(f)$是 N 个独立的随机变量 $L(Y,f(X))$的样本均值，$R(f)$是随机变量$L(Y,f(X))$的期望值。如果损失函数取值于区间$[0,1]$，即对所有$i$，$[a_i，b_i ]＝[0,1]$，那么由Hoeffding不等式可得：</p><script type="math/tex; mode=display">\begin{aligned} P(R(f)-\hat{R}(f) \geqslant \varepsilon) &= P(E[\sum_{i=1}^{N}L\left(y_i,f(x_i) \right)]- \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right) \geqslant N \cdot\varepsilon) \\& \leqslant \exp \left(\frac{-2 (N \cdot \varepsilon)^{2}}{\sum_{i=1}^{n}\left(b_{i}-a_{i}\right)^{2}}\right)\\ & = \exp \left(\frac{-2 (N \cdot \varepsilon)^{2}}{N}\right)\\& = exp \left( -2N\varepsilon^2\right)\end{aligned}</script><p>由于$\mathcal{F}=\{ f_1,f_2,…,f_d \}$是一个有限集合，故</p><script type="math/tex; mode=display">\begin{aligned}P(\exists f \in \mathcal{F}:R(f)-\hat{R}(f) \geqslant \varepsilon) &=P\left(\bigcup_{f \in \mathcal{F}}\{R(f)-\hat{R}(f) \geqslant \varepsilon\}\right) \\& \leqslant \sum_{f \in \mathcal{F}}P(R(f)-\hat{R}(f) \geqslant \varepsilon) \\& \leqslant d  \exp(-2N\varepsilon^2)\end{aligned}</script><p>或者等价的，对任意$f \in \mathcal{F}$，有</p><script type="math/tex; mode=display">P(R(f)-\hat{R}(f)<\varepsilon) \geqslant 1-d \exp \left(-2 N \varepsilon^{2}\right)</script><p>令</p><script type="math/tex; mode=display">\delta=d \exp \left(-2 N \varepsilon^{2}\right)</script><p>则</p><script type="math/tex; mode=display">P(R(f)<\hat{R}(f)+\varepsilon) \geqslant 1-\delta</script><p>即至少以概率$1-\delta$有$\mathrm{R}(\mathrm{f})&lt;\hat{R}(\mathrm{f})+\mathcal{E}，$其中$\mathcal{E}$由式(18)得到，即为式(15)</p></blockquote><h1 id="1-7-生成模型与判别模型"><a href="#1-7-生成模型与判别模型" class="headerlink" title="1.7 生成模型与判别模型"></a>1.7 生成模型与判别模型</h1><blockquote><p>​        监督学习方法又可以分为<strong>生成方法（generative approach）</strong>和<strong>判别方法（discriminative approach）</strong>。所学到的模型分别称为<strong>生成模型（generative model）</strong>和<strong>判别模型（discriminative model）。</strong></p><p>​        生成方法由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。之所以称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系。典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型。</p><p>​        判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等</p></blockquote><h1 id="1-8-分类问题"><a href="#1-8-分类问题" class="headerlink" title="1.8 分类问题"></a>1.8 分类问题</h1><blockquote><p>​        分类是监督学习的一个核心问题。在监督学习中，当<strong>输出变量Y取有限个离散值时</strong>，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为<strong>分类器</strong>（classifier）。</p><p>​        评价分类器性能的指标一般是<strong>分类准确率（accuracy）</strong>，其定义是：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。</p><p>​        对于二分类问题常用的评价指标是<strong>精确率（precision）与召回率（recall）</strong>。通常以关注的类为<strong>正类</strong>，其他类为<strong>负类</strong>，分类器在测试数据集上的预测或正确或不正确，4种情况出现的总数分别记作：</p><ul><li>TP——将正类预测为正类数；</li><li>FN——将正类预测为负类数；</li><li>FP——将负类预测为正类数；</li><li>TN——将负类预测为负类数；</li></ul><p>则<strong>精确率</strong>定义为</p><script type="math/tex; mode=display">P = \frac{TP}{TP+FP}</script><p><strong>召回率</strong>定义为</p><script type="math/tex; mode=display">R=\frac{TP}{TP+FN}</script><p>此外，还有$F_1$值，是精确率和召回率的调和均值，即</p><script type="math/tex; mode=display">\frac{1}{F_1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R}) \\F_{1}=\frac{2 T P}{2 T P+F P+F N}</script><p>精确率和召回率都高时，$F_1$值也会高。</p></blockquote><h1 id="1-9-标注问题"><a href="#1-9-标注问题" class="headerlink" title="1.9 标注问题"></a>1.9 标注问题</h1><blockquote><p>​        标注问题的输入是一个<strong>观测序列</strong>，输出是一个<strong>标记序列或状态序列</strong>。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。</p><p>​        评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率。其定义与分类模型相同。</p><p>​        标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。</p></blockquote><h1 id="1-10-回归问题"><a href="#1-10-回归问题" class="headerlink" title="1.10 回归问题"></a>1.10 回归问题</h1><blockquote><p>​        回归用于预测输入变量（自变量）和输出变量（因变量）之间的<strong>关系</strong>，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。</p><p>​        回归问题按照输入变量的个数，分为<strong>一元回归和多元回归</strong>；按照输入变量和输出变量之间关系的类型即模型的类型，分为<strong>线性回归和非线性回归</strong>。</p><p>​        回归学习最常用的损失函数是<strong>平方损失函数</strong>，在此情况下，回归问题可以由著名的<strong>最小二乘法（least squares）</strong>求解。</p></blockquote><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p>​        <strong>1.1</strong>     说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型n次独立的数据生成结果，其中k次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。</p><p>​        <strong>解：</strong> </p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">极大似然估计</th><th style="text-align:center">贝叶斯估计</th></tr></thead><tbody><tr><td style="text-align:center">模型</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">策略</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">算法</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr></tbody></table></div><p>​                定义随机变量$A$为一次伯努利实验的结果，可知$A$的取值为$\{0,1\}$；</p><p>​                设$P\{A = 1\} = \theta$，则$P\{A = 0\} = 1- \theta$</p><p>​                <strong>极大似然估计</strong></p><p>​                首先得到似然函数：</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{n}P(A_i)= C_{n}^{k}\theta^k(1-\theta)^{n-k} \\</script><p>​                然后取对数得到对数似然函数：</p><script type="math/tex; mode=display">\ln(L(\theta)) = \ln C_{n}^{k} + k \cdot \ln\theta + (n-k) \cdot \ln(1-\theta)</script><p>​                然后在上式两边对 $\theta$ 求导可得</p><script type="math/tex; mode=display">\frac{\partial \ln(L(\theta))}{\partial \theta} = \frac{k}{\theta} - \frac{n-k}{1-\theta}</script><p>​                令上式等于 0 ， 可得</p><script type="math/tex; mode=display">\theta = \frac{k}{n}</script><p>​                <strong>贝叶斯估计</strong></p><p>​                todo</p><p>​        <strong>1.2 </strong>    通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。</p><p>​        <strong>证明：</strong> 当模型是条件概率分布，即$\mathcal{F}=\left\{P\left|P_{\theta}(Y | X), \theta \in \mathbf{R}^{n}\right\}\right.$；</p><p>​                    损失函数是对数损失函数时，即$L(Y, P(Y | X))=-\log P(Y | X)$;</p><p>​                    则<strong>经验风险</strong>如下：</p><script type="math/tex; mode=display">\begin{aligned}R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right) & = \frac{1}{N}\sum_{i=1}^{N}L(y_i,P(y_i|x_i))\\&= \frac{1}{N}\sum_{i=1}^{N}-\log P_\theta(y_i|x_i)\\\end{aligned}</script><p>​                    则经验风险最小化为：</p><script type="math/tex; mode=display">\hat{\theta} = arg\min _{P \in \mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}-\log P_\theta(y_i|x_i)\\</script><p>​                    <strong>极大似然法：</strong></p><p>​                    先求似然函数，如下：</p><script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N} P_{\theta}\left(y_{i} | x_{i}\right)</script><p>​                    所以极大似然估计为：</p><script type="math/tex; mode=display">\hat{\theta}=arg \max_{P\in \mathcal{F}}  \sum_{i=1}^{N}\log P_{\theta}\left(y_{i} | x_{i}\right)</script><p>等价于：</p><script type="math/tex; mode=display">\hat{\theta}=arg \min_{P\in \mathcal{F}}  \sum_{i=1}^{N}-\log P_{\theta}\left(y_{i} | x_{i}\right)</script><p>​                因此证得模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。</p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/c179653c.html">https://gongjintao.com/post/c179653c.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-1-统计学习&quot;&gt;&lt;a href=&quot;#1-1-统计学习&quot; class=&quot;headerlink&quot; title=&quot;1.1 统计学习&quot;&gt;&lt;/a&gt;1.1 统计学习&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定义&lt;/strong&gt;：统计学习（statistical learning）是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习也称为统计机器学习（statistical machine learning）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="读书笔记" scheme="https://gongjintao.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="《统计学习方法》" scheme="https://gongjintao.com/tags/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript中的数组迭代方法总结 </title>
    <link href="https://gongjintao.com/post/5189ba24.html"/>
    <id>https://gongjintao.com/post/5189ba24.html</id>
    <published>2019-06-30T01:28:11.368Z</published>
    <updated>2020-02-22T08:54:27.431Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ECMAScript-5中的数组迭代方法总结"><a href="#ECMAScript-5中的数组迭代方法总结" class="headerlink" title="ECMAScript 5中的数组迭代方法总结"></a><em>ECMAScript 5</em>中的数组迭代方法总结</h1><p>在 <strong><em>ECMAscript 5</em></strong> 中为数组定义了5个迭代方法，每个方法都接收两个参数：<a id="more"></a></p><blockquote><ol><li>要在每一项上运行的函数,该函数可接受三个参数(<em>通常只需要第一个参数值，后面两个可以忽略</em>)：<ul><li>数组项的值: <em>item</em></li><li>该项在数组中的位置: <em>index</em></li><li>数组对象本身: <em>array</em></li></ul></li><li>运行调用函数(第一个参数)的作用域对象，为可选参数。如果有第二个参数，则调用的函数被看做是第二个参数的方法。也就是说，在调用函数时传递进去的第二个参数作为它的this关键字的值来使用。</li></ol></blockquote><p>下面依次介绍这5个方法的作用和用法：</p><h2 id="every"><a href="#every" class="headerlink" title="every()"></a>every()</h2><blockquote><p>对数组中的每一项运行给定函数，如果该函数对每一项都返回<strong>true</strong>，则返回<strong>true</strong>；否则返回<strong>false</strong>。示例如下：</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> everyResult = arr.every(<span class="function"><span class="keyword">function</span>(<span class="params">item</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (item &gt; <span class="number">2</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">alert(everyResult); <span class="comment">//false</span></span><br></pre></td></tr></table></figure><h2 id="some"><a href="#some" class="headerlink" title="some()"></a>some()</h2><blockquote><p>对数组中的每一项运行给定函数，如果该函数对任一项返回<strong>true</strong>，则返回<strong>true</strong>；否则返回<strong>false</strong>。示例如下：</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> someResult = arr.some(<span class="function"><span class="keyword">function</span>(<span class="params">item</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (item &gt; <span class="number">2</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">alert(someResult); <span class="comment">//true</span></span><br></pre></td></tr></table></figure><font color="red"><em> 注意：</em>every()和some()方法较为相似。要区分它们的判断条件。前者需全部满足才返回true，后者只需一项满足即可。</font><h2 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h2><blockquote><p>对数组中的每一项运行给定函数，返回该函数会返回<strong>true</strong>的项组成的数组。示例如下：</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> filterResult = arr.filter(<span class="function"><span class="keyword">function</span>(<span class="params">item</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (item &gt; <span class="number">2</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">alert(filterResult);    <span class="comment">//[3,4,5,4,3]</span></span><br></pre></td></tr></table></figure><h2 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h2><blockquote><p>对数组中的每一项运行给定函数，返回每次函数调用的结果组成的数组。示例如下：</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> mapResult = arr.map(<span class="function"><span class="keyword">function</span>(<span class="params">item</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (item*<span class="number">2</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">alert(mapResult);   <span class="comment">//[2,4,6,8,10,8,6,4,2]</span></span><br></pre></td></tr></table></figure><font color="red"> <em>注意：</em> filter()和map()返回的都是新的数组，但不改变原数组的值。</font><h2 id="forEach"><a href="#forEach" class="headerlink" title="forEach()"></a>forEach()</h2><blockquote><p>对数组中的每一项运行给定函数，这个函数没有返回值,本质上与使用for循环迭代数组一样。示例如下：</p></blockquote><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">arr.forEach( <span class="function"><span class="keyword">function</span>(<span class="params">item, index</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// 执行某些操作</span></span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'index:'</span>+index+<span class="string">' item:'</span>+item);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><blockquote><p>数组对象是javascript中常用的类型，要熟悉数组的各种方法，需在平常开发中反复练习并理解记忆。</p></blockquote><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/5189ba24.html">https://gongjintao.com/post/5189ba24.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ECMAScript-5中的数组迭代方法总结&quot;&gt;&lt;a href=&quot;#ECMAScript-5中的数组迭代方法总结&quot; class=&quot;headerlink&quot; title=&quot;ECMAScript 5中的数组迭代方法总结&quot;&gt;&lt;/a&gt;&lt;em&gt;ECMAScript 5&lt;/em&gt;中的数组迭代方法总结&lt;/h1&gt;&lt;p&gt;在 &lt;strong&gt;&lt;em&gt;ECMAscript 5&lt;/em&gt;&lt;/strong&gt; 中为数组定义了5个迭代方法，每个方法都接收两个参数：
    
    </summary>
    
      <category term="Web前端" scheme="https://gongjintao.com/categories/Web%E5%89%8D%E7%AB%AF/"/>
    
    
      <category term="JavaScript" scheme="https://gongjintao.com/tags/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title>CCF认证题库201712-2————游戏</title>
    <link href="https://gongjintao.com/post/8cf8d4b3.html"/>
    <id>https://gongjintao.com/post/8cf8d4b3.html</id>
    <published>2019-06-30T01:28:11.244Z</published>
    <updated>2020-02-22T08:54:27.246Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>　有n个小朋友围成一圈玩游戏，小朋友从1至n编号，2号小朋友坐在1号小朋友的顺时针方向，3号小朋友坐在2号小朋友的顺时针方向，……，1号小朋友坐在n号小朋友的顺时针方向。<br>游戏开始，从1号小朋友开始顺时针报数，接下来每个小朋友的报数是上一个小朋友报的数加1。若一个小朋友报的数为k的倍数或其末位数（即数的个位）为k，则该小朋友被淘汰出局，不再参加以后的报数。当游戏中只剩下一个小朋友时，该小朋友获胜。<a id="more"></a><br>例如，当n=5, k=2时：<br>　　1号小朋友报数1；<br>　　2号小朋友报数2淘汰；<br>　　3号小朋友报数3；<br>　　4号小朋友报数4淘汰；<br>　　5号小朋友报数5；<br>　　1号小朋友报数6淘汰；<br>　　3号小朋友报数7；<br>　　5号小朋友报数8淘汰；<br>　　3号小朋友获胜。<br>　　给定n和k，请问最后获胜的小朋友编号为多少？</p></blockquote><h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><blockquote><p>　　输入一行，包括两个整数n和k，意义如题目所述。</p></blockquote><h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><blockquote><p>　　输出一行，包含一个整数，表示获胜的小朋友编号。</p></blockquote><h3 id="样例输入"><a href="#样例输入" class="headerlink" title="样例输入"></a>样例输入</h3><blockquote><p>5 2</p></blockquote><h3 id="样例输出"><a href="#样例输出" class="headerlink" title="样例输出"></a>样例输出</h3><blockquote><p>3</p></blockquote><h3 id="样例输入-1"><a href="#样例输入-1" class="headerlink" title="样例输入"></a>样例输入</h3><blockquote><p>7 3</p></blockquote><h3 id="样例输出-1"><a href="#样例输出-1" class="headerlink" title="样例输出"></a>样例输出</h3><blockquote><p>4</p></blockquote><h3 id="数据规模和约定"><a href="#数据规模和约定" class="headerlink" title="数据规模和约定"></a>数据规模和约定</h3><blockquote><p>对于所有评测用例，1 ≤ n ≤ 1000，1 ≤ k ≤ 9。</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><blockquote><p>可以设置数组初始值为0，淘汰的便设置为1；<br>然后逐步循环；<br>设置判断条件，即数组中0的个数为1，表明只有一个未被淘汰，退出循环汰。</p></blockquote><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">check</span><span class="params">(<span class="keyword">int</span> a[],<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum=<span class="number">0</span>,i;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">if</span>(a[i]==<span class="number">0</span>)</span><br><span class="line">      &#123;</span><br><span class="line">        sum++;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(sum==<span class="number">1</span>)</span><br><span class="line">       <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">       <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">   <span class="keyword">int</span> n,k,i=<span class="number">0</span>,data=<span class="number">1</span>;</span><br><span class="line">   <span class="built_in">cin</span>&gt;&gt;n&gt;&gt;k;</span><br><span class="line">   <span class="keyword">int</span> arr[n];</span><br><span class="line">   <span class="built_in">memset</span>(arr,<span class="number">0</span>,n);</span><br><span class="line">   <span class="keyword">while</span>(check(arr,n))</span><br><span class="line">   &#123;</span><br><span class="line">     <span class="keyword">if</span>(arr[i]==<span class="number">0</span>)</span><br><span class="line">     &#123;</span><br><span class="line">       data++;</span><br><span class="line">      &#125;</span><br><span class="line">     <span class="keyword">if</span>(data%k==<span class="number">0</span>||data%<span class="number">10</span>==k)</span><br><span class="line">     &#123;</span><br><span class="line">      arr[i]=<span class="number">1</span>;</span><br><span class="line">     &#125;</span><br><span class="line">      i++;</span><br><span class="line">      <span class="keyword">if</span>(i==n)</span><br><span class="line">     &#123;</span><br><span class="line">       i=<span class="number">0</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">   <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">     <span class="keyword">if</span>(arr[i]==<span class="number">0</span>)&#123;</span><br><span class="line">       <span class="built_in">cout</span>&lt;&lt;i+<span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="声明：菜鸟拙见，大神勿喷"><a href="#声明：菜鸟拙见，大神勿喷" class="headerlink" title="(声明：菜鸟拙见，大神勿喷)"></a><font color="red" size="4" face="宋体">(声明：菜鸟拙见，大神勿喷)</font></h2><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/8cf8d4b3.html">https://gongjintao.com/post/8cf8d4b3.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;　有n个小朋友围成一圈玩游戏，小朋友从1至n编号，2号小朋友坐在1号小朋友的顺时针方向，3号小朋友坐在2号小朋友的顺时针方向，……，1号小朋友坐在n号小朋友的顺时针方向。&lt;br&gt;游戏开始，从1号小朋友开始顺时针报数，接下来每个小朋友的报数是上一个小朋友报的数加1。若一个小朋友报的数为k的倍数或其末位数（即数的个位）为k，则该小朋友被淘汰出局，不再参加以后的报数。当游戏中只剩下一个小朋友时，该小朋友获胜。
    
    </summary>
    
      <category term="CCF试题库" scheme="https://gongjintao.com/categories/CCF%E8%AF%95%E9%A2%98%E5%BA%93/"/>
    
    
      <category term="C/C++" scheme="https://gongjintao.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>CCF认证题库201403-4————无线网络</title>
    <link href="https://gongjintao.com/post/b6dc74a0.html"/>
    <id>https://gongjintao.com/post/b6dc74a0.html</id>
    <published>2019-03-03T11:27:46.000Z</published>
    <updated>2020-02-22T08:54:26.888Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>　目前在一个很大的平面房间里有 n 个无线路由器，每个无线路由器都固定在某个点上。任何两个无线路由器只要距离不超过 r 就能互相建立网络连接。<br>　　除此以外，另有 m 个可以摆放无线路由器的位置。你可以在这些位置中选择至多 k 个增设新的路由器。<br>　　你的目标是使得第 1 个路由器和第 2 个路由器之间的网络连接经过尽量少的中转路由器。请问在最优方案下中转路由器的最少个数是多少？</p></blockquote><a id="more"></a><h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><blockquote><p>　第一行包含四个正整数 n,m,k,r。(2 ≤ n ≤ 100,1 ≤ k ≤ m ≤ 100, 1 ≤ r ≤ 108)。<br>　　接下来 n 行，每行包含两个整数 xi 和 yi, 表示一个已经放置好的无线 路由器在 (xi, yi) 点处。输入数据保证第 1 和第 2 个路由器在仅有这 n 个路由器的情况下已经可以互相连接 (经过一系列的中转路由器)。<br>　　接下来 m 行，每行包含两个整数 xi 和 yi, 表示 (xi, yi) 点处可以增设 一个路由器。<br>　　输入中所有的坐标的绝对值不超过 108, 保证输入中的坐标各不相同。</p></blockquote><h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><blockquote><p>输出只有一个数，即在指定的位置中增设 k 个路由器后，从第 1 个路 由器到第 2 个路由器最少经过的中转路由器的个数。</p></blockquote><h3 id="样例输入"><a href="#样例输入" class="headerlink" title="样例输入"></a>样例输入</h3><blockquote><p>5 3 1 3<br>0 0<br>5 5<br>0 3<br>0 5<br>3 5<br>3 3<br>4 4<br>3 0</p></blockquote><h3 id="样例输出"><a href="#样例输出" class="headerlink" title="样例输出"></a>样例输出</h3><blockquote><p>2</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><blockquote><p>此题属于求最短路，可以用 BFS (广度优先搜索) 快速方便解出，因为从顶点出发，层序遍历时，最优先到达终点的路径就是最短路径。需要注意的是，增设的路由器只有  k 个，所有对每个点进行遍历时，需要对 k 进行判断。但不是设一个全局的 cnt 来表示增设的路由器，应该是这条路径上的增设的路由器个数，因为如果设全局，即便不在该路径上，但只要遍历到了增设位置， cnt 就会加 1，使得判断失误。所以应该对每个顶点结果体设一个单独的 cnt，用来累计从起点一直到当前点所增设的路由器个数。</p></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAXN = <span class="number">201</span>; <span class="comment">// 最大点数 </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">dot</span>&#123;</span></span><br><span class="line">ll x, y; <span class="comment">// 点的坐标</span></span><br><span class="line"><span class="keyword">int</span> level; <span class="comment">// 层次，用来计算路径长度</span></span><br><span class="line"><span class="keyword">bool</span> flag; <span class="comment">// 用于判断点是否是新增的点 </span></span><br><span class="line"><span class="keyword">int</span> cnt; <span class="comment">// 用于累计从起点到当前点增设的路由器的个数</span></span><br><span class="line">&#125;Dot[MAXN];</span><br><span class="line"><span class="comment">// 原来路由器个数，另有m个可以摆放路由器的位置，可以增设的路由器个数, 半径 </span></span><br><span class="line"><span class="keyword">int</span> n, m, k, r;</span><br><span class="line"><span class="comment">// 图 </span></span><br><span class="line"><span class="keyword">int</span> G[MAXN][MAXN];</span><br><span class="line"><span class="comment">// 是否被访问 </span></span><br><span class="line"><span class="keyword">bool</span> vis[MAXN] = &#123;<span class="literal">false</span>&#125;; </span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断两点是否可达 </span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">inRange</span><span class="params">(<span class="keyword">int</span> u, <span class="keyword">int</span> v)</span> </span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">pow</span>(Dot[u].x-Dot[v].x, <span class="number">2</span>) + <span class="built_in">pow</span>(Dot[u].y-Dot[v].y, <span class="number">2</span>) &lt;= <span class="built_in">pow</span>(r, <span class="number">2</span>);</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">BFS</span><span class="params">(<span class="keyword">int</span> st, <span class="keyword">int</span> ed)</span> </span>&#123;</span><br><span class="line"><span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q; </span><br><span class="line">q.push(st);</span><br><span class="line">Dot[st].level = <span class="number">0</span>; <span class="comment">// 初始化顶点层次为 0 </span></span><br><span class="line">Dot[st].cnt = <span class="number">0</span>; <span class="comment">// 初始化访问顶点时，增设点数为 0 </span></span><br><span class="line">vis[st] = <span class="literal">true</span>; <span class="comment">// 设置被访问 </span></span><br><span class="line"><span class="keyword">while</span>(!q.empty())&#123;</span><br><span class="line"><span class="keyword">int</span> u = q.front(); <span class="comment">//取出顶点</span></span><br><span class="line">q.pop(); <span class="comment">// 弹出顶点 </span></span><br><span class="line"><span class="keyword">if</span>(u == ed) <span class="keyword">return</span> Dot[u].level - <span class="number">1</span>;</span><br><span class="line"><span class="comment">// 总顶点数为 n + m </span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> v = <span class="number">1</span>; v &lt;= n + m; v++)&#123;</span><br><span class="line"><span class="comment">// 如果 顶点可达 &amp;&amp; 顶点未被访问 </span></span><br><span class="line"><span class="keyword">if</span>( G[u][v] == <span class="number">1</span> &amp;&amp; vis[v] == <span class="literal">false</span> &amp;&amp; Dot[u].cnt &lt;= k) &#123;</span><br><span class="line">vis[v] = <span class="literal">true</span>; </span><br><span class="line">                <span class="comment">// 对增设的路由进行判断，如果当前是增设的路由，则路径上的增设路由加 1，否则不变</span></span><br><span class="line">Dot[v].cnt = (Dot[v].flag) ? Dot[u].cnt+<span class="number">1</span> : Dot[u].cnt; </span><br><span class="line">q.push(v);</span><br><span class="line">Dot[v].level = Dot[u].level + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;n&gt;&gt;m&gt;&gt;k&gt;&gt;r;</span><br><span class="line">    <span class="comment">// 设图的初值均为 0，表示不可达</span></span><br><span class="line">fill(G[<span class="number">0</span>], G[<span class="number">0</span>] + MAXN*MAXN, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= n+m; i++)&#123;</span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;Dot[i].x&gt;&gt;Dot[i].y;</span><br><span class="line"><span class="keyword">if</span>(i &lt;= n) Dot[i].flag = <span class="literal">false</span>;<span class="comment">// 设置是否是增设位置</span></span><br><span class="line"><span class="keyword">else</span> Dot[i].flag = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">    <span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= MAXN; i++)&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = i+<span class="number">1</span>; j &lt;= MAXN; j++)&#123;</span><br><span class="line"><span class="keyword">if</span>(inRange(i, j)) G[i][j] = G[j][i] = <span class="number">1</span>;<span class="comment">// 如果在半径内，则设为 1 表示两点可达</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> step = BFS(<span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;step&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="声明：菜鸟拙见，大神勿喷"><a href="#声明：菜鸟拙见，大神勿喷" class="headerlink" title="(声明：菜鸟拙见，大神勿喷)"></a><font color="red" size="4" face="宋体">(声明：菜鸟拙见，大神勿喷)</font></h2><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/b6dc74a0.html">https://gongjintao.com/post/b6dc74a0.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;　目前在一个很大的平面房间里有 n 个无线路由器，每个无线路由器都固定在某个点上。任何两个无线路由器只要距离不超过 r 就能互相建立网络连接。&lt;br&gt;　　除此以外，另有 m 个可以摆放无线路由器的位置。你可以在这些位置中选择至多 k 个增设新的路由器。&lt;br&gt;　　你的目标是使得第 1 个路由器和第 2 个路由器之间的网络连接经过尽量少的中转路由器。请问在最优方案下中转路由器的最少个数是多少？&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="CCF试题库" scheme="https://gongjintao.com/categories/CCF%E8%AF%95%E9%A2%98%E5%BA%93/"/>
    
    
      <category term="C/C++" scheme="https://gongjintao.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>CCF认证题库201312-4————有趣的数</title>
    <link href="https://gongjintao.com/post/bfb9437a.html"/>
    <id>https://gongjintao.com/post/bfb9437a.html</id>
    <published>2019-03-01T02:27:46.000Z</published>
    <updated>2020-02-22T08:54:26.871Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>我们把一个数称为有趣的，当且仅当：</p><ol><li><p><strong>它的数字只包含 0, 1, 2, 3</strong>，且这四个数字都出现过至少一次。</p></li><li><p>所有的 0 都出现在所有的 1 之前，而所有的 2 都出现在所有的 3 之前。</p></li><li><p>最高位数字不为 0。<br><a id="more"></a>因此，符合我们定义的最小的有趣的数是 2013。除此以外，4 位的有趣的数还有两个：2031 和 2301。<br>请计算恰好有 n 位的有趣的数的个数。由于答案可能非常大，只需要输出答案除以 1000000007 的余数。</p></li></ol></blockquote><h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><blockquote><p>输入只有一行，包括恰好一个正整数 n (4 ≤ n ≤ 1000)。</p></blockquote><h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><blockquote><p>输出只有一行，包括恰好 n 位的整数中有趣的数的个数除以 1000000007 的余数。</p></blockquote><h3 id="样例输入"><a href="#样例输入" class="headerlink" title="样例输入"></a>样例输入</h3><blockquote><p>4</p></blockquote><h3 id="样例输出"><a href="#样例输出" class="headerlink" title="样例输出"></a>样例输出</h3><blockquote><p>3</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><blockquote><p>此题采用动态规划的解法。</p><p>由题可知，1 和 3 分别出现在 0 和 2 之后，所以先考虑 0 或 2 的位置，又由于 0 不能摆在最高位，所以先考虑 2 的摆放。在不考虑所有数字都齐的情况下，则共有如下六种合法的状态：</p><ol><li>只含 2；</li><li>只含 2, 0;</li><li>只含 2, 3;</li><li>只含 2, 0, 1;</li><li>只含 2, 0, 3;</li><li>包含 4 种数字。</li></ol><p>用 <code>dp[i][j],j=1,2,3,4,5</code>，来表示长度为<code>i</code>的整数，满足上面的第<code>j</code>号状态的个数。</p><p>如<code>dp[3][1]=1</code>,即长度为 3， 只包含 2 的整数只有 1 个，即<code>222</code>；</p><p>如<code>dp[3][2]=2</code>，即长度为 3， 只包含 2 和 0 的整数有 2 个，因为所有的 2 必须在 3 前面，则只有三种情况，即<code>223,233</code>；</p><p>如<code>dp[4][3]=3</code>，即长度为4，只包含 2 和 3 的整数有 3 个，因为所有的 2 必须在 3 前面，则只有三种情况，即<code>2223,2233,2333</code>；</p><p>…</p><p>所以我们可以推出状态转移方程如下：</p><ol><li><code>dp[i][1]=1</code>;</li><li><code>dp[i][2]=2*dp[i-1][2]+dp[i-1][1]</code>，即位数为 i 且只含2、0的整数<ul><li>可以由位数为 i-1 且只含2、0的整数在末尾添加 0 或 2 所得；</li><li>可以由位数为 i-1 且只含 2 的整数在末尾添加 0 所得。</li></ul></li><li><code>dp[i][3]=dp[i-1][3]+dp[i-1][1]</code>，即位数为 i 且只含 2、3 的整数<ul><li>可以由位数为 i-1 且只含2、3的整数在末尾添加 3 可得；</li><li>可以由为数为 i-1 且只含 2 的整数在末尾添加 3 可得。</li></ul></li><li><code>dp[i][4]=2*dp[i-1][4]+dp[i-1][2]</code>，即位数为 i 且只含 2、0 和 1 的整数<ul><li>可以由位数为 i-1 且只含2、0 和 1 的整数在末尾添加 1 或 2 可得；</li><li>可以由为数为 i-1 且只含 2 或 0 的整数在末尾添加 1 可得。</li></ul></li><li><code>dp[i][5]=2*dp[i-1][5]+dp[i-1][2]+dp[i-1][3]</code>，即位数为 i 且只含 2、0 和 3 的整数<ul><li>可以由位数为 i-1 且只含2、0 和 3 的整数在末尾添加 2 或 3 可得；</li><li>可以由为数为 i-1 且只含 2 或 0 的整数在末尾添加 3 可得。</li><li>可以由为数为 i-1 且只含 2 或 3 的整数在末尾添加 0 可得。</li></ul></li><li><code>dp[i][6]=2*dp[i-1][6]+dp[i-1][4]+dp[i-1][5]</code>，即位数为 i 且包含 4 种数字的整数<ul><li>可以由位数为 i-1 且包含 4 种数字的整数在末尾添加 1 或 3 可得；</li><li>可以由为数为 i-1 且只含 2 、0 和  1 的整数在末尾添加 3 可得;</li><li>可以由为数为 i-1 且只含 2 、0 和  3 的整数在末尾添加 1 可得</li></ul></li></ol></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> mod 1000000007</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">1002</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> dp[maxn][<span class="number">6</span>];</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span> <span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="built_in">memset</span>(dp,<span class="number">0</span>,<span class="keyword">sizeof</span>(dp));</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;n;</span><br><span class="line">dp[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= n; i++)&#123;</span><br><span class="line">dp[i][<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">dp[i][<span class="number">2</span>]=(<span class="number">2</span>*dp[i<span class="number">-1</span>][<span class="number">2</span>] % mod + dp[i<span class="number">-1</span>][<span class="number">1</span>]) % mod;</span><br><span class="line">dp[i][<span class="number">3</span>]=(dp[i<span class="number">-1</span>][<span class="number">3</span>] + dp[i<span class="number">-1</span>][<span class="number">1</span>]) % mod;</span><br><span class="line">dp[i][<span class="number">4</span>]=(<span class="number">2</span>*dp[i<span class="number">-1</span>][<span class="number">4</span>] % mod + dp[i<span class="number">-1</span>][<span class="number">2</span>]) % mod;</span><br><span class="line">dp[i][<span class="number">5</span>]=((<span class="number">2</span>*dp[i<span class="number">-1</span>][<span class="number">5</span>] % mod + dp[i<span class="number">-1</span>][<span class="number">2</span>]) % mod + dp[i<span class="number">-1</span>][<span class="number">3</span>]) % mod;</span><br><span class="line">dp[i][<span class="number">6</span>]=((<span class="number">2</span>*dp[i<span class="number">-1</span>][<span class="number">6</span>] % mod + dp[i<span class="number">-1</span>][<span class="number">4</span>]) % mod + dp[i<span class="number">-1</span>][<span class="number">5</span>]) % mod;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;dp[n][<span class="number">6</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="声明：菜鸟拙见，大神勿喷"><a href="#声明：菜鸟拙见，大神勿喷" class="headerlink" title="(声明：菜鸟拙见，大神勿喷)"></a><font color="red" size="4" face="宋体">(声明：菜鸟拙见，大神勿喷)</font></h2><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/bfb9437a.html">https://gongjintao.com/post/bfb9437a.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;我们把一个数称为有趣的，当且仅当：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;它的数字只包含 0, 1, 2, 3&lt;/strong&gt;，且这四个数字都出现过至少一次。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;所有的 0 都出现在所有的 1 之前，而所有的 2 都出现在所有的 3 之前。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最高位数字不为 0。&lt;br&gt;
    
    </summary>
    
      <category term="CCF试题库" scheme="https://gongjintao.com/categories/CCF%E8%AF%95%E9%A2%98%E5%BA%93/"/>
    
    
      <category term="C/C++" scheme="https://gongjintao.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>MVC和MVVM模式特点和区别</title>
    <link href="https://gongjintao.com/post/ec52047e.html"/>
    <id>https://gongjintao.com/post/ec52047e.html</id>
    <published>2019-01-19T08:07:46.000Z</published>
    <updated>2020-02-22T08:54:27.400Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h1><blockquote><p>MVC 和 MVVM 都是常见的软件架构设计模式（Architectural Pattern），它通过分离关注点来改进代码的组织方式。它们的不同部分是 C(Controller)、VM(View-Model)，而相同的部分则是 MV(Model-View)。</p></blockquote><a id="more"></a><h1 id="2-MVC"><a href="#2-MVC" class="headerlink" title="2. MVC"></a>2. MVC</h1><blockquote><p><strong>M(Model)</strong>：<strong>数据保存层</strong>，用于存储业务的数据，一旦数据发生变化，Model 将发送到相关的 View，用户得到反馈</p><p><strong>V(View)</strong>：<strong>视图层</strong>，用于向用户展示数据，并传送指令到 Controller</p><p><strong>C(Controller)</strong>: <strong>业务逻辑层</strong>，用于完成用户请求的事件，然后通知 Model 改变数据</p></blockquote><p>图示如下：</p><p><img src="http://gjtblog-images.oss-cn-beijing.aliyuncs.com/MVC.jpg" alt="MVC"></p><blockquote><p>MVC 允许在不改变视图的情况下改变视图对用户输入的响应方式，用户对 View 的操作交给了 Controller 处理，在 Controller 中响应 View 的事件调用 Model 的接口对数据进行操作，一旦 Model 发生变化便通知相关视图进行更新。<font color="red">注意：MVC 中也可以直接通过 Controller 接受指令。</font></p></blockquote><h1 id="3-MVVM"><a href="#3-MVVM" class="headerlink" title="3. MVVM"></a>3. MVVM</h1><blockquote><p><strong>M(Model)</strong>：<strong>数据层</strong>，不同于 MVC 中的 Model , MVVM 中的 Model 层只关注数据本身，不关心任何行为。</p><p><strong>V(View)</strong>：<strong>视图层</strong>，用于向用户渲染展示数据</p><p><strong>VM(View-Model)</strong>: <strong>视图逻辑层</strong>，可以实现数据的双向绑定，当 Model 发生变化，ViewModel 就会自动更新；ViewModel 变化，Model 也会更新。View 的变动，自动反映在 ViewModel，而ViewModel 的更新，也会自动引起 View 的变化。</p></blockquote><p>图示如下：</p><p><img src="https://gjtblog-images.oss-cn-beijing.aliyuncs.com/MVVM.jpg" alt="MVVM"></p><blockquote><p>整体来看，比 MVC/MVP 精简了很多，不仅仅简化了业务与界面的依赖，还解决了数据频繁更新，不用操作DOM。因为在 MVVM 中，View 不知道 Model 的存在，ViewModel 和 Model 也察觉不到 View，这种低耦合模式可以使开发过程更加容易，提高应用的可重用性。</p></blockquote><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/ec52047e.html">https://gongjintao.com/post/ec52047e.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1.概述&quot;&gt;&lt;/a&gt;1.概述&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;MVC 和 MVVM 都是常见的软件架构设计模式（Architectural Pattern），它通过分离关注点来改进代码的组织方式。它们的不同部分是 C(Controller)、VM(View-Model)，而相同的部分则是 MV(Model-View)。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Web前端" scheme="https://gongjintao.com/categories/Web%E5%89%8D%E7%AB%AF/"/>
    
    
  </entry>
  
  <entry>
    <title>ECMAScript6学习笔记</title>
    <link href="https://gongjintao.com/post/92cd569b.html"/>
    <id>https://gongjintao.com/post/92cd569b.html</id>
    <published>2019-01-06T05:12:15.000Z</published>
    <updated>2020-02-22T08:54:27.577Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-变量"><a href="#1-变量" class="headerlink" title="1.变量"></a>1.变量</h3><blockquote><p><strong><code>var</code>声明变量的问题</strong></p><ol><li>可以重复声明变量</li><li>无法限制修改</li><li>没有块级作用域</li></ol><p><strong>ES6中新增了两个新的声明方法 <code>let</code>和<code>const</code></strong></p><ul><li><code>let</code>     不能重复声明，用于声明变量，有块级作用域</li><li><code>const</code>   不能重复声明，用于声明常量，有块级作用域</li></ul></blockquote><h3 id="2-函数"><a href="#2-函数" class="headerlink" title="2.函数"></a>2.函数<a id="more"></a></h3><h4 id="2-1-箭头函数：-gt"><a href="#2-1-箭头函数：-gt" class="headerlink" title="2.1 箭头函数：()=&gt;{}"></a>2.1 <strong>箭头函数</strong>：<code>()=&gt;{}</code></h4><p>&gt;</p><blockquote><ul><li><p>如果只有一个参数，<code>()</code>则可以省略</p></li><li><p>如果只有一个<code>return</code>,<code>{}</code>可以省略</p></li></ul></blockquote><h4 id="2-2-函数的参数"><a href="#2-2-函数的参数" class="headerlink" title="2.2 函数的参数"></a>2.2 <strong>函数的参数</strong></h4><h5 id="2-2-1-参数扩展-展开"><a href="#2-2-1-参数扩展-展开" class="headerlink" title="2.2.1 参数扩展/展开"></a>2.2.1 参数扩展/展开</h5><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//收集参数</span></span><br><span class="line"> <span class="function"><span class="keyword">function</span> <span class="title">show</span>(<span class="params">a,b,...args</span>)</span>&#123;&#125;</span><br><span class="line"> </span><br><span class="line">show(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>);  <span class="comment">//...arg会接收剩余的参数(3,4,5)，但必须时最后一个位置</span></span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//展开数组</span></span><br><span class="line"><span class="keyword">let</span> arr =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>];</span><br><span class="line">...arr; <span class="comment">//相当于将arr展开:1,2,3</span></span><br></pre></td></tr></table></figure><h5 id="2-2-2-默认参数"><a href="#2-2-2-默认参数" class="headerlink" title="2.2.2 默认参数"></a>2.2.2 <strong>默认参数</strong></h5><blockquote><p>即参数一开始就有默认值</p></blockquote><h3 id="3-解构赋值"><a href="#3-解构赋值" class="headerlink" title="3.解构赋值"></a>3.解构赋值</h3><blockquote><ol><li><p>左右两边结构必须一样</p></li><li><p>右边必须是个有值的变量</p></li><li><p>生命和赋值不能分开（必须在一句语句中完成）</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> [a,b,c] = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]; <span class="comment">//数组</span></span><br><span class="line"><span class="keyword">let</span> &#123;a,b,c&#125; = &#123;<span class="attr">a</span>:<span class="number">1</span>, <span class="attr">b</span>:<span class="number">2</span>, <span class="attr">c</span>:<span class="number">8</span>&#125;;<span class="comment">//json</span></span><br><span class="line"><span class="keyword">var</span> &#123; bar, foo &#125; = &#123; <span class="attr">foo</span>: <span class="string">"aaa"</span>, <span class="attr">bar</span>: <span class="string">"bbb"</span> &#125;;<span class="comment">//对象</span></span><br></pre></td></tr></table></figure></li></ol></blockquote><h3 id="4-数组"><a href="#4-数组" class="headerlink" title="4.数组"></a>4.数组</h3><h4 id="1-map-一对一的映射"><a href="#1-map-一对一的映射" class="headerlink" title="1. map(一对一的映射)"></a>1. map(一对一的映射)</h4><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> arr = [<span class="number">12</span>,<span class="number">5</span>,<span class="number">8</span>];</span><br><span class="line"><span class="comment">/*let result = arr.map(function(item)&#123;</span></span><br><span class="line"><span class="comment">    return item*2;</span></span><br><span class="line"><span class="comment">&#125;)*/</span></span><br><span class="line"><span class="keyword">let</span> result = arr.map(<span class="function"><span class="params">item</span>=&gt;</span>item*<span class="number">2</span>);</span><br><span class="line">alert(result); <span class="comment">//输出24,10,16</span></span><br></pre></td></tr></table></figure><h4 id="2-reduce-一堆对一个"><a href="#2-reduce-一堆对一个" class="headerlink" title="2. reduce(一堆对一个)"></a>2. reduce(一堆对一个)</h4><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> arr =[<span class="number">12</span>,<span class="number">34</span>,<span class="number">5</span>,<span class="number">33</span>];</span><br><span class="line"> </span><br><span class="line"> <span class="comment">//求平均值</span></span><br><span class="line"> <span class="keyword">let</span> result = arr.reduce(<span class="function"><span class="keyword">function</span>(<span class="params">tmp,item,index</span>)</span>&#123;</span><br><span class="line">     <span class="keyword">if</span>(index!=arr.length<span class="number">-1</span>)&#123; <span class="comment">//不是最后一次运算</span></span><br><span class="line">         <span class="keyword">return</span> tmp+item;</span><br><span class="line">     &#125;<span class="keyword">else</span>&#123;<span class="comment">//最后一次运算</span></span><br><span class="line">        <span class="keyword">return</span> (tmp+item)/arr.length;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;)</span><br><span class="line"> alert(result);<span class="comment">//21</span></span><br></pre></td></tr></table></figure><h4 id="3-filter-过滤器"><a href="#3-filter-过滤器" class="headerlink" title="3. filter(过滤器)"></a>3. filter(过滤器)</h4><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">var</span> filterResult = arr.filter(<span class="function"><span class="keyword">function</span>(<span class="params">item</span>)</span>&#123;</span><br><span class="line">       <span class="keyword">return</span> (item &gt; <span class="number">2</span>);</span><br><span class="line">   &#125;);</span><br><span class="line">   </span><br><span class="line">   alert(filterResult);    <span class="comment">//[3,4,5,4,3]</span></span><br></pre></td></tr></table></figure><h4 id="4-forEach-迭代"><a href="#4-forEach-迭代" class="headerlink" title="4. forEach(迭代)"></a>4. forEach(迭代)</h4> <figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">var</span> arr = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>];</span><br><span class="line"> </span><br><span class="line"> arr.forEach( <span class="function"><span class="keyword">function</span>(<span class="params">item, index</span>) </span>&#123;</span><br><span class="line">     <span class="comment">// 执行某些操作</span></span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'index:'</span>+index+<span class="string">' item:'</span>+item);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="5-字符串"><a href="#5-字符串" class="headerlink" title="5.字符串"></a>5.字符串</h3><h4 id="5-1-两个新方法"><a href="#5-1-两个新方法" class="headerlink" title="5.1 两个新方法"></a>5.1 两个新方法</h4><blockquote><p><strong>startsWith(用于判断字符串的开头)</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> str =<span class="string">'https://www.baidu.com'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(str.startsWith(<span class="string">'https://'</span>))&#123;</span><br><span class="line">    alert(<span class="string">"加密网址"</span>)；</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    alert(<span class="string">"普通网址"</span>)；</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>endsWith(用于判断字符串的结尾)</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> str=<span class="string">"name.txt"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(str.endsWith(<span class="string">'.txt'</span>))&#123;</span><br><span class="line">    alert(<span class="string">"文本文件"</span>);</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span>(str.endsWith(<span class="string">'.jpg'</span>))&#123;</span><br><span class="line">    alert(<span class="string">"JPG图片"</span>);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    alert(<span class="string">"其他"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><h4 id="5-2-模板字符串-用反单引号-来定义字符串"><a href="#5-2-模板字符串-用反单引号-来定义字符串" class="headerlink" title="5.2. 模板字符串(用反单引号`来定义字符串)"></a>5.2. 模板字符串(用反单引号`来定义字符串)</h4><blockquote><ul><li>可以直接将在字符串中解析变量，将变量写在 ${}中；</li><li>可以定义多行字符串</li></ul></blockquote><h3 id="6-面向对象"><a href="#6-面向对象" class="headerlink" title="6.面向对象"></a>6.面向对象</h3><blockquote><ol><li><p><strong>class关键字、构造器和类分开了</strong></p></li><li><p><strong>class里面可以直接加方法</strong></p></li></ol></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span></span>&#123;</span><br><span class="line">       <span class="keyword">constructor</span>(id,name)&#123;</span><br><span class="line">           <span class="keyword">this</span>.id = id;</span><br><span class="line">           <span class="keyword">this</span>.name =name;</span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">       showName()&#123;</span><br><span class="line">           alert(<span class="keyword">this</span>.name);</span><br><span class="line">       &#125;</span><br><span class="line">       showId()&#123;</span><br><span class="line">           alert(<span class="keyword">this</span>.id);</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">var</span> u1 =<span class="keyword">new</span> User(<span class="number">1</span>,<span class="string">'Tom'</span>);</span><br><span class="line">   u1.showName();</span><br><span class="line">   u1.showId();</span><br></pre></td></tr></table></figure><blockquote><p><strong>继承</strong></p></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VipUser</span> <span class="keyword">extends</span> <span class="title">User</span></span>&#123;</span><br><span class="line">    <span class="keyword">constructor</span>(id,name,level)&#123;</span><br><span class="line">        <span class="keyword">super</span>(id,name);</span><br><span class="line">        <span class="keyword">this</span>.level = level;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    showLevel()&#123;</span><br><span class="line">        alert(<span class="keyword">this</span>.level);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> v1 = <span class="keyword">new</span> VipUser(<span class="number">2</span>,<span class="string">'Jack'</span>,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">v1.showName();</span><br><span class="line">v1.showId();</span><br><span class="line">v1.showLevel();</span><br></pre></td></tr></table></figure><h3 id="7-Promise"><a href="#7-Promise" class="headerlink" title="7.Promise"></a>7.Promise</h3><blockquote><p>所谓 Promise 对象，就是代表了未来某个将要发生的事件（通常是一个异步操作）。它的好处在于，有了 Promise 对象，就可以将异步操作以同步操作的流程表达出来，避免了层层嵌套的回调函数。</p></blockquote><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> p = <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function"><span class="keyword">function</span>(<span class="params">resolve,reject</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="comment">/*异步操作成功*/</span>)&#123;</span><br><span class="line">       resolve(value);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        reject(error);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">p.then(<span class="function"><span class="keyword">function</span>(<span class="params">value</span>)</span>&#123;</span><br><span class="line">    <span class="comment">//操作成功</span></span><br><span class="line">&#125;,<span class="function"><span class="keyword">function</span>(<span class="params">error</span>)</span>&#123;</span><br><span class="line">    <span class="comment">//操作失败</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><blockquote><p><strong>利用ajax请求数据</strong></p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"viewport"</span> <span class="attr">content</span>=<span class="string">"width=device-width, initial-scale=1.0"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"X-UA-Compatible"</span> <span class="attr">content</span>=<span class="string">"ie=edge"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span> </span><br><span class="line"><span class="javascript">        <span class="keyword">let</span> p = <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function"><span class="keyword">function</span>(<span class="params">resolve,reject</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">                $.ajax(&#123;</span></span><br><span class="line"><span class="javascript">                url: <span class="string">'data/abc.txt'</span>,</span></span><br><span class="line"><span class="javascript">                dataType:<span class="string">'json'</span>,</span></span><br><span class="line">                success(arr)&#123;</span><br><span class="line">                    resolve(arr);</span><br><span class="line">                &#125;,</span><br><span class="line">                error(err)&#123;</span><br><span class="line">                    reject(err);</span><br><span class="line">                &#125;</span><br><span class="line">             &#125;)</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line"><span class="javascript">        p.then(<span class="function"><span class="keyword">function</span>(<span class="params">arr</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">            alert(<span class="string">'成功了'</span>+arr); </span></span><br><span class="line"><span class="javascript">        &#125;,<span class="function"><span class="keyword">function</span>(<span class="params">err</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">            alert(<span class="string">'失败了'</span>+err);</span></span><br><span class="line">        &#125;)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>利用ajax请求多个数据</strong></p></blockquote><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">"en"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">"viewport"</span> <span class="attr">content</span>=<span class="string">"width=device-width, initial-scale=1.0"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">"X-UA-Compatible"</span> <span class="attr">content</span>=<span class="string">"ie=edge"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span>&gt;</span></span><br><span class="line">        </span><br><span class="line"><span class="javascript">        <span class="function"><span class="keyword">function</span> <span class="title">createPromise</span>(<span class="params">url</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">            <span class="keyword">return</span>  <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function"><span class="keyword">function</span>(<span class="params">resolve,reject</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">                $.ajax(&#123;</span></span><br><span class="line">                url,</span><br><span class="line"><span class="javascript">                dataType:<span class="string">'json'</span>,</span></span><br><span class="line">                success(arr)&#123;</span><br><span class="line">                    resolve(arr);</span><br><span class="line">                &#125;,</span><br><span class="line">                error(err)&#123;</span><br><span class="line">                    reject(err);</span><br><span class="line">                &#125;</span><br><span class="line">             &#125;)</span><br><span class="line">         &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="javascript">         <span class="built_in">Promise</span>.all([</span></span><br><span class="line"><span class="javascript">             createPromise(<span class="string">'data/abc.txt'</span>),<span class="comment">//也可直接写一个$.ajax(&#123;&#125;),因为其返回的就是一个Promise对象</span></span></span><br><span class="line"><span class="javascript">             createPromise(<span class="string">'data/json.txt'</span>)</span></span><br><span class="line"><span class="javascript">         ]).then(<span class="function"><span class="keyword">function</span>(<span class="params">arr</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">            <span class="keyword">let</span> [res1,res2] = arr;</span></span><br><span class="line">            alert(res1);</span><br><span class="line">            alert(res2);</span><br><span class="line"><span class="javascript">         &#125;,<span class="function"><span class="keyword">function</span>(<span class="params">err</span>)</span>&#123;</span></span><br><span class="line"><span class="javascript">             alert(<span class="string">'至少有一个失败了'</span>);</span></span><br><span class="line"><span class="javascript">             <span class="built_in">console</span>.log(err);</span></span><br><span class="line">         &#125;)</span><br><span class="line">  </span><br><span class="line">    <span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="8-Generator"><a href="#8-Generator" class="headerlink" title="8.Generator"></a>8.Generator</h3><blockquote><p>首先，可以把它理解成一个函数的内部状态的遍历器，每调用一次，函数的内部状态发生一次改变（可以理解成发生某些事件）。<br>在形式上，Generator 是一个普通函数，但是有两个特征。一是，function 命令与函数名之间有一个星号；二是，函数体内部使用 yield 语句，定义遍历器的每个成员，即不同的内部状态。</p><p><strong>通过yield传参</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> *<span class="title">show</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">            alert(<span class="string">'a'</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">let</span> a = <span class="keyword">yield</span>; <span class="comment">//yield可以传参</span></span><br><span class="line">    </span><br><span class="line">            alert(<span class="string">'b'</span>);</span><br><span class="line">            alert(a);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">let</span> gen = show();</span><br><span class="line">        gen.next(<span class="number">12</span>); <span class="comment">//第一个next()传参并没有用，所以这里弹出'a'</span></span><br><span class="line">        gen.next(<span class="number">5</span>);  <span class="comment">//next()传参会传给yield，所以弹出'b'之后，弹出的是5</span></span><br></pre></td></tr></table></figure></p><p><strong>通过yield返回值</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> *<span class="title">show</span>(<span class="params">a</span>)</span>&#123;            </span><br><span class="line">            alert(a);</span><br><span class="line">            <span class="keyword">let</span> tmp = a*<span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">             <span class="keyword">yield</span> tmp;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">let</span> b = tmp+<span class="number">1</span>; <span class="comment">//11</span></span><br><span class="line">            alert(b);</span><br><span class="line">    </span><br><span class="line">             <span class="keyword">yield</span> b;</span><br><span class="line">    </span><br><span class="line">           <span class="keyword">let</span>  c = b*<span class="number">3</span>;<span class="comment">//33</span></span><br><span class="line">            alert(c);</span><br><span class="line">    </span><br><span class="line">            <span class="keyword">return</span> c;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">let</span> gen = show(<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">let</span> res1 = gen.next(); <span class="comment">//&#123;value:10,done:false&#125;</span></span><br><span class="line">        <span class="built_in">console</span>.log(res1);</span><br><span class="line">        <span class="keyword">let</span> res2 = gen.next(); <span class="comment">//&#123;value:11，done:false&#125;</span></span><br><span class="line">        <span class="built_in">console</span>.log(res2);</span><br><span class="line">        <span class="keyword">let</span> res3 =gen.next(); <span class="comment">//&#123;value:33，done:true&#125;</span></span><br><span class="line">        <span class="built_in">console</span>.log(res3);</span><br><span class="line">    <span class="built_in">Promise</span>适合一次性读一堆数据的操作，Generator适合有逻辑性的读取的操作</span><br></pre></td></tr></table></figure></p></blockquote><h3 id="9-模块编程"><a href="#9-模块编程" class="headerlink" title="9. 模块编程"></a>9. 模块编程</h3><blockquote><p>在 ES6 中导入模块，使用 </p><ul><li><p><code>import 模块名 from ‘模块标识符’</code></p></li><li><p><code>import &#39;模块标识符&#39;</code> //例如导入css文件</p></li></ul><p>在 ES6 中使用<code>export default</code> 和 <code>export</code> 向外暴露成员</p><font color="red">注意：</font><ul><li><p><code>export default</code>向外暴露的成员，可以用任意变量来接收</p></li><li><p>在一个模块中，<code>export default</code> 只允许向外暴露一次</p></li><li><p>在一个模块中，可以同时使用 <code>export default</code> 和 <code>export</code> 向外暴露成员</p></li><li><p>使用 <code>export</code> 向外暴露的成员，只能使用<code>{}</code>的形式来接受，这种形式叫做<strong>【按需导出】</strong></p></li><li><p>使用<code>export</code> 向外暴露的成员，必须严格按照导出时的名称来接收，可以用 <code>as</code>来起别名</p></li></ul><p>相比在 <strong>node.js</strong> 中是使用<code>var 名称 = require(&#39;模块表示符&#39;)</code>来导入</p><p><strong>ES6</strong>中是用 <code>module.exports</code>和<code>exports</code>来暴露成员</p></blockquote><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/92cd569b.html">https://gongjintao.com/post/92cd569b.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-变量&quot;&gt;&lt;a href=&quot;#1-变量&quot; class=&quot;headerlink&quot; title=&quot;1.变量&quot;&gt;&lt;/a&gt;1.变量&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;var&lt;/code&gt;声明变量的问题&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;可以重复声明变量&lt;/li&gt;
&lt;li&gt;无法限制修改&lt;/li&gt;
&lt;li&gt;没有块级作用域&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;ES6中新增了两个新的声明方法 &lt;code&gt;let&lt;/code&gt;和&lt;code&gt;const&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;let&lt;/code&gt;     不能重复声明，用于声明变量，有块级作用域&lt;/li&gt;
&lt;li&gt;&lt;code&gt;const&lt;/code&gt;   不能重复声明，用于声明常量，有块级作用域&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;2-函数&quot;&gt;&lt;a href=&quot;#2-函数&quot; class=&quot;headerlink&quot; title=&quot;2.函数&quot;&gt;&lt;/a&gt;2.函数
    
    </summary>
    
      <category term="Web前端" scheme="https://gongjintao.com/categories/Web%E5%89%8D%E7%AB%AF/"/>
    
    
      <category term="ES6" scheme="https://gongjintao.com/tags/ES6/"/>
    
  </entry>
  
  <entry>
    <title>Vue中的生命周期</title>
    <link href="https://gongjintao.com/post/99504a05.html"/>
    <id>https://gongjintao.com/post/99504a05.html</id>
    <published>2019-01-05T14:25:24.000Z</published>
    <updated>2020-02-22T08:54:27.416Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-生命周期定义"><a href="#1-生命周期定义" class="headerlink" title="1.生命周期定义"></a>1.生命周期定义</h4><blockquote><p>生命周期是指：从Vue实例创建、运行、到销毁期间，总是伴随着各种事件的发生，这些事件统称为生命周期。</p></blockquote><a id="more"></a><h4 id="2-生命周期钩子"><a href="#2-生命周期钩子" class="headerlink" title="2.生命周期钩子"></a>2.生命周期钩子</h4><blockquote><p>生命周期钩子即生命周期事件的别名</p><p>生命周期钩子 = 生命周期函数 = 生命周期事件</p></blockquote><h4 id="3-生命周期函数分类"><a href="#3-生命周期函数分类" class="headerlink" title="3.生命周期函数分类"></a>3.生命周期函数分类</h4><blockquote><ol><li>创建期间的生命周期函数<ul><li><code>beforeCreate</code> ：实例刚在内存中被创建出来；此时，还没有初始化好 data 和 methods 属性</li><li><code>created</code> : 实例已经在内存中创建好，此时 data 和 methods 已经初始化好了</li><li><code>beforeMount</code>：表示模板已经在内存中编译完成，还没有渲染到页面中去</li><li><code>mounted</code>: 内存中的模板已经真实的挂载到浏览器中的页面中了，用户已经可以看到渲染好的页面了</li></ul></li><li>运行期间的生命周期函数<ul><li><code>beforeUpdate</code> : 状态更新前执行该函数，此时界面还没有被更新，但是数据已经被更新，因此还没有开始重新渲染DOM节点</li><li><code>updated</code>: 实例更新完毕之后调用该函数，页面和data数据已经保持同步了，都是最新的数据，界面已经被重新渲染好了</li></ul></li><li>销毁期间的生命周期函数<ul><li><code>beforeDestroy</code>: 实例销毁之前调用。Vue实例从运行阶段进入了销毁阶段，实例上的data 和 methods ，过滤器，指令等都处于可用阶段还没有被真正销毁</li><li><code>destroyed</code>:Vue实例已经被完全销毁，实例上的data 和 methods，过滤器，指令等都不可用</li></ul></li></ol></blockquote><h4 id="4-生命周期函数图"><a href="#4-生命周期函数图" class="headerlink" title="4.生命周期函数图"></a>4.生命周期函数图</h4><p><img src="http://gjtblog-images.oss-cn-beijing.aliyuncs.com/lifecycle.png" alt></p><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/99504a05.html">https://gongjintao.com/post/99504a05.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;1-生命周期定义&quot;&gt;&lt;a href=&quot;#1-生命周期定义&quot; class=&quot;headerlink&quot; title=&quot;1.生命周期定义&quot;&gt;&lt;/a&gt;1.生命周期定义&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;生命周期是指：从Vue实例创建、运行、到销毁期间，总是伴随着各种事件的发生，这些事件统称为生命周期。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Web前端" scheme="https://gongjintao.com/categories/Web%E5%89%8D%E7%AB%AF/"/>
    
    
      <category term="Vue" scheme="https://gongjintao.com/tags/Vue/"/>
    
  </entry>
  
  <entry>
    <title>CCF认证题库201312-3————最大的矩形</title>
    <link href="https://gongjintao.com/post/47ebd58b.html"/>
    <id>https://gongjintao.com/post/47ebd58b.html</id>
    <published>2018-09-11T11:27:46.000Z</published>
    <updated>2020-02-22T08:54:26.856Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>在横轴上放了 n 个相邻的矩形，每个矩形的宽度是 1，而第 i（1 ≤ i ≤ n）个矩形的高度是 hi。这 n 个矩形构成了一个直方图。例如，下图中六个矩形的高度就分别是 3, 1, 6, 5, 2, 3。<br><img src="http://118.190.20.162/RequireFile.do?fid=Fmhrjgtb" alt><br><a id="more"></a>请找出能放在给定直方图里面积最大的矩形，它的边要与坐标轴平行。对于上面给出的例子，最大矩形如下图所示的阴影部分，面积是 10。<br><img src="http://118.190.20.162/RequireFile.do?fid=gNe4DHhD" alt></p></blockquote><h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><blockquote><p>第一行包含一个整数 n，即矩形的数量 (1 ≤ n ≤ 1000)。<br>　第二行包含 n 个整数 h1, h2, … , hn，相邻的数之间由空格分隔。(1 ≤ hi ≤ 10000)。hi 是第 i 个矩形的高度。</p></blockquote><h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><blockquote><p>输出一行，包含一个整数，即给定直方图内的最大矩形的面积。</p></blockquote><h3 id="样例输入"><a href="#样例输入" class="headerlink" title="样例输入"></a>样例输入</h3><blockquote><p>6<br>3 1 6 5 2 3</p></blockquote><h3 id="样例输出"><a href="#样例输出" class="headerlink" title="样例输出"></a>样例输出</h3><blockquote><p>10</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><blockquote><p>用的是最直接的思路，所以并非很好的解法。<br>因为对于直方图中的每一个矩形，都会决定一个最大矩形。高即该矩形的高，宽则是左右两个方向比它高的最后一个之间的相隔个数。然后在从这些矩形中找出最大的即可。</p></blockquote><h3 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAXN=<span class="number">1002</span>;</span><br><span class="line"><span class="keyword">int</span> h[MAXN] = &#123;<span class="number">0</span>&#125;,n;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Sarea</span><span class="params">(<span class="keyword">int</span> n,<span class="keyword">int</span> h[])</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line"><span class="keyword">int</span> left = i ,right = i; </span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j = i<span class="number">-1</span>;j&gt;=<span class="number">0</span>;j--)&#123;<span class="comment">//往左遍历</span></span><br><span class="line"><span class="keyword">if</span>(h[j]&gt;=h[i])&#123;</span><br><span class="line">left = j;        <span class="comment">//若遍历的矩形比该矩形高，则继续，边界left等于当前矩形的序号</span></span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">break</span>;  <span class="comment">//若比该矩形低，则结束</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j= i+<span class="number">1</span>;j&lt;=n+<span class="number">1</span>;j++)&#123; <span class="comment">//向右遍历，同上</span></span><br><span class="line"><span class="keyword">if</span>(h[j]&gt;=h[i])&#123;</span><br><span class="line">right = j;</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> v = h[i]*(right-left == <span class="number">0</span>?<span class="number">1</span>:right-left+<span class="number">1</span>);<span class="comment">//若right==left表明最大的矩形只有该矩形一条。</span></span><br><span class="line"><span class="keyword">if</span>(v&gt;ans) ans = v;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;n;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;h[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;Sarea(n,h);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="声明：菜鸟拙见，大神勿喷"><a href="#声明：菜鸟拙见，大神勿喷" class="headerlink" title="(声明：菜鸟拙见，大神勿喷)"></a><font color="red" size="4" face="宋体">(声明：菜鸟拙见，大神勿喷)</font></h2><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/47ebd58b.html">https://gongjintao.com/post/47ebd58b.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;在横轴上放了 n 个相邻的矩形，每个矩形的宽度是 1，而第 i（1 ≤ i ≤ n）个矩形的高度是 hi。这 n 个矩形构成了一个直方图。例如，下图中六个矩形的高度就分别是 3, 1, 6, 5, 2, 3。&lt;br&gt;&lt;img src=&quot;http://118.190.20.162/RequireFile.do?fid=Fmhrjgtb&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="CCF试题库" scheme="https://gongjintao.com/categories/CCF%E8%AF%95%E9%A2%98%E5%BA%93/"/>
    
    
      <category term="C/C++" scheme="https://gongjintao.com/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>CCF认证题库201703-3————Markdown</title>
    <link href="https://gongjintao.com/post/4545002f.html"/>
    <id>https://gongjintao.com/post/4545002f.html</id>
    <published>2018-09-05T07:51:51.000Z</published>
    <updated>2020-02-22T08:54:27.185Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><blockquote><p>Markdown 是一种很流行的轻量级标记语言（lightweight markup language），广泛用于撰写带格式的文档。例如以下这段文本就是用 Markdown 的语法写成的：<br><img src="http://118.190.20.162/RequireFile.do?fid=Yy7nr9Yt" alt="01"><br><a id="more"></a>这些用 Markdown 写成的文本，尽管本身是纯文本格式，然而读者可以很容易地看出它的文档结构。同时，还有很多工具可以自动把 Markdown 文本转换成 HTML 甚至 Word、PDF 等格式，取得更好的排版效果。例如上面这段文本通过转化得到的 HTML 代码如下所示：<br><img src="http://118.190.20.162/RequireFile.do?fid=247472gj" alt="02"><br>本题要求由你来编写一个 Markdown 的转换工具，完成 Markdown 文本到 HTML 代码的转换工作。简化起见，本题定义的 Markdown 语法规则和转换规则描述如下：<br>　　●区块：区块是文档的顶级结构。本题的 Markdown 语法有 3 种区块格式。在输入中，相邻两个区块之间用一个或多个空行分隔。输出时删除所有分隔区块的空行。<br>　　○段落：一般情况下，连续多行输入构成一个段落。段落的转换规则是在段落的第一行行首插入 <code>&lt;p&gt;</code>，在最后一行行末插入 <code>&lt;/p&gt;</code>。<br>　　○标题：每个标题区块只有一行，由若干个 <code>#</code> 开头，接着一个或多个空格，然后是标题内容，直到行末。<code>#</code> 的个数决定了标题的等级。转换时，<code># Heading</code> 转换为 <code>&lt;h1&gt;Heading&lt;/h1&gt;</code>，<code>## Heading</code> 转换为 <code>&lt;h2&gt;Heading&lt;/h2&gt;</code>，以此类推。标题等级最深为 6。<br>　　○无序列表：无序列表由若干行组成，每行由 <code>*</code> 开头，接着一个或多个空格，然后是列表项目的文字，直到行末。转换时，在最开始插入一行 <code>&lt;ul&gt;</code>，最后插入一行 <code>&lt;/ul&gt;</code>；对于每行，<code>* Item</code> 转换为 <code>&lt;li&gt;Item&lt;/li&gt;</code>。本题中的无序列表只有一层，不会出现缩进的情况。<br>　　●行内：对于区块中的内容，有以下两种行内结构。<br>　　○强调：<code>_Text_</code> 转换为 <code>&lt;em&gt;Text&lt;/em&gt;</code>。强调不会出现嵌套，每行中 <code>_</code> 的个数一定是偶数，且不会连续相邻。注意 <code>_Text_</code> 的前后不一定是空格字符。<br>　　○超级链接：<code>[Text](Link)</code> 转换为 <code>&lt;a href=&quot;Link&quot;&gt;Text&lt;/a&gt;</code>。超级链接和强调可以相互嵌套，但每种格式不会超过一层。</p></blockquote><h3 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h3><blockquote><p>　　输入由若干行组成，表示一个用本题规定的 Markdown 语法撰写的文档。</p></blockquote><h3 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h3><blockquote><p>　　输出由若干行组成，表示输入的 Markdown 文档转换成产生的 HTML 代码。</p></blockquote><h3 id="样例输入"><a href="#样例输入" class="headerlink" title="样例输入"></a>样例输入</h3><blockquote><p># Hello</p><p>Hello, world!</p></blockquote><h3 id="样例输出"><a href="#样例输出" class="headerlink" title="样例输出"></a>样例输出</h3><blockquote><p><code>&lt;h1&gt;Hello&lt;/h1&gt;</code><br><code>&lt;p&gt;Hello, world!&lt;/p&gt;</code></p></blockquote><h3 id="评测用例规模与约定"><a href="#评测用例规模与约定" class="headerlink" title="评测用例规模与约定"></a>评测用例规模与约定</h3><blockquote><p>　　本题的测试点满足以下条件：<br>　　●本题每个测试点的输入数据所包含的行数都不超过100，每行字符的个数（包括行末换行符）都不超过100。<br>　　●除了换行符之外，所有字符都是 ASCII 码 32 至 126 的可打印字符。<br>　　●每行行首和行末都不会出现空格字符。<br>　　●输入数据除了 Markdown 语法所需，内容中不会出现 <code>#</code>、<code>*</code>、<code>_</code>、<code>[</code>、<code>]</code>、<code>(</code>、<code>)</code>、<code>&lt;</code>、<code>&gt;</code>、<code>&amp;</code> 这些字符。<br>　　●所有测试点均符合题目所规定的 Markdown 语法，你的程序不需要考虑语法错误的情况。</p></blockquote><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><blockquote><p>分两个部分进行处理，即区块和行列元素；然后建立vector容器html，存入处理后的每一行字符串。<br>1.首先处理行列元素，即处理强调符号和超级链接，分别构建deal1()和deal2()两个处理函数。注意的是这两个可以相互嵌套，所以需要将两个函数嵌套处理，所以需要提前申明。<br>2.区块分为三种，即段落，标题和无序列表。关键是段落和无序列表处理；当区块结束和文件末尾时，如果该区块是段落，则要在上一行末尾添加”&lt;/p&gt;”;如果该区块是无序列表，则要添加一个”&lt;/ul&gt;”。</p></blockquote><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">deal1</span><span class="params">(<span class="built_in">string</span> line)</span></span>;</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">deal2</span><span class="params">(<span class="built_in">string</span> line)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//强调处理 </span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">deal1</span><span class="params">(<span class="built_in">string</span> line)</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> pos;</span><br><span class="line"><span class="keyword">while</span>((pos = line.find(<span class="string">"_"</span>))!=<span class="built_in">string</span>::npos)</span><br><span class="line">&#123;</span><br><span class="line">line.replace(pos,<span class="number">1</span>,<span class="string">"&lt;em&gt;"</span>);</span><br><span class="line">line.replace(line.find(<span class="string">"_"</span>,pos),<span class="number">1</span>,<span class="string">"&lt;/em&gt;"</span>);</span><br><span class="line">line = deal2(line);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> line;</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="comment">//超链接处理 </span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">deal2</span><span class="params">(<span class="built_in">string</span> line)</span></span>&#123;</span><br><span class="line"><span class="keyword">int</span> pos;</span><br><span class="line"><span class="keyword">while</span>((pos = line.find(<span class="string">"["</span>))!=<span class="built_in">string</span>::npos)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> posR = line.find(<span class="string">"]"</span>,pos);</span><br><span class="line"><span class="keyword">int</span> posL1 = line.find(<span class="string">"("</span>,posR);</span><br><span class="line"><span class="keyword">int</span> posR1 = line.find(<span class="string">")"</span>,posL1);</span><br><span class="line"><span class="built_in">string</span> text = line.substr(pos+<span class="number">1</span>,posR-pos<span class="number">-1</span>);</span><br><span class="line"><span class="built_in">string</span> link = line.substr(posL1+<span class="number">1</span>,posR1-posL1<span class="number">-1</span>);</span><br><span class="line"><span class="built_in">string</span> a = <span class="string">"&lt;a href=\""</span>+link+<span class="string">"\"&gt;"</span>+text+<span class="string">"&lt;/a&gt;"</span>;</span><br><span class="line">line.replace(pos,posR1-pos+<span class="number">1</span>,a);</span><br><span class="line">line = deal1(line);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> line; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="built_in">string</span> line;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;  html;</span><br><span class="line"><span class="keyword">int</span> pSum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">bool</span> paraFlag = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">int</span> uSum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">bool</span> listFlag = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">while</span>(getline(<span class="built_in">cin</span>,line))</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(line.size()!=<span class="number">0</span>)<span class="comment">//如果字符串长度不为0 </span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">int</span> pos, sumH = <span class="number">0</span>; </span><br><span class="line"><span class="comment">//判断是否是段落 </span></span><br><span class="line"><span class="keyword">if</span>(*(line.begin())!=<span class="string">'#'</span> &amp;&amp; *(line.begin())!=<span class="string">'*'</span>)</span><br><span class="line">&#123;</span><br><span class="line">pSum++;</span><br><span class="line">paraFlag = <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(paraFlag)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(pSum==<span class="number">1</span>) line = <span class="string">"&lt;p&gt;"</span>+line;<span class="comment">//是否是段落开头  </span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//判断是否是标题 </span></span><br><span class="line"><span class="keyword">while</span>((pos = line.find(<span class="string">"#"</span>))!=<span class="built_in">string</span>::npos)</span><br><span class="line">&#123;</span><br><span class="line">sumH++;<span class="comment">//标题级别</span></span><br><span class="line">line.erase(pos,<span class="number">1</span>);</span><br><span class="line">&#125; </span><br><span class="line"> </span><br><span class="line"> <span class="comment">//判断是否是无序列表 </span></span><br><span class="line"><span class="keyword">while</span>((pos=line.find(<span class="string">"*"</span>))!=<span class="built_in">string</span>::npos) </span><br><span class="line">&#123;</span><br><span class="line">uSum++; </span><br><span class="line">listFlag = <span class="literal">true</span>;</span><br><span class="line">line.erase(pos,<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//将"#”和"*"去掉后，再去掉开头的多余空格 </span></span><br><span class="line"><span class="keyword">while</span>(*line.begin()==<span class="string">' '</span>)</span><br><span class="line">&#123;</span><br><span class="line">line.erase(<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(listFlag)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span>(uSum==<span class="number">1</span>) html.push_back(<span class="string">"&lt;ul&gt;"</span>); </span><br><span class="line">line = <span class="string">"&lt;li&gt;"</span> + line + <span class="string">"&lt;/li&gt;"</span>; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//处理强调和超链接 </span></span><br><span class="line">line = deal1(line);</span><br><span class="line">line = deal2(line);</span><br><span class="line"><span class="keyword">if</span>(sumH)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">char</span> tmp[<span class="number">2</span>];</span><br><span class="line"><span class="built_in">sprintf</span>(tmp,<span class="string">"%d"</span>,sumH);</span><br><span class="line"><span class="built_in">string</span> s= tmp;</span><br><span class="line">line = <span class="string">"&lt;h"</span>+s+<span class="string">"&gt;"</span>+line+<span class="string">"&lt;/h"</span>+s+<span class="string">"&gt;"</span>;</span><br><span class="line">&#125;</span><br><span class="line">html.push_back(line);</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line"><span class="comment">//如果录入的是空行，说明是区块结束 </span></span><br><span class="line"><span class="keyword">if</span>(uSum &gt;= <span class="number">1</span>)<span class="comment">//如果区块结束，无序标签数目大于等于1，则在后面添加&lt;/ul&gt;,同时标志和计数器重置 </span></span><br><span class="line">&#123;</span><br><span class="line">html.push_back(<span class="string">"&lt;/ul&gt;"</span>); </span><br><span class="line">uSum = <span class="number">0</span>;</span><br><span class="line">listFlag =<span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//如果区块结束，段落数目大于等于1，则在前一行后面添加&lt;/p&gt;,同时标志和计数器重置</span></span><br><span class="line"><span class="keyword">if</span>(pSum&gt;=<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;::iterator it = html.end()<span class="number">-1</span>;</span><br><span class="line">*it =*it+<span class="string">"&lt;/p&gt;"</span>; </span><br><span class="line">pSum =  <span class="number">0</span>;</span><br><span class="line">paraFlag = <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">line.clear();<span class="comment">//清空容器 </span></span><br><span class="line"> &#125; </span><br><span class="line"><span class="comment">//文件尾 也要判断 </span></span><br><span class="line"><span class="keyword">if</span>(uSum &gt;= <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">html.push_back(<span class="string">"&lt;/ul&gt;"</span>); </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span>(pSum&gt;=<span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;::iterator it = html.end()<span class="number">-1</span>;</span><br><span class="line"><span class="built_in">string</span> tmp = *it+<span class="string">"&lt;/p&gt;"</span>;</span><br><span class="line">*it = tmp; </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;html.size();i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;html[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="解题总结"><a href="#解题总结" class="headerlink" title="解题总结"></a>解题总结</h3><blockquote><p>要熟悉string的各个内置函数的常见用法：<br>line.find(str,pos):表示从字符串line的pos位置往后面查找字串str，如果有则返回首次出现的位置，若不存在则返回<code>string::npos</code>;<br>line.substr(pos,len):表示从line的pos位置截取长度为len的字串，包括pos位置的值；<br>line.replace(pos,len,str):表示用字符串str替换line的pos位置往后的len长度的字串；<br>line.erase(pos_0,pos_n)：表示移除line中从位置pos_0到pos_n中间的字符串，包括pos_0,不包括pos_n，即[pos_0,pos_n);</p></blockquote><h2 id="声明：菜鸟拙见，大神勿喷"><a href="#声明：菜鸟拙见，大神勿喷" class="headerlink" title="(声明：菜鸟拙见，大神勿喷)"></a><font color="red" size="4" face="宋体">(声明：菜鸟拙见，大神勿喷)</font></h2><p><strong>本文作者</strong>：Gong Jintao<br><strong>本文地址</strong>： <a href="https://gongjintao.com/post/4545002f.html">https://gongjintao.com/post/4545002f.html</a> <br><strong>版权声明</strong>：本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" rel="noopener">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Markdown 是一种很流行的轻量级标记语言（lightweight markup language），广泛用于撰写带格式的文档。例如以下这段文本就是用 Markdown 的语法写成的：&lt;br&gt;&lt;img src=&quot;http://118.190.20.162/RequireFile.do?fid=Yy7nr9Yt&quot; alt=&quot;01&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="CCF试题库" scheme="https://gongjintao.com/categories/CCF%E8%AF%95%E9%A2%98%E5%BA%93/"/>
    
    
      <category term="C/C++" scheme="https://gongjintao.com/tags/C-C/"/>
    
  </entry>
  
</feed>
